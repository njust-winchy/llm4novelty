paper_id,sentence,label
1025,Part-of-speech tagging (POS tagging) is an important task in natural language processing (NLP) because it can help in tasks such as syntactic parsing or named entity recognition.,0
1025,"This task is well-studied for high-resource languages such as English and Chinese, yet under-explored for Thai.",0
1025,Modern approaches includes neural networks where word representation is composed from a character representation or otherwise pretrained.,0
1025,"(Bohnet et al., 2018)  (Akbik et al., 2018) A major challenge in Thai POS tagging is that Thai is an isolating language, so inflectional morphology cannot guide the tagger.",0
1025,A word can be composed by juxtaposing a few subwords.,0
1025,"For example, ??????????? is a noun denoting the Crime Suppression Division.",0
1025,The word can be further split into ??? (division) and ???????? (suppress).,0
1025,And the ???????? verb compound can be further segmented into ???? and ???? .,0
1025,"Therefore, a Thai POS tagger should further segment the word and use the subwords as features.",0
1025,"We hypothesize that if a word is segmented into syllables, then syllable features should improve the performance.",1
1025,"Recently, the paradigm of pre-training and fine-tuning model such as ELMo (Peters et al., 2018) and BERT (Devlin et al., 2018) has received much attention.",0
1025,This paradigm uses wordpiece representation to prevent out-of-vocabulary problem and learn the representation for potential infixes.,0
1025,We hypothesize that this approach suits Thai POS tagging because noun and verb compounds can be captured by the wordpieces.,0
1025,"In this study, we explore syllable and wordpiece representation for Thai in handling out of vocabularies problems and improving the overall performance of Thai POS tagging.",1
1025,"To the best of our knowledge, the ORCHID corpus has been the only Thai corpus annotated with POS tags (Sornlertlamvanich et al., 1970).",0
1025,The tagset consists of 42 tags designed specifically to model the Thai language.,0
1025,"This tagset has never been mapped to a more manageable Universal Dependency (UD) (Nivre et al., 2020) tagset, which can be used across many different languages.",0
1025,"We aim to extend the existing Thai POS tagging studies by converting ORCHID tagset to UD tagset, which is more suitable for downstream tasks such as dependency parsing.",0
1025,Our contributions can be summarized as follows:,0
1025,"? We show that subword-level representations, namely syllable, character, and byte-pair encoding (BPE) are better than word embedding for POS tagging.",1
1025,? We present the BERT-based state-of-the-art model for Thai POS tagging against many strong neural baselines.,1
1025,? We mapped the widely-used ORCHID tagset to the new standard Universal POS tagset.,1
1032,"Mongolian is a morphologically rich language and its words are formed by attaching suffixes to roots (Kullmann and Tserenpil, 2008).",0
1032,"Each word has a root and zero or more suffixes, which are called Mongolian morphemes.",0
1032,The morphemes in a word indicate the basic word features and provide grammatical or semantic relations among words in the sentence.,0
1032,"Mongolian morphological segmentation aims to split Mongolian words into their morphemes, which facilitates the Mongolian NLP tasks, such as name entity recognition (Wang et al., 2016;, information retrieval (Liu et al., 2012), machine translation (Fan et al., 2017;Yang et al., 2016), and speech synthesis (Liu et al., 2017).",0
1032,"There are about 60 thousand of morphemes in Mongolian, and the number of their formed words is more than 7 million.",0
1032,It becomes a tendency to process Mongolian text on morphemes rather than on words to make full use of the morpheme information in Mongolian NLP tasks.,0
1032,"Besides, Segmenting Mongolian words into morpheme can alleviate the data-sparse problem and out-of-vocabulary (OOV) problem.",0
1032,"Therefore, Mongolian morphological segmentation is an essential preprocessing step and effects the downstream Mongolian NLP tasks.",0
1032,Mongolian morphological segmentation is close related to the words themselves and their context.,0
1032,Table 1 shows several morphological segmentation examples.,0
1032,"In usually, a Mongolian word corresponds to only one segmentation, such as the target words "" (bariba)"" and "" (barigulba)"" in sentences I and II.",0
1032,"But in some cases, parts of Mongolian words correspond to several segmentation results according to the context where they appear.",0
1032,"For example, the unit "" (n)"" in the word "" (negun)"" in the sentence III is a morpheme.",0
1032,It is just part of the morpheme " (han)" in the word " (algvrhan)" in the sentence IV.,0
1032,"In the sentences V and VI, the target word "" (higed)"" is segmented into different morphemes according to its context, and such words are called multi-category word.",0
1032,"For these two words "" (algvrhan)"" and "" (higed)"", the more preferred segmentation results of the existing system are "" + + (algvr+ha+n)"" and "" + (hi+ged)"".",0
1032,"This type of error, which has more morphemes than the gold standard, is called overcut error.",0
1032,"The experiments in (Narisong et al., 2016) show that more than 81% of errors are the overcut error.",0
1032,"In summary, Mongolian morphological segmentation is still a challenging task.",0
1032,Target word and its segmentation,0
1032,Meaning of the sentence I.,0
1032,? (tere baising bariba.),0
1032,(bariba) + (bari+ba),0
1032,He builts the house.,0
1032,? (tere baising barigulba.),0
1032,(barigulba) + + (bari+gul+ba ),0
1032,He had the house built.,0
1032,? (tere tvs gajar negun sagvrixiba .),0
1032,He emigrated to the local area.,0
1032,? (tere algvrhan garcv yabvn_a.),0
1032,He trudged out.,0
1032,? (tere ajil higed yabvba. ),0
1032,He went to work.,0
1032,? (tere nqm higed haranda-ji abvba.),0
1032,(higed) (higed),0
1032,He took the book and pencil.,0
1032,"This study proposes a novel approach to Mongolian morphological segmentation, which addresses challenges by incorporating inner-word and out-word features.",1
1032,The proposed network consists of two encoders and one decoder.,1
1032,"First, a standard self-attention network is utilized as an inner-word encoder, which conducts connections between two arbitrary characters in a word and draws the inner-word features directly.",1
1032,"Meanwhile, a bidirectional LSTM (BiLSTM) network is used as the out-word encoder to extract the out-word features of the word in the sentence.",1
1032,"Finally, a doubly attentive decoder is employed to get the segmented result.",1
1032,Results show that our model outperforms several baselines and is competitive with the state-of-the-art for Mongolian morphological segmentation.,0
1032,The contribution of this paper is as follows.,0
1032,This paper proposed a network for Mongolian morphological segmentation according to the Mongolian characteristics.,1
1032,Two well-designed encoders were introduced in the network to extract the inner-word-level feature information and the contextual information between the target word and other words in the sentence.,1
1032,A doubly attentive decoder distinguishes and balances the inner-word and out-word information utilization in the decode stage.,1
1032,The experiment demonstrates that our approach achieves very competitive performance.,0
1040,"Morphological inflection has gained substantial interest in recent years due to a number of shared tasks focusing on morphology learning (Cotterell et al., 2016;Cotterell et al., 2017;Cotterell et al., 2018;McCarthy et al., 2019;Vylomova et al., 2020;Kann et al., 2020).",0
1040,"Systems for inflection are trained on a sequence of pairs: {Lemma, MorphoSyntactic Descriptor (MSD)}, free of context, and must produce as output an inflected word form.",0
1040,"For example, given the input pair {run, V.PTCP;PRS}, a successful system should produce the present participial form: running.",0
1040,The shared tasks have illuminated several idiosyncrasies of inflection.,0
1040,"Although the state-of-the-art methods have been inspired by Neural Machine Translation (NMT), inflection is, in many ways, a more straightforward task.",0
1040,"Often, a majority of characters can be copied directly from input to output, and reordering of tokens is not necessary to the extent that it is in translation.",0
1040,"Thus, systems with copymechanisms (Makarov et al., 2017), and hard, monotonic attention (Aharoni and Goldberg, 2017) tend to perform well, even when data is scarce.",0
1040,Recurrent neural architectures require that the output from a previous time step be fed back into the model.,0
1040,"During training, existing systems all use a variant of teacher forcing, which feeds gold-standard tokens into the model at time t + 1.",0
1040,"This methodology differs significantly from how the model progresses at test-time, where silver tokens must be used.",0
1040,This problem has been dubbed exposure bias by Wiseman and Rush (2016).,0
1040,"We hypothesize that exposure bias can lead models to overfit the training data, particularly in low-resource settings.",1
1040,"In this paper, we compare teacher forcing to silver label propagation, or student forcing, where model predictions are fed into the decoder during training instead of gold standard labels.",1
1040,Our experiments on a geographically and linguistically diverse set of 10 languages show that student forcing typically outperforms teacher forcing in a low-resource setting.,1
1040,"Our analysis of the results suggests that teacher forced models are, indeed, overfitting the training data by ignoring input characters at test time.",1
1040,"We note substantial gains when student forcing is applied to soft-attentional systems, which bears promise for tasks other than inflection; however, less prominent gains are also observed in conjunction with hard attention.",1
1042,"Event chain, also known as script (Roger and Robert, 1977), is a structural knowledge format that models stereotypical human activities in a given scenario, e.g., ""dining at a restaurant"" and ""catching a thief"" in Fig.",0
1042,Representing such knowledge in a machine-readable way can help machine understand the semantics of natural language and further perform human-like inferences.,0
1042,"Besides, event representation can also support a series of downstream applications, such as question answering , discourse understanding  and information extraction (Liu et al., 2019a), etc.",0
1042,"Existing work on event representation mainly model event chain from three aspects, the intra-event based (Weber et al., 2018;Granroth-Wilding and Clark, 2016), the individual-event based (Li et al., 2018;Wang et al., 2017) and the event-segment (Lv et al., 2019) based models.",0
1042,"These methods concentrate on depicting the relations among homogeneous modeling objectives, e.g., the inter-event relations.",0
1042,"However, the relations among heterogeneous ones, e.g., the subordinated relations between word and event, which are also critical to the event chain, have not yet been taken into account in previous work.",0
1042,"Besides, (Lv et al., 2019) found the event segments, a set of individual events related to each other, were helpful to predict the missing event and event segments could be continuous or discontinuous (See Fig.",0
1042,"Although the self-attention mechanism can implicitly represent such discontinuous event segments by greedily making connections among all events (Lv et al., 2019), it inevitably introduces noises.",0
1042,"In this paper, we attempt to deal with these issues by proposing a heterogeneous-event (HeterEvent) graph network .",1
1042,"Specifically, we define two different types of nodes in the HeterEvent graph, including word and event nodes, which respectively represent unique words and individual events in the event chain.",1
1042,"Then we construct three types of edges, namely word-word edges denoting the co-occurrence relations within word nodes, word-event edges denoting the subordinate relations between word and event nodes, and event-event edges denoting the order relations within event nodes.",1
1042,We also design a message passing process to realize information interactions among homo or heterogeneous nodes.,1
1042,"Furthermore, the HeterEvent graph can explicitly represent discontinuous event segments by finding a path between their corresponding nodes in the graph.",1
1042,"We evaluate our proposal on one-step (Granroth-Wilding and Clark, 2016) and multi-step (Lee and Goldwasser, 2018) inference tasks, and the experimental results prove that our proposals present a stronger inference ability than existing baselines in event prediction.",0
1042,Our research contributions in this paper are in three folds: 1.,0
1042,"To the best of our knowledge, we are the first to construct a heterogeneous graph network to model the event chain.",1
1042,Our model outperforms the existing baselines on one-step and multi-step inference tasks.,0
1042,"Our proposed HeterEvent is a expandable framework that can be easily adapted to other granularities of information nodes, e.g., subwords or event scenario, which both are a part of the event chain.",1
1096,"The ultimate goal of opinion mining and sentiment analysis (Pang and Lee, 2008) is to automatically digest opinions of users towards a certain product to accommodate decision making.",0
1096,"While some of these opinions are explicitly articulated in product reviews that users write, most of them are unknown since users have not bought most of the products.",0
1096,"Alternative solutions such as aspect-based sentiment analysis (Mukherjee and Liu, 2012;Pontiki et al., 2016) and recommendation systems (Resnick and Varian, 1997;Bobadilla et al., 2013) exist, however these only offer superficial outputs that are not as expressive as textual reviews.",0
1096,"Thus, the task of automatically generating reviews given their attributes such as user and product, or review generation (Dong et al., 2017), is necessary to achieve this goal.",0
1096,"Much of the previous work (Dong et al., 2017;Sharma et al., 2018) has framed review generation as A2T (Attribute-to-Text problem), where the given input is a non-linguistic data (i.e., attribute identifiers for user, product, and rating) and the output is the review text.",0
1096,"In this problem setup, the key challenge is to learn rich representations of the attributes, which are then used to produce the text using either template-based surface realization methods (Kukich, 1983;McKeown, 1992) or neural-based decoders (Mei et al., 2016;Wiseman et al., 2017), as shown in the red box in Figure 1.",0
1096,"However, it is difficult for the model to learn these representations merely from the given attribute identifiers since they do not convey any semantics regarding the attributes.",0
1096,"Our key contribution is AT2T (Attribute-matched-Text-to-Text), of augmenting inductive biases of attributes with their matching reference reviews, as illustrated as the blue box in Figure 1.",1
1096,"For example, as shown in Figure 1, multiple references together contain inductive biases missing in attribute encodings, such as frequently reviewed aspects of the product (e.g., talking about plot and character aspects) or habitual user phrases (e.g., ""looking forward to the next book"").",0
1096,"These references greatly help text generation since not only do they reinforce the representations learned from the attributes, they also allow the use of techniques used in sequence-to-sequence learning such as attention (Bahdanau et al., 2015) and copy (See et al., 2017) mechanisms.",0
1096,"In related problem domains of generating abstractive summaries or dialogue utterances, such bias is introduced by a T2T (Text-to-Text) approach, of generating extractive summary first (Gehrmann et al., 2018) or retrieving informative prior turns (Cai et al., 2018), then generate using these as references documents.",0
1096,"Central to the framework is reference retrieval, as relevant references provide valuable context for generation, but, in contrast, noisy references rather hinder generation.",0
1096,"For reference retrieval in T2T, lexical features, e.g., TF-IDF, have been used, assigning relevance based on the degree of word overlap between two texts.",0
1096,"In AT2T, however, lexical features are not directly applicable or effective.",0
1096,"First, unlike T2T where input and output are both texts, our input is a list of identifiers, i.e., (user ID, product ID, rating).",1
1096,"As a result, we cannot expedite the process of finding a matching reference, as in T2T solutions using lexical features for a fast retrieval, e.g., using inverted index.",0
1096,"Second, lexical similarity cannot fully capture rating, as sentimental lexicons appear in a small portion of text .",0
1096,"For example, flipping a single lexicon (from 'good' to 'bad') from lexically identical sentences can completely invert the rating.",0
1096,"One alternative solution, complementing lexical similarity, is to assign additional credits to references labeled with the input rating.",0
1096,"However, references -labeled with a different rating, but having useful rating-related context -are forced to be penalized.",0
1096,"For example, in Figure 1, no additional credits are assigned to ref#2 labeled with the nearly opposite rating, although it contains useful context for the given rating, e.g., ""enjoyed"".",0
1096,We later empirically show that not lexical similarity or rating accuracy of references guarantee rating semantics of generated reviews.,1
1096,"To address these limitations, we propose two approaches: pseudo-supervised and reinforcement learning framework, denoted as SL and RL respectively.",1
1096,"First, we expedite matching in SL using identifiers.",1
1096,"For efficient retrieval without lexical features, we propose a parametric coarse-filtering approach using attribute identifiers, having constant time complexity for each instance in a candidate pool.",1
1096,"Second, to generate reviews which are compatible with input rating, we retrieve references which maximize the rating accuracy of generated reviews -rather than references labeled with the input rating.",1
1096,RL enables such retrieval: a retrieval model is trained to maximize rewards including rating accuracy as well as lexical similarity of generated reviews.,1
1096,"To validate the effectiveness of AT2T, we perform experiments on a dataset consisting of product reviews from Amazon Books, aligned with their corresponding attributes: user, product and rating (Dong et al., 2017).",0
1096,"Our extensive experiments using automatic evaluation methods show that utilizing relevant references hugely helps generation model in terms of content similarity, and rating accuracy.",0
1096,"Moreover, our human evaluations show that our model generates more informative and grammatical texts compared to previous models.",0
1101,"The COVID-19 pandemic reminded us of the need for a tool that biomedical researchers can use to sift through existing research to extract novel insights, and ultimately help them make novel drug discoveries.",0
1101,The rate of new publications in the biomedical field is on the rise.,0
1101,"PubMed reports that more than 1 million biomedical research papers are published each year, amounting to nearly two papers per minute (Landhuis, 2016).",0
1101,"For papers mentioning COVID-19 alone, as of June 2020 more than 8000 peerreviewed publications have been published on PubMed.",0
1101,"With the rate of scientific papers on COVID-19 doubling every fourteen days (Coren, 2020), it is imperative to have a language understanding tool that can extract relevant information from credible literature, such as the research methodology, data, authors, results, and citations (Hao, 2020).",0
1101,"In this paper, we address the problem from an information retrieval perspective, extracting the textual and contextual information from the corpus by taking a hierarchical approach.",1
1101,"Traditional search approaches such as Lucene-based Elasticsearch (Gormley and Tong, 2015) using BM-25 & Jaccard-based matrices are efficient in retrieving objective answers where the primary task is to extract specific parts of the passage.",0
1101,However such methods struggle in the contextual retrieval of documents for which we need latent space representation of the query and the corpus of passages.,0
1101,"Our work leverages the BERT language model architecture to pre-train a large-scale biomedical language representation model, named BioMedBERT.",1
1101,The work is inspired by the research of  from Korea University & Clova AI research group.,0
1101,"In said work, we use the new BREATHE dataset which combines full text articles and abstracts from ten data sources in the biomedical domain and use it to train our BioMedBERT model.",0
1101,We use the BERT LARGE model as a pre-training backbone to achieve new state-of-the-art results for question answering in the biomedical domain.,0
1101,"In addition, we obtained impressive results by combining BioMedBERT embeddings with Elasticsearch to obtain highly relevant results for information retrieval.",1
1101,"This is achieved by using a neural passage re-ranking mechanism, which learns the inherent structural dependencies in the query and the research articles.",1
1101,We validated our search algorithm by formulating BioASQ as a retrieval dataset.,1
1137,Question Generation (QG) aims to endow machines with the ability to ask relevant and to-the-point questions about a document.,0
1137,"QG has important practical applications, such as generating assessments for course materials in education (Heilman and Smith, 2010;Lindberg et al., 2013), prompting user interaction in dialog systems (Shukla et al., 2019), enabling machines to ask clarification questions such as FAQs (Saeidi et al., 2018;Krishna and Iyyer, 2019), and automatically building large-scale QA datasets for the research community (Du et al., 2017;Zhao et al., 2018).",0
1137,"Recent approaches to question generation (Du et al., 2017;Zhao et al., 2018;Liu et al., 2019;Pan et al., 2020) have used Seq2Seq models with attention (Bahdanau et al., 2014), which feeds the input document into an RNN-based encoder, and generates a question about the document through a decoder.",0
1137,"The training objective is to maximize the log likelihood of the ground-truth question paired with each input document using teacher forcing (Williams and Zipser, 1989).",0
1137,"However, as the ground-truth questions are insufficient to account for the many equivalent ways of asking a question, this likelihood-based training suffers from the problem of exposure bias (Ranzato et al., 2016); i.e., the model does not learn how to distribute probability mass over sequences that are valid but different from the ground truth (Hosking and Riedel, 2019).",0
1137,"To address this issue, previous QG works proposed to optimize the model directly on question-specific rewards (e.g., fluency, answerability) via Reinforcement Learning (RL).",0
1137,"This process decouples the training procedure from the ground truth data, so that the space of possible questions can be better explored.",0
1137,"Moreover, it allows the training to target on specific properties we want the question to exhibit, such as relevant to a specific topic or whether the question is answerable from the document's content.",0
1137,"Although various rewards have been employed for QG -such as BLEU (Kumar et al., 2018), the answerability reward (Zhang and Bansal, 2019;, and the word movers distance  -optimizing the reward scores does not always lead to higher question quality in practice, as observed by Hosking and Riedel (2019).",0
1137,How to define robust and effective QG-specific rewards still requires further investigation.,0
1137,We aim to analyze the effectiveness of question-specific rewards in QG.,1
1137,"Instead of using general NLG metrics such as BLEU, we target three QG-related metrics that are commonly cited in human evaluations of question quality: (1) Fluency indicates whether the question follows the grammar and accords with the correct logic; (2) Relevance indicates whether the question is relevant to the document; and (3) Answerability indicates whether the question is answerable given the document.",1
1137,"We design a specific RL reward for each metric: a language model based reward for fluency, a discriminator-based reward for relevance, and an QA-based reward for answerability.",1
1137,"After optimizing each reward via RL, we conduct comprehensive analysis, including automatic and human evaluation, to arrive at the following conclusions: (1) both individual and joint optimization of these rewards can lead to performance gain in automated metrics such as BLEU, but this does not guarantee an improvement in the real question quality; (2) the reward for relevance substantially helps to improve the question quality, while the reward for answerability reduces the quality due to the bottleneck brought by the innate capability of the QA model; and (3) whether a reward can improve the real question quality strongly depends on whether the reward score correlates well with human judgement.",1
1172,"In recent years, safe Internet for children has gained interest in many research domains (Tomczyk and Kopeck?, 2016;Byrne and Burton, 2017;Livingstone, 2019).",0
1172,"However, most studies focus on abusive texts containing hate, violence, pornography, etc.",0
1172,"(Liu and Forss, 2015;Suvorov et al., 2013).",0
1172,"On the contrary, the adequacy of textual contents with the reading and understanding capabilities of children remains yet mainly unresolved in computational linguistics.",0
1172,"Hence, this paper propose a new method to predicting this adequacy.",1
1172,"Among related works, (Schwarm and Ostendorf, 2005) explored the possibility of predicting from which US school grade newspaper articles could be read.",0
1172,"This task was modelled as a classification problem among 4 classes using support vector machine fed with word-based n-gram probabilities, as well as lexical and syntactic features.",0
1172,"(Islam and Rahman, 2014) has proposed a readability classification method for Bangla news articles for children.",0
1172,"This method predicts if a text is either very easy, easy, medium or difficult.",0
1172,"More recently, (Blandin et al., 2020) proposed different feed-forward (FF) neural models for age recommendation on texts targeting either children (from 0 to 14) or adults.",0
1172,"The authors consider this as a regression task and explore various linguistic features and word embedding features, from which word embedding are shown as the most contributory.",0
1172,"Overall, one can notice that these papers either rely on hand-crafted features or on simple models which consider texts as a global object rather than word sequences.",0
1172,This motivates us to further explore with word embeddings only and introduce recurrent neural networks (RNNs).,1
1172,"More broadly, in the field of text readability, inherited from historical approaches like (Kincaid and Chissom, 1975), audiences other than children have been studied, e.g., second language learners (Xia et al., 2016), adults readers (Crossley et al., 2017 or patients interacting with doctors (Balyan et al., 2019).",0
1172,"In parallel, text understanding by children is a well known question in psycho-linguistics and cognitive sciences.",0
1172,"In particular, key findings have shown the impact of memory (Gathercole, 1999), temporality (Tartas, 2010;Hickmann, 2012), and emotions (Davidson, 2006;Mouw et al., 2019).",0
1172,"Related results also exist in the learning to read domain (Frith, 1985).",0
1172,"Following the recent trends in NLP, the contribution of this paper is to tackle age prediction as a regression problem using RNNs based on Long Short Term Memories (LSTMs) and fed with pre-trained word embeddings.",1
1172,We propose several variants of this architecture and compare them extensively to naive and FF approaches.,1
1172,We also investigate the difference between predicting age at the sentence and text levels.,1
1172,"Let one note that the use of more advanced architectures like transformers (Vaswani et al., 2017) is left for the future, since they are known to require very large amounts of data.",0
1172,"The experiments are carried out on a French corpus of around 1, 500 texts of 160K sentences, from encyclopedia, newspapers and fictions for a wide range of different age levels, including adults.",0
1172,"We think that this corpus is another interesting aspect of our work, since, compared to others, it is not limited to a specific genre or public.",1
1172,"In the remainder, Section 2 defines with more precision the age prediction task and presents the related data.",0
1172,"Then, Section 3 presents the adopted approach and the underlying models.",0
1172,"Finally, Section 4 details and discusses the results.",0
1180,Word embedding is one of the core research areas in natural language processing.,0
1180,"Its usefulness has been demonstrated in a wide variety of NLP tasks, e.g.",0
1180,"dependency parsing, sentiment analysis, and machine reading comprehension.",0
1180,"Modern embedding methods are usually based on the distributional hypothesis, namely, co-occurred words tend to purport similar semantic meanings.",0
1180,"Although the distributional word vectors perform well on lots of tasks, they have a well-known tendency to conflate the semantic similarity information with the semantic relatedness (Hill et al., 2015).",0
1180,"Therefore, the similarity between word vectors cannot reflect the precise semantic relation between word pairs, but just a semantic association (Yih et al., 2012).",0
1180,"For instance, if two words are antonyms, their corresponding word vectors could be very close geometrically, which makes it very hard to distinguish one word from the other.",0
1180,"One way to solve this problem is to inject some lexical constraints, such as antonym relationships into the word vectors, the aim is to make antonyms far apart from each other.",0
1180,This process is often referred as semantic specialization .,0
1180,"There are two kinds of semantic specialization methods: (1) joint specialization methods, which word vectors are trained from scratch by incorporating the lexical knowledge into the learning objective of the distributional models.",0
1180,Pham et al.,0
1180,"(2015) inject a synonym/antonym margin loss into the skip-gram objective to enforce that antonym pairs have low similarity while synonym pairs have high similarity; (2) retrofitting methods (also referred as postprocessing methods), that pre-trained word vectors are fine-tuned by injecting the lexical information into vector spaces.",0
1180,proposed the ATTRACT-REPEL(AR) algorithm which tries to push or pull a pair of words by a margin compared with its corresponding negative samples.,0
1180,Another retrofitting method is to inject lexical constraints into the word similarity matrix and obtain the tuned word vectors via the matrix decomposition.,0
1180,The method proposed by Sedoc et al.,0
1180,(2017) is in this line of research.,0
1180,"Generally speaking, the retrofitting methods have a better performance (Mrk?ic et al., 2016) while the joint specialization methods can specialize all words.",0
1180,"In this paper, we want to address the following question: Can we design a retrofitting method that is interpretable and has a competitive performance with the start-of-the-art methods?",1
1180,"Thus, we propose a novel model called HRSWE (Heterogeneously Retrofitted Spectral Word Embedding).",1
1180,"The basic idea is that if the difference between the similarity of an antonym pair and the minimum similarity of all pairs is large (small), the weight of lexical constraint about this antonym pair will be high (low).",1
1180,Similar ideas are applied for synonym pairs.,0
1180,"Moreover, words i and k will tend to be synonyms if words i and k have a common synonym(antonym) j.",0
1180,"On the other hand, i and k will tend to be antonyms if (i, j) are synonyms and (j, k) are antonyms or the other way around.",0
1180,"This phenomenon is called contagion, which will be modeled via the matrix multiplication of thesauri matrices.",0
1180,"After the similarity matrix is constructed, we do spectral decomposition on the similarity matrix to obtain the specialized embedding.",1
1180,"We also care about whether the performance of a specialization method still holds when the thesauri used by the method is perturbed, which is called the robustness.",1
1180,"In this paper, we explore a few perturbation methods.",0
1180,"Overall, HRSWE is slightly better than AR in terms of robustness given these perturbations.",1
1180,The contributions of our method are as follows.,0
1180,"? Foremost, our method is more interpretable than AR in three folds.",0
1180,"First, our embedding has a clear algebraic relationship with the original word embedding, while ATTRACT-REPEL does not have this property.",1
1180,"In particular, our embedding can be formulated as an analytical form in terms of the injected similarity matrix.",1
1180,"Furthermore, the importance of synonym and antonym information injected into the similarity matrix is quantified by experiments.",1
1180,"Finally, the significance of the contagion information is demonstrated by experiments as well.",1
1180,"? In terms of performance, on one hand, our novel method not only has a much better performance compared with Word2Vec but also achieves a competitive performance with the state-of-the-art method ATTRACT-REPEL on three tasks.",0
1180,"Furthermore, our method has slightly better robustness compared with ATTRACT-REPEL.",0
1180,"On the other hand, our method is faster than ATTRACT-REPEL by at least one order of magnitude in terms of running time.",0
1180,It makes our method appealing.,0
1196,"For morphologically rich languages, morphological analysis and disambiguation plays a critical role in most natural language processing (NLP) tasks.",0
1196,"When inflections are generated by piecing together multiple morphemes, a large and sparse vocabulary is produced, requiring tools to unpack the individual morphemes for downstream NLP tasks such information extraction and machine translation.",0
1196,A key characteristic of these languages is that morphemes often have specific meaning (often relating to properties of the words they form or referring to contextual entities) and their combination into words is mostly regular.,0
1196,Figure 1 shows a typical morphological units contained in the word 'ntuzamwibeshyeho' (Never underestimate him/her).,0
1196,"While several morphologically rich languages such as Turkish already have mature tools for morphological segmentation (C ??ltekin, 2014), Kinyarwanda still lacks appropriate tools for the task.",0
1196,A key limitation in the effort is the need to have high quality datasets manually annotated by language experts.,0
1196,"With limited funding opportunities, research on NLP for low resource languages lags behind recent advancements made for NLP on high resource languages.",0
1196,"In this work, we leverage an easy to collect stemming dataset and transform it into a resource for morphological disambiguation.",1
1196,"While the focus here is on Kinyarwanda verbal forms, the method can be applied to other morphologically rich languages.",0
1196,Collecting stemming data is much faster and less prone to errors than full morphological segmentations which require subtle linguistic knowledge.,0
1196,"Through a maximum likelihood approach, we are able to combine morphological properties of stems with inflectional similarity information from word embeddings to accurately disambiguate candidate segmentations from a morphological analyser.",1
1196,Our work here pertains to non-contextual verb-phrase disambiguation but is a key step towards contextual disambiguation.,1
1196,Figure 1: Morphological segmentation of the word 'ntuzamwibeshyeho'.,0
1196,Each morpheme unit has a specific meaning or role in order to get the meaning of the word.,0
1196,The word is an inflection of the lemma 'kwibeshya' (to err) which a derivation from 'kubeshya' (to lie) by adding a reflexive -ii-.,0
1196,"Therefore, a literal morpheme-by-morpheme translation of the word would be 'Never lie to yourself about him/her' while the real meaning is 'Never underestimate him/her'.",0
1196,Score Probability -nti u/1 -za ---mu -ibeshy -------e ho 3.471e-04 0.184 -nti u/7 -za ---mu -ibeshy -------e ho 2.873e-04 0.152 -nti u/5 -za ---mu -ibeshy -------e ho 2.590e-04 0.137 -nti u/7 -za ---mu ii beshy -----y -e ho 1.401e-04 0.074 -nti u/1 -za ---mu ii beshy -----y -e ho 1.393e-04 0.074 -nti u/5 -za ---mu ii beshy -----y -e ho 1.388e-04 0.074 -nti u/5 -za ---mu ii beshy -------e ho 1.161e-04 0.062 -nti u/1 -za ---mu ii beshy -------e ho 1.147e-04 0.061 -nti u/7 -za ---mu ii beshy -------e ho 1.101e-04 0.058 -nti u/1 -za ---mu ii besh -----y -e ho 7.310e-05 0.039 -nti u/5 -za ---mu ii besh -----y -e ho 6.950e-05 0.037 -nti u/7 -za ---mu ii besh -----y -e ho 6.364e-05 0.034 -nti u/1 -za ---mu -ibeshy -----y -e ho 1.743e-05 0.009 -nti u/5 -za ---mu -ibeshy -----y -e ho 7.583e-06 0.004 -nti u/7 -za ---mu -ibeshy -----y -e ho 1.270e-06 0.001,0
1196,Table 1: Potential segmentations for the inflected verb 'ntuzamwibeshyeho' as produced by a finite state based morphological analyzer.,0
1196,Hyphens(-) represent potential morpheme slots that are not filled for this instance.,0
1196,The stem or root is shown in bold.,0
1196,"The top three options only differ in the entity class for the subject -u/5 is for humans, 3rd person singular -u/1 also for humans but 2nd person singular -while u/7 is for inanimate object.",0
1196,The disambiguation tools is therefore able to discern that the subject would be mostly likely be a 2nd person.,0
1196,"Note that the tool has chosen to base on the derived stem '-ibeshy-' (to err) rather than the original root '-beshy-' (to lie), reflecting more focus on semantics.",0
1200,"Cross-modal semantic understanding has become an attractive challenge in natural language processing and computer vision, inspiring many tasks such as image captioning (Xu et al., 2015;Vinyals et al., 2015) and visual question answering (VQA) (Antol et al., 2015;Anderson et al., 2018;Shimizu et al., 2018).",0
1200,"However, in these missions, the co-reference between vision and language is usually performed in a single round and they do not have many interaction with human over a period of time.",0
1200,"In 2017, Das et al.",0
1200,"introduced a continuous conversational task, visual dialog (Das et al., 2017).",0
1200,This task needs an AI agent to answer a sequence of questions based on visually-grounded information and contextual information from a dialog history.,0
1200,"Recently, a manual investigation (Kim et al., 2020) on the Visual Dialog dataset (VisDial) tried to figure out how many questions can be answered with images and how many of them need conversation history to be answered.",0
1200,The investigation shows that around 80% of the questions can be answered with images and about 20% questions need the knowledge from dialog history.,0
1200,"Therefore, one of the key challenges in visual dialog is how to effectively utilize these underlying contents in the textual and visual information, i.e., input questions, dialog history and input image.",0
1200,"In previous works, such as RvA (Niu et al., 2019) and DAN (Kang et al., 2019), both tended to explicitly reason over past dialog interactions by referring back to previous references.",0
1200,But they ignored the underlying relational structure which contributes to dialog inference.,0
1200,"Nowadays, researchers have attempted to consider the fixed graph attention or embedding to resolve the problem with structural representations Schwartz et al., 2019).",0
1200,They focused on the textual modality though neglected the rich underlying information in image.,0
1200,"In this task, despite its significance to artificial intelligence and human-computer interaction, the agent requires understanding a series of multi-modal entities, and reasons the rich information in both vision and language.",0
1200,An ideal inference algorithm should be able to find out the underlying relational structure and give a reasonable answer based on this structure.,0
1200,"To address aforementioned problem, we pay more attention on visual-textual relation and propose the VTAGI in Figure 1 to explore potential information for structural inference.",1
1200,The agent will first obtain Figure 1: An overview of VTAGI.,0
1200,"We present two modules, VTA and VGAT.",0
1200,"VTA takes the attended visual features and attended textual features including question and history as inputs, resulting in integrated image representations reflecting semantics of certain objects in the image.",1
1200,VGAT aims to construct a visual graph combined with textual context.,1
1200,The relations among the nodes are top-5 important.,1
1200,"For example, the most thickest link between the node v 1 and v k indicates the most important dependencies of them.",0
1200,"? and denote matrix multiplication and element-wise product, respectively.",1
1200,"the question features, history features and visual features by employing different attention mechanisms.",1
1200,"However, the semantics of the visual features and the textual concepts are usually inconsistent, and the representations of the image lack of the global structural information.",0
1200,"Thus, the VTA module is to align the visual features and global textual contents with their relevant counterparts in each image domain.",1
1200,"As a result, the visual features contain more specific semantic information.",1
1200,"For example, in Figure 1, the visual feature v 1 is linked to the text of ""giraffe, only, beside, trees"", because each visual feature considers all contextual information.",0
1200,"In order to infer more reasonably and connect integral individual visual features, we design the VGAT module to construct a visual graph which shows the different relationships among various visual features.",1
1200,"For example, considering the feature v 1 in Figure 1, the thickest link between the v 1 and v k indicates the most important relationship of the two features.",0
1200,This module learns how to select other nodes related to the current node.,1
1200,"In the last step in this process, each visual feature node in this structural module is connected to its related nodes.",1
1200,"Through the two modules, the final visual features possess more related semantic information and they are intra-connected in the graph, which are beneficial to inference.",1
1237,"With the development of large-scale pre-trained Language Models (LMs) such as BERT (Devlin et al., 2018), XLNet , and T5 (Raffel et al., 2019), tremendous progress has been made in Question Answering (QA).",0
1237,"Fine tuning pre-trained LMs on task-specific data has surpassed human performance on QA datasets such as SQuAD (Rajpurkar et al., 2016) and NewsQA (Trischler et al., 2016).",0
1237,"Nevertheless, most existing QA systems largely deal with factoid questions and assume a simplified setup such as multiple-choice questions, retrieving spans of text from given documents, and filling in the blanks.",0
1237,"However, in many more realistic situations such as online communities, people tend to ask 'descriptive' questions (e.g., 'How to improve the sound quality of echo dot?').",0
1237,"Answering such questions requires the identification, linking, and integration of relevant information scattered over long-form multiple documents for the generation of free-form answers.",0
1237,We are particularly interested in developing a QA system for questions from e-shopping communities using customer reviews.,0
1237,"Compared to factoid QA systems, building a review QA system faces the following challenges: (1) as opposed to extractive QA where answers can be directly extracted from documents or multiple-choice QA where systems only need to make a selection over a set of pre-defined answers, review QA needs to gather evidence across multiple documents and generate answers in freeform text; (2) while factoid QA mostly centres on 'entities' and only needs to deal with limited types of questions, review QA systems are often presented with a wide variety of 'descriptive' questions;",0
1237,(3) customer reviews may contain contradictory opinions.,0
1237,Review QA systems need to automatically identify the most prominent opinion given a question for answer generation.,0
1237,"In our work here, we focus on the AmazonQA dataset (Gupta et al., 2019), which contains a total of 923k questions and most of the questions are associated with 10 reviews and one or more answers.",0
1237,We propose a novel Cross-passage Hierarchical Memory Network named CHIME to address the aforementioned challenges.,1
1237,"Regular neural QA models search answers by interactively comparing the question and supporting text, which is in line with human cognition in solving factoid questions.",0
1237,"While for opinion questions, the cognition process is deeper: reading larger scale and more complex texts, building crosstext comprehension, continually refine the opinions, and finally form the answers.",0
1237,"Therefore, CHIME is designed to maintain hierarchical dual memories to closely simulates this cognition process.",1
1237,"In this model, a context memory dynamically collect cross-passage evidences, an answer memory stores and continually refines answers generated as CHIME reads supporting text in a sequential manner.",1
1237,Figure 1 illustrates the setup of our task and an example output generated from CHIME.,1
1237,The top box shows a Figure 1: Illustration of the review QA task and the general idea of CHIME.,1
1237,The example question (the top box) is paired with 10 reviews (left panel) and one or more answers (right upper panel).,1
1237,"CHIME is trained on the (Question, Review, Answer) triplet.",1
1237,"During testing, CHIME is presented with a question and 10 related reviews and generates an answer (right bottom box).",1
1237,"Both reviews and answers in this example contain contradictory information as highlighted by colors, while the question contains complex sub-questions.",1
1237,CHIME is able to identify relevant evidence and generate clear answers.,1
1237,question extracted from our test set while the left panel and the right upper panel show the related 10 reviews and the paired 4 actual answers.,1
1237,We can observe that the question can be decomposed into complex sub-questions and both reviews and answers contain contradictory information.,1
1237,"However, CHIME can deal with such information effectively and generate appropriate answers as shown in the right-bottom box.",0
1237,"In summary, we have made the following contributions:",0
1237,? We propose a novel Cross-passage HIerarchical MEmory Network (CHIME) for review QA.,1
1237,"Compared with many multi-passage QA models, CHIME does not rely on explicit helpful ranking information of supporting reviews, but can capture cross-passage contextual information and effectively identify the most prominent opinion in reviews.",1
1237,"? CHIME reads reviews sequentially, overcoming the input length limitation affecting most of the existing transformer-based systems, and diminishing the memory requirement at training time.",1
1237,? Experimental results on the AmazonQA dataset show that CHIME outperforms a number of competitive baselines in terms of the quality of answers generated.,0
1265,"Pre-trained language models have received great interest in the natural language processing (NLP) community in the last recent years (Dai and Le, 2015;Radford, 2018;Howard and Ruder, 2018;Baevski et al., 2019;.",0
1265,"These models are trained in a semi-supervised fashion to learn a general language model, for example, by predicting the next word of a sentence (Radford, 2018).",0
1265,"Then, transfer learning (Pan and Yang, 2010;Moeed et al., 2020) can be used to leverage the learned knowledge for a down-stream task, such as text-classification (Do and Ng, 2006;Aggarwal and Zhai, 2012;Reimers et al., 2019;Sun et al., 2019b).",0
1265,Devlin et al.,0
1265,"(2019) introduced the ""Bidirectional Encoder Representations from Transformers"" (BERT), a pre-trained language model based on the Transformer architecture (Vaswani et al., 2017).",0
1265,"BERT is a deeply bidirectional model that was pre-trained using a huge amount of text with a masked language model objective where the goal is to predict randomly masked words from their context (Taylor, 1953).",0
1265,"The fact is, BERT has achieved state of the art results on the ""General Language Understanding Evaluation"" (GLUE) benchmark (Wang et al., 2018) by only training a single, task-specific layer at the output and fine-tuning the base model for each task.",0
1265,"Furthermore, BERT demonstrated its applicability to many other natural language tasks since then including but not limited to sentiment analysis Li et al., 2019), relation extraction (Baldini Soares et al., 2019; and word sense disambiguation Hadiwinoto et al., 2019;, as well as its adaptability to languages other than English (Martin et al., 2020;Antoun et al., 2020;Agerri et al., 2020).",0
1265,"However, the fine-tuning data set often contains thousands of labeled data points.",0
1265,"This plethora of training data is often not available in real world scenarios (Tan and Zhang, 2008;Wan, 2008;Salameh et al., 2015;Fang and Cohn, 2017).",0
1265,"In this paper, we focus on the low-resource setting with less than 1,000 training data points.",0
1265,Our research attempts to answer the question if pool-based active learning can be used to increase the performance of a text classifier based on a Transformer architecture such as BERT.,1
1265,"That leads to the next question: How can layer freezing techniques (Yosinski et al., 2014;Howard and Ruder, 2018;, i.e.",0
1265,"reducing the parameter space, impact model training convergence with fewer data points?",0
1265,"To answer these questions, we explore the use of recently introduced Bayesian approximations of model uncertainty (Gal and Ghahramani, 2016) for data selection that potentially leads to faster convergence during fine-tuning by only introducing new data points that maximize the knowledge gain of the model.",1
1265,"To the best of our knowledge, the work presented in this paper is the first demonstration of combining modern transfer learning using pre-trained Transformer-based language model such as the BERT model with active learning to improve performance in low-resource scenarios.",1
1265,"Furthermore, we explore the effect of trainable parameters reduction on model performance and training stability by analyzing the layer-wise change of model parameters to reason about the selection of layers excluded from training.",1
1265,"The main findings of our work are summarized as follows: a) we found that the model's classification uncertainty on unseen data can be approximated by using Bayesian approximations and therefore, used to efficiently select data for manual labeling in an active learning setting; b) by analyzing layer-wise change of model parameters, we found that the active learning strategy specifically selects data points that train the first and thus more general natural language understanding layers of the BERT model rather than the later and thus more task-specific layers.",1
1272,We introduce a new resource for speech-centric natural language processing (speech NLP) -more than a thousand transcriptions and error annotations for 383 distinct recordings from the CROWDED Corpus .,1
1272,"CROWDED is a crowdsourced English corpus of short monologues on business topics, recorded by both native and non-native speakers.",1
1272,"It was created in response to the lack of speech corpora freely available for research use, and the lack of appropriate native speaker reference corpora with which language learners' exam monologues can be compared.",1
1272,"In this new project, crowdworkers were asked to first correct existing speech transcriptions and then to edit the resulting transcriptions to make them more fluent.",0
1272,These new annotations enable both post-editing of noisy speech transcriptions -such as might come from automatic speech recognisers -and also grammatical error correction for spoken English.,1
1272,"There has been a marked increase in openly-available NLP resources in recent years, especially for English, but these have on the whole been sourced from written texts.",0
1272,"Public resources for speech NLP, on the other hand, are relatively scarce, even for English -though English is again by far the best served in this respect.",0
1272,"Obtaining linguistic data from large, distributed, online workers ('crowdsourcing') has become a well-established practice.",0
1272,"With the contribution of these new annotations, we note that the CROWDED Corpus now features a thousand recordings which have all been transcribed, and of which almost 40% now have improved transcriptions and error annotations thanks to this work.",0
1272,The entire corpus has been collated through crowdsourcing means.,0
1272,"In this paper we describe the method for collecting the new data, analyse the annotations received, and report on some initial grammatical error detection experiments (GED).",1
1272,In the GED experiments we trial various configurations which have been successful in GED for written texts.,0
1272,"We found that we can identify errors in the transcriptions fairly reliably using a publicly-available sequence labeller adapted to take contextual word representations as additional input, similar to previous work on GED in written corpora (Rei and Yannakoudakis, 2016;Bell et al., 2019).",1
1272,Place licence statement here for the camera-ready version.,0
1289,Machine reading comprehension (MRC) aims at teaching machines to read and understand given text.,0
1289,"Many models (Devlin et al., 2018;Yang et al., 2019;Liu et al., 2019) have defeated humans on the performance of SQuAD (Rajpurkar et al., 2016;Rajpurkar et al., 2018), as shown on its leaderboard 1 .",0
1289,"However, such performances are not indicative that these models can completely understand the text.",0
1289,"Specifically, using an adversarial method, Jia and Liang (2017) demonstrated that the current models do not precisely understand natural language.",0
1289,"Moreover, Sugawara et al.",0
1289,(2018) demonstrated that many datasets contain a considerable number of easy instances that can be answered based on the first few words of questions.,0
1289,Multi-hop datasets have recently been created which require a model to read and perform multi-hop reasoning over multiple paragraphs to answer the question.,0
1289,"Currently, there are four multi-hop MRC datasets over textual data: ComplexWebQuestions (Talmor and Berant, 2018), QAngaroo (Welbl et al., 2018), HotpotQA (Yang et al., 2018), and R 4 C (Inoue et al., 2020).",0
1289,The first two datasets were created by incorporating the documents (from Web or Wikipedia) with a knowledge base (KB).,0
1289,"Owing to their building procedures, these datasets have no information to explain the predicted answers.",0
1289,"Meanwhile, the other two datasets were created mainly based on crowdsourcing.",0
1289,"In HotpotQA, the authors introduced the sentence-level supporting facts (SFs) information that are used to explain the predicted answers.",0
1289,"However, as discussed in Inoue et al.",0
1289,"(2020), the task of classifying sentence-level SFs is a binary classification task that is incapable of evaluating the reasoning and inference skills of the model.",0
1289,"Further, data analyses (Chen and Durrett, 2019;Min et al., 2019) revealed that many examples in HotpotQA do not require multi-hop reasoning to solve.",0
1289,"Recently, to evaluate the internal reasoning of the reading comprehension system, Inoue et al.",0
1289,(2020) proposed a new dataset R 4,0
1289,C that requires systems to provide an answer and derivations.,0
1289,A derivation is a semi-structured natural language form that is used to explain the answers.,0
1289,"C is created based on HotpotQA and has 4,588 questions.",0
1289,"However, the small size of the dataset implies that the dataset cannot be used as a multi-hop dataset with a comprehensive explanation for training end-to-end systems.",0
1289,"In this study, we create a large and high quality multi-hop dataset 2WikiMultiHopQA 2 with a comprehensive explanation by combining structured and unstructured data.",1
1289,"To enhance the explanation and evaluation process when answering a multi-hop question on Wikipedia articles, we introduce new information in each sample, namely evidence that contains comprehensive and concise information to explain the predictions.",1
1289,"Evidence in our dataset is a set of triples, where each triple is a structured data (subject entity, property, object entity) obtained from the Wikidata (see Figure 1 for an example).",1
1289,"Our dataset has four types of questions: comparison, inference, compositional, and bridge comparison.",1
1289,All questions in our dataset are created by using a set of predefined templates.,1
1289,Min et al.,0
1289,"(2019) classified the comparison questions in HotpotQA in three types: multi-hop, context-dependent multihop, and single-hop.",0
1289,"Based on this classification, we removed all templates in our list that make questions become single-hop or context-dependent multi-hop to ensure that our comparison questions and bridge-comparison questions are multi-hop.",1
1289,We carefully designed a pipeline to utilise the intersection information between Wikipedia's summary and Wikidata and have a special treatment for each type of question that guarantees multi-hop steps and the quality of the questions.,1
1289,"Further, by utilising the logical rule information in the knowledge graph, such as father(a, b) ^father(b, c) ) grandf ather(a, c), we can create more natural questions but still require multi-hop reasoning.",1
1289,We conducted two different evaluations on our dataset: difficulty and multi-hop reasoning of the dataset.,0
1289,"To evaluate the difficulty, we used a multi-hop model to compare the performance on HotpotQA and our dataset.",0
1289,All results from our dataset are lower than those observed in HotpotQA while human scores are comparable on both datasets.,0
1289,This suggested that the number of difficult questions in our dataset is greater than that in HotpotQA.,0
1289,Similar to Min et al.,0
1289,"(2019), we used a single-hop BERT model to test the multi-hop reasoning in our dataset.",0
1289,"The result of our dataset is lower than the result of HotpotQA by 23.5 F1, indicating that many examples in our dataset require multi-hop reasoning to be solved.",0
1289,"Through experiments, we confirmed that although our dataset is generated by hand-crafted templates and the set of predefined logical rules, our dataset guarantees that it is challenging for multihop models and requires multi-hop reasoning.",1
1289,"In summary, our main contributions are as follows: (1) We utilise Wikipedia and Wikidata to create a large and high quality multi-hop dataset that has comprehensive explanations from question to answer.",1
1289,"(2) We provide new information in each sample -evidence information useful for interpreting the predictions and testing the understanding, reasoning, and inference skill of the model.",1
1289,(3) We use logical rules to generate a simple natural question but still require the model to undertake multi-hop reasoning when answering a question.,1
1289,"The full dataset, baseline model, and all information that we used when constructing the dataset are available at https://.",0
1315,"Recent studies have found that press releases issued by research institutions are a major source of misinformation in science communication, which is later spread to mainstream media (Woloshin and Schwartz, 2002;Brechman et al., 2009;Sumner et al., 2014;Sumner et al., 2016).",0
1315,"Exaggeration in press releases threatens to undermine public trust in science, which is a foundation for the scientific research enterprise.",0
1315,The influence of press releases on science communication has increased since the 1980s.,0
1315,"With the growing competition for reputation and funding, research institutions are increasingly using press releases as a public relations tool for research promotion (Brechman et al., 2009;Sumner et al., 2014).",0
1315,"At the same time, independent journalism faces financial challenges and staff shortage (Galewitz, 2006;Schwitzer, 2008).",0
1315,"Newspapers, especially small newspapers, rely heavily on press release material to write science news stories (De Semir et al., 1998;Schwitzer, 2008;Woloshin et al., 2009;Taylor et al., 2015).",0
1315,"Prior research also shows that high quality press releases seem to improve the quality of associated news stories (Schwartz et al., 2012).",0
1315,"Since press releases are now the dominant link between academia and news media, the information quality of press releases plays a critical role in communicating science research to the public.",0
1315,The problem of exaggeration in press releases is particularly severe in health research.,0
1315,"For example, over a third of press releases in health research contained exaggerated advice, causal claims from correlational findings, or inference of animal studies to humans (Sumner et al., 2014).",0
1315,"The errors and inaccuracies in reporting health research findings may also misinform the public about their conditions, diagnosis, and treatments.",0
1315,"On the other hand, medicine and health are the dominant topic in science news articles.",0
1315,"research outside the health domain was much less covered in science news (Suleski and Ibaraki, 2010).",0
1315,"Therefore, studying exaggeration in the health domain is particularly important for both science communication and public health.",0
1315,"Researchers and media watchdogs such as HealthNewsReview have been conducting manual content analysis to estimate and monitor press release quality (Smith et al., 2005;Cassels and Lexchin, 2008;Schwitzer, 2008;Sumner et al., 2014).",0
1315,"Due to the large number of press releases and scarce funding, this labor-intensive approach is difficult to maintain.",0
1315,"In fact, HealthNewsReview stopped operation at the end of 2018 after a nearly 13-year run.",0
1315,"Manual content analysis is also inadequate for answering important research questions that require large-scale, real-time analyses.",0
1315,"For example, has the exaggeration problem worsened or improved over the years?",0
1315,Do some institutions exaggerate more than others?,0
1315,Seeking answers to these research questions requires developing a computational approach to automatically analyze exaggeration in press releases.,0
1315,"In this study we propose an NLP approach for automatically detecting exaggerated causal claims, one of the most common types of exaggeration, by comparing claims in press releases to the corresponding claims in the original research papers.",1
1315,An exaggerated causal claim is defined as a causal statement in a press release with a correlational counterpart in the corresponding research paper.,1
1315,Our study consists of several subtasks.,0
1315,"First, we developed a corpus with paper-press pairs.",1
1315,"Press releases were downloaded from EurekAlert, the main online portal for publishing academic press releases.",0
1315,Doi links were obtained from EurekAlert and ScienceDaily to associate the press releases with the original research papers in PubMed.,0
1315,"The corpus includes 61,029 press releases with references to 59,287 PubMedindexed journal papers.",0
1315,"We then trained a classification model to identify 46,103 papers on observational studies; they are designed to establish correlational findings, but are often exaggerated as causal (Woloshin et al., 2009;Sumner et al., 2014;Zweig and DeVoto, 2018).",1
1315,"Furthermore, 14,426 observational studies contained structured abstracts, in which the sentences in the conclusion subsection were used as main statements in research papers.",0
1315,"In press releases, the headline, first, and second sentences are considered as the main statements, according to the ""inverted pyramid"" structure commonly used by news stories.",0
1315,"Some research papers were associated with multiple press releases, resulting in 15,884 paper-press pairs dated from 2008 to May 2020-the final dataset used in our exaggeration analysis.",0
1315,"Second, we developed BERT-based sentence classifiers to categorize the main statements in press releases and research papers respectively.",1
1315,"The sentences were classified by their strength as either ""direct causal"", ""conditional causal"", ""correlational"", or ""not claim"".",0
1315,"Third, by applying the sentence classifiers to the 15,584 paper-press pairs, we were then able to identify the press releases that used direct causal claims to describe correlational findings in observational studies.",1
1315,Then we used the identified exaggeration cases to help answer the following questions: (1) what is the trend of exaggerated causal claims from 2008 to 2020?,0
1315,(2) did university press releases make more exaggerated claims than journal press releases?,0
1317,Would you say leopards are yellow?,0
1317,"Most likely, some people would while others would not.",0
1317,"Both interpretations are valid, as the interpretation depends on a person's boundaries for the properties yellow and brown.",0
1317,"Selecting only one judgment would disregard the vagueness of the expression, a phenomenon at the heart of lexical semantics.",0
1317,"At the same time, most people would probably agree that wine can be red without having to think about it.",0
1317,"A high number of semantic annotation tasks is characterized by unclear, difficult, ambiguous and vague examples.",0
1317,"Annotation, in particular when distributed among a crowd, has the potential of capturing different interpretations, conceptualizations and perspectives and can thus provide highly relevant semantic information.",0
1317,"Existing evaluation and label extraction methods, however, still heavily rely on agreement between annotators, which implies a single correct interpretation.",0
1317,Finished datasets rarely provide indications about difficulty and ambiguity on the level of annotated units.,0
1317,The explanatory power of NLP experiments that aim to evaluate or analyze models depends on the informativeness of the data.,0
1317,"For example, 'diagnostic' experiments (a way of analyzing models) (Belinkov and Glass, 2019) are used to learn more about how non-transparent (deep) learning models behave and what kind of information they can capture in latent representations.",0
1317,"The conclusions drawn from diagnostic classification crucially depend on the quality and informativeness of the underlying data (Hupkes et al., 2018).",0
1317,Diagnostic experiments can be seen as a type of evaluation which goes beyond traditional approaches evaluating system output.,0
1317,"Traditional evaluation datasets, however, should be held to similarly high standards.",0
1317,Having rich and accurate information about instances in an evaluation set can facilitate targeted error analyses and enable insights into general tendencies.,0
1317,"Information about phenomena inherent in most (semantic) tasks, such as ambiguity and varying degrees of difficulty, is valuable information and forms a crucial aspect of the data.",0
1317,"For instance, an analysis of natural language inference models shows that classifiers do not necessarily capture the same type of ambiguity and uncertainty as reflected in the annotations (Pavlick and Kwiatkowski, 2019).",0
1317,Signals of disagreement from crowd annotations can provide valuable insights in model behavior and should thus be included in evaluation sets.,0
1317,"In this paper, we present an approach to crowd-annotation for a diagnostic dataset which attempts to tackle these limitations.",1
1317,The dataset is meant to test which semantic properties are captured by distributional word representations.,0
1317,The task is designed to trigger fine-grained semantic judgements of potentially ambiguous examples.,0
1317,"The behavior of ambiguous words in distributional semantic models is not well understood and thus particularly interesting (Yaghoobzadeh et al., 2019;Del Tredici and Bel, 2015).",0
1317,We investigate to what extent existing and new quality metrics indicate annotation accuracy on the one hand and ambiguity and difficulty of annotation units on the other hand.,0
1317,"We evaluate our task from three perspectives: (1) comparison against an expert-annotated gold standard, (2) a task-specific coherence metric independent of agreement and (3) evaluation in terms of inter-annotator agreement metrics compared to predefined expectations about agreement and disagreement.",0
1317,"In particular, we aim to investigate (1) how we can exploit the strengths and weaknesses of various suggested metrics to select and aggregate labels provided by the crowd, ( 2) to what degree disagreement among workers occurs in cases where it is expected and legitimate and (3) which metrics are suitable for detecting annotation units with legitimate and informative disagreement.",1
1317,"1 Disagreement has been shown to indicate ambiguous cases when measured with the CrowdTruth framework (Aroyo and Welty, 2014;Dumitrache et al., 2018).",0
1317,"However, we are not aware of work which compares different (dis)agreement and difficulty metrics.",0
1317,"To the best of our knowledge, there is no study which tests how well different metrics can be used to identify ambiguous annotation units in a set of units annotated in terms of expected and legitimate disagreement.",1
1317,We show that the metrics we use give complementary insights and can be used to filter and aggregate labels in a way that produces highquality annotations.,1
1317,"Despite a relatively low inter-annotator-agreement, we show that worker behavior follows our expectations about agreement and disagreement and that high-quality labels can be extracted from the annotations, in particular for cases where we expect worker agreement.",1
1317,"The remainder of this paper is structured as follows: After reviewing related work (Section 2), we introduce the use-case of a diagnostic dataset (Section 3) and describe the annotation task (Section 4).",0
1317,We present our expert-annotated gold standard in Section 5 and different quality metrics in Section 6.,0
1317,"The results of our experiments are described in Section 7, followed by a discussion and conclusion.",0
1324,Evaluating a dialogue response is crucial for the development of open-domain dialogue systems.,0
1324,"It allows for comparison between different systems, which is similar to how machine translation community uses BLEU (Papineni et al., 2002) to evaluate the overall quality of the translation and determines whether a system is the state-of-the-art (Bahdanau et al., 2015;Sennrich et al., 2016;Aharoni et al., 2019).",0
1324,"Without automatic evaluation metrics, many studies (Zhang et al., 2018;Zhan et al., 2019;Adiwardana et al., 2020) rely on either expert or crowdsourced evaluation which are both time-consuming and cost ineffective.",0
1324,"Thus, various automatic evaluation metrics have been proposed to score the overall quality of a dialogue response.",0
1324,"Word overlap-based metrics, which were adopted from MT community to measure the overlapping words between reference and candidate sentences, have been used to evaluate the dialogue responses (Sordoni et al., 2015;Zhang et al., 2018).",0
1324,"However, Liu et al.",0
1324,"(2016) showed that these metrics, i.e., BLEU (Papineni et al., 2002), METEOR (Banerjee and Lavie, 2005), or ROUGE score (Lin, 2004), do not correlate well with human judgements, because there are many possible responses to reply to a given context.",0
1324,"Recently, learning-based metrics, which aim to predict the overall quality of a response, achieved a good correlation score with human judgement, compared with word overlap-based metrics.",0
1324,Various training settings have been explored.,0
1324,"For example, ADEM (Lowe et al., 2017a) is trained to predict the score by learning to regress on the human judgements.",0
1324,PONE  is trained with next utterance prediction task with sophisticated samplings.,0
1324,Zhan et al.,0
1324,(2019) combined two submetrics on syntactic and semantic aspect into a single metric.,0
1324,"However, these metrics are not configurable and may suffer from several limitations.",0
1324,"First, they may not capture a particular quality that is essential for a particular task, as shown in Table 1 that BERTScore and BERT-RUBER assigns relatively high score to the unspecific response.",0
1324,"Generally, a single overall score is usually comprised of different qualities, such as readability, specificity, and empathy, and the  Table 1: Examples on how metrics on overall quality may not capture specificity.",0
1324,"B, R, U, and H denotes scores from BERTScore, BERT-RUBER, USL-H (proposed), and human, respectively.",0
1324,importance of each aspect differs according to the task.,0
1324,"For example, specificity is preferred in foodordering chatbots whereas fluency is preferred in language-teaching chatbots.",0
1324,"However, the existing metrics are not flexible to such changes.",0
1324,"BERTScore (Zhang et al., 2020), for example, relies on using pretrained BERT embedding (Devlin et al., 2019) to compute similarity between reference and candidate responses; thus this does not guarantee good correlation for the specificity quality (Table 1).",0
1324,Another limitation is the difficulty in enhancing only a specific aspect of the metric.,0
1324,"Suppose there is a single metric that can capture both sensibleness and specificity, and a new state-of-the-art metric on the latter quality is subsequently developed; it would be complicated to modify the existing metrics (i.e.",0
1324,BLEU or ADEM) to include this new SOTA metric.,0
1324,"Aside from evaluating a response using only a single overall score, some studies (Zhang et al., 2018;Weston et al., 2018;Smith et al., 2020) evaluate the response on various aspects, i.e., fluency, relevancy, specificity, and empathy.",0
1324,"The limitation of this approach is that with multiple scores to consider, it becomes unclear to determine which response is better.",0
1324,Is a specific response more preferable than an empathetic one?,0
1324,"To address these issues, we first propose to simplify the various qualities by grouping them into three main aspects: understandability (Nbel, 1997), sensibleness (Adiwardana et al., 2020), and likability.",1
1324,"We assume these groups have hierarchical properties in the following way: (i) a response is acceptable if it is understandable and make sense to the given context, (ii) other qualities (i.e.",1
1324,"empathy, specificity, or both) are just additional qualities to make the response more likable for a given task.",1
1324,"Second, we propose a simple evaluation metric to combine scores for each aspect together to get USL-H score, which stands for Understandability, Sensibleness, and Likability in Hierarchy.",1
1324,USL-H can be modified to remove or add an additional quality or to replace a sub-metric with a more optimal alternative.,1
1324,This removes the barrier of requiring a single complicated model and instead enables a combination of heuristics with other sub-metrics.,0
1324,"For simplicity, we demonstrate the configurability using only specificity as our likability aspect.",1
1324,"Experimenting on the DailyDialog dataset (Li et al., 2017), we demonstrate that using valid utterance prediction, next utterance prediction, and masked language model as sub-metrics for understandability, sensibleness, and specificity, respectively, and combining them as a single metric using our method achieves a high correlation score with human judgement for both Pearson and Spearman correlations.",1
1324,"Through various experiments, the results show that USL-H can be configured to capture particular qualities of a response, or to replace any sub-metrics of an aspect with a better performing alternative.",1
1324,"The main contributions of this paper are the following: (i) the grouping of various qualities of dialogue responses into three main aspects: understandability, sensibleness, and likability, (ii) introducing a configurable hierarchical evaluation metric that can be modified to work with a sets of response's quality and sub-metrics according to the task while achieving good correlation with human judgements.",1
1331,"As an important fine-grained subtask in the field of sentiment analysis, Aspect-Based Sentiment Classification (ABSC) (Pang et al., 2008;Liu, 2012) aims to detect the sentiment polarities of aspect terms mentioned in review text.",0
1331,"For example, in Fig.",0
1331,"1, given the aspect term food, it is expected to identify its corresponding sentiment polarity as positive.",0
1331,"The main limitation of ABSC lies in that aspect terms need to be annotated before aspect sentiment classification, which is not applicable to real applications.",0
1331,"To address this problem, many studies have been proposed to explore Aspect Term-based Sentiment Analysis (ATSA), which performs aspect term extraction and aspect sentiment classification jointly (Mitchell et al., 2013;Zhang et al., 2015;Luo et al., 2019;Hu et al., 2019).",0
1331,"However, ATSA still suffers from a major obstacle that it only considers explicit aspects but completely ignore implicit aspects in text.",0
1331,Take the review in Fig.,0
1331,1 as an example.,0
1331,"Although the second clause does not mention any aspect term, it clearly expresses user's negative sentiment towards the service.",0
1331,"More importantly, we observe that existing benchmark datasets contain a large amount of such reviews.",0
1331,"For instance, Table 2 shows the proportion of reviews with implicit aspects in the Restaurant dataset from SemEval 2015, and it is clear that nearly 25% of the examples contain implicit aspects.",0
1331,"Since these examples also convey valuable information, we should no longer ignore them, as the previous ATSA methods do.",0
1331,"Motivated by this, we focus on Aspect-Category based Sentiment Analysis (ACSA) in this paper, aiming to perform joint aspect category detection and category-oriented sentiment classification.",1
1331,"Compared with ATSA, ACSA has the following two advantages: On the one hand, for each aspect mentioned in a piece of text, even if it does not have the corresponding aspect term, there must be a corresponding aspect category so that we can identify user's sentiment over it.",1
1331,"On the other hand, from the perspective of application in real scenarios, aspect category detection and category-oriented sentiment classification can meet the demand of opinion summary from multiple aspect level granularity.",1
1331,"However, research in this area is relatively rare, and only a few preliminary studies have been carried out.",0
1331,"(Schmitt et al., 2018) proposed a joint model by extending sentiment labels with one more dimension to indicate the occurrence of each aspect category, which is shown to outperform traditional pipeline methods.",0
1331,"Another feasible solution is to perform Cartesian product for aspect categories and sentiment labels, which essentially performs multi-label sentiment classification for each aspect category.",0
1331,"Nevertheless, most of these methods fail to explicitly model the hierarchical relationship between aspect category detection and category-oriented sentiment classification.",0
1331,"In particular, when there are many aspect categories, it is difficult for these methods to learn the inner-relations among multiple categories and the inter-relations between categories and sentiments.",0
1331,"In this paper, we re-formalize the task as a category-sentiment hierarchy prediction problem, which contains a two-layer hierarchy output structure.",1
1331,"The lower layer is to detect aspect categories, which can be modelled as a multi-label classification problem (i.e., one review may contain more than one category).",1
1331,"The higher layer is to perform category-oriented sentiment classification, which can be modelled as a multi-class classification problem for each detected category.",1
1331,"Under the hierarchy output structure, our model contains three modules: the bottom module leverages BERT to obtain hidden representations of the two sub-tasks respectively.",1
1331,"In the middle module, we propose a Hierarchical Graph Convolutional Network (Hier-GCN), where the lower-level GCN is to model the inner-relations among multiple categories, and the higher-level GCN is to capture the interrelations between categories and category-oriented sentiments.",1
1331,"Based on the interactive representations generated from Hier-GCN, the top module performs category-sentiment hierarchy prediction to generate the final output.",1
1331,We conduct experiments on four benchmark product review datasets from SemEval 2015 and SemEval 2016.,0
1331,Our observations are as follows.,0
1331,"First, the hierarchy output structure shows better performance than other existing structures.",0
1331,"On this basis, the proposed Hier-GCN architecture can bring additional performance gains, and consistently achieves the best results across the four datasets.",0
1331,"Moreover, further analysis shows that Hier-GCN is also superior over existing methods on implicit aspect-based sentiment analysis.",0
1336,Machine Translation (MT) models have achieved state-of-the-art results on several high-resource language pairs.,0
1336,This success is mostly due to advances in modeling and the availability of clean parallel corpora.,0
1336,"Yet, advances in MT on low-resource language pairs are still lagging behind mainly due to the scarcity of parallel training data.",0
1336,"Augmenting with back-translated large monolingual corpora (Sennrich et al., 2016) can partly offset the effects of small amounts of training data, but it is not applicable for pairs where the target language doesn't have large monolingual corpora.",0
1336,"On the other hand, freely available benchmark datasets across different tasks have historically provided reference points for researchers to drive their fields forward.",0
1336,"Notable examples include The General Language Understanding Evaluation (GLUE) (Wang et al., 2018) for NLU and the Conference on Machine Translation (WMT) datasets for MT.",0
1336,"Still, benchmark datasets for most low-resource language pairs remain a much-needed resource.",0
1336,"Arabic dialects, like most low-resource languages, lack freely available benchmark datasets that can be used to evaluate models.",0
1336,This makes previous research results difficult to track and reproduce.,0
1336,"In this paper, we introduce benchmark datasets for evaluation on MT tasks between Arabic dialects, Modern Standard Arabic (MSA) and English.",1
1336,We describe the design considerations and data collection guidelines we adopt.,0
1336,We provide an analysis and an empirical evaluation of state-of-the-art MT models on our benchmark datasets.,0
1336,We explore optimal unsupervised segmentation parameters and introduce a novel data augmentation method which we call bootstrapping.,1
1336,The goal of the project is to provide datasets that are reliable public evaluation benchmarks to track progress in the translation quality across different dialects.,0
1336,"We put a specific emphasis on creating datasets that are not only able to assess the performance of MT systems but also test their robustness on two levels: the domain, and the dialectal diversity.",1
1341,"Neural machine translation (NMT) models (Kalchbrenner and Blunsom, 2013;Cho et al., 2014;Sutskever et al., 2014;Bahdanau et al., 2015;Gehring et al., 2017;Vaswani et al., 2017) have achieved state-of-the-art results and have been widely used in many fields.",0
1341,"Due to numerous parameters, NMT models can only play to their advantages based on large-scale training data.",0
1341,"However, in practical applications, NMT models often need to perform translation for some specific domain with only a small quantity of in-domain data available.",0
1341,"In this situation, continual training (Luong and Manning, 2015), which is also referred to as fine-tuning, is often employed to improve the in-domain translation performance.",0
1341,"In this method, the model is first trained with large-scale general-domain training data and then continually trained with the in-domain data.",0
1341,"With this method, the in-domain performance can be improved greatly, but unfortunately, the general-domain performance decline significantly, since NMT models tend to overfit to frequent observations (e.g.",0
1341,"words, word co-occurrences, translation patterns) in the in-domain data but forget previously learned knowledge.",0
1341,This phenomenon is called catastrophic forgetting.,0
1341,Figure 1 shows the performance trends on the in-domain and general-domain.,0
1341,Many methods have been proposed to address the catastrophic forgetting problem under the scheme of fine-tuning.,0
1341,Freitag and Al-Onaizan (2016) ensembles the general-domain model and the fine-tuned model together so that the integrated model can consider both domains.,0
1341,Dakwale and Monz (2017) introduces domain-specific output layers for both of the domains and thus the domain-specific features of the two domains can be well preserved.,0
1341,Thompson et al.,0
1341,"(2019), Barone et al.",0
1341,"(2017), and  propose regularization-based methods that introduce an additional loss to the original objective to help the model trade off between the general-domain and in-domain.",0
1341,"All these methods show their effectiveness and have mitigated the performance decline on general-domain, but we still don't know what happened inside the model during continual training and why these methods can alleviate the catastrophic forgetting problem.",0
1341,The study on these can help to understand the working mechanism of continual training and inspire more effective solutions to the problem in return.,0
1341,"Given above, in this paper, we focus on the catastrophic forgetting phenomenon and investigate the roles of different model parts during continual training.",1
1341,"To this end, we explore the model from the granularities of modules and parameters (neurons).",1
1341,"In the module analyzing experiments, we operate the model in two different ways, by freezing one particular module or freezing the whole model except for this module.",1
1341,We found that different modules preserve knowledge for different domains.,1
1341,"In the parameter analyzing experiments, we erase parameters according to their importance which is evaluated by the Taylor expansion-based method (Molchanov et al., 2017) .",1
1341,"According to the experimental results, we found that some parameters are important for both of the general-domain and in-domain and meanwhile they change greatly during domain adaptation which may result in catastrophic forgetting.",1
1341,"To ensure the validity and reliability of the findings, we conducted experiments over different language pairs and domains.",0
1341,Our main contributions are summarized as follows:,0
1341,"? We propose two analyzing methods to explore the model from the perspectives of modules and parameters, which can help us understand the cause of catastrophic forgetting during continual training.",1
1341,? We find that some modules tend to maintain the general-domain knowledge while some modules are more essential for adapting to the in-domain.,1
1341,"? We find that some parameters are important for both of the general-domain and in-domain, and their over-change in values may result in performance slipping.",1
1367,This paper surveys Phase I of a project whose primary goal is to serve Indigenous communities in Canada by producing software that would enhance their efforts to preserve and revitalize their languages.,0
1367,"This phase of the project began in March 2017, ended in March 2020, and had funding of $6 million; it involved a collaboration by several research organizations.",0
1367,Phase II is ongoing.,0
1367,"Different communities have very different linguistic needs, so our project was made up of a diverse set of subprojects.",0
1367,"Because there is relatively little textual or speech data for Indigenous languages in Canada (with the partial exception of Inuktut), most of the technologies developed within the project described here have been rule-based, rather than relying on data-driven machine learning.",0
1367,"Different approaches have been taken to linguistic research involving Indigenous languages (Cameron et al., 1992;Czaykowska-Higgins, 2009).",0
1367,"Most of the work carried out within this project fits into the ""empowerment"" approach, in which research is carried out collaboratively, with equal emphasis on the agenda of the linguist and of the community.",0
1367,The most ambitious subproject described here was suggested to us by an Indigenous educator: the creation of a verb conjugator for Kanyen'kha (Mohawk).,1
1367,"Similarly, the ""readalong"" subproject for automating word-speech alignment for audio books was in response to strong interest from several communities.",0
1367,The project was guided by an Advisory Committee made up of Indigenous people with expertise in language revitalization.,0
1367,Their counsel has been invaluable; they are listed at [self-identifying URL].,0
1367,At no stage did the project claim ownership of Indigenous language data collected with the project's funding.,0
1367,"We were determined to break with the unfortunate history of academics and government departments refusing to return linguistic data to the Indigenous communities from which it was collected (see (Pool, 2016), (Keegan, 2019), and (Brinklow et al., forthcoming)).",0
1405,"Modern deep learning techniques typically require a lot of labeled data (Bowman et al., 2015;Conneau et al., 2017).",0
1405,"However, in real-world applications, such large labeled data sets are not always available.",0
1405,"This is especially true in some specific domains, such as the biomedical and materials science domain, where annotating data requires expert knowledge and is usually time-consuming (Karimi et al., 2015;Friedrich et al., 2020).",0
1405,Different approaches have been investigated to solve this low-resource problem.,0
1405,"For example, transfer learning pretrains language representations on self-supervised or rich-resource source tasks and then adapts these representations to the target task (Ruder, 2019;Gururangan et al., 2020).",0
1405,"Data augmentation expands the training set by applying transformations to training instances without changing their labels (Wang and Perez, 2017).",0
1405,"Recently, there is an increased interest on applying data augmentation techniques on sentence-level and sentence-pair natural language processing (NLP) tasks, such as text classification (Wei and Zou, 2019;Xie et al., 2019), natural language inference (Min et al., 2020) and machine translation .",0
1405,"Augmentation methods explored for these tasks either create augmented instances by manipulating a few words in the original instance, such as word replacement (Zhang et al., 2015;Wang and Yang, 2015;Cai et al., 2020), random deletion (Wei and Zou, 2019), or word position swap (S ?ahin and Steedman, 2018;Min et al., 2020); or create entirely artificial instances via generative models, such as variational auto encoders (Yoo et al., 2019;Mesbah et al., 2019) or back-translation models (Yu et al., 2018;Iyyer et al., 2018).",0
1405,"Different from these NLP tasks, named entity recognition (NER) makes predictions on the token level.",0
1405,"That is, for each token in the sentence, NER models predict a label indicating whether the token belongs to a mention and which entity type the mention has.",0
1405,"Therefore, applying transformations to tokens may also change their labels.",0
1405,"Due to such a difficulty, data augmentation for NER is comparatively less studied.",0
1405,"In this work, we fill this research gap by exploring data augmentation techniques for NER, a token-level sequence labeling problem.",0
1405,Our contributions can be summarized as follows:,0
1405,We survey previously used data augmentation techniques for sentence-level and sentence-pair NLP tasks and adapt some of them for the NER task.,0
1405,"We conduct empirical comparisons of different data augmentation methods using two domainspecific data sets: MaSciP (Mysore et al., 2019) and i2b2-2010 (Uzuner et al., 2011).",1
1405,Results show that simple augmentation can even improve over a strong baseline with large-scale pretrained transformers.,1
1405,"In this section, we survey previously used data augmentation methods for NLP tasks, grouping them into four categories:",0
1405,Word replacement Various word replacement variants have been explored for text classification tasks.,0
1405,Zhang et al.,0
1405,"(2015) and Wei and Zou (2019) replace words with one of its synonyms, retrieved from an English thesaurus (e.g., WordNet).",0
1405,Kobayashi (2018) replace words with other words that are predicted by a language model at the word positions.,0
1405,Xie et al.,0
1405,(2019) replace uninformative words with low TF-IDF scores with other uninformative words for topic classification tasks.,0
1405,"For machine translation, word replacement has also been used to generate additional parallel sentence pairs.",0
1405,replace words in both the source and the target sentence by other words uniformly sampled from the source and the target vocabularies.,0
1405,Fadaee et al.,0
1405,"(2017) search for contexts where a common word can be replaced by a low-frequency word, relying on recurrent language models.",0
1405,Gao et al.,0
1405,"(2019) replace a randomly chosen word by a soft word, which is a probabilistic distribution over the vocabulary, provided by a language model.",0
1405,"In addition, there are two special word replacement cases, inspired by dropout and masked language modeling: replacing a word by a zero word (i.e., dropping entire word embeddings) (Iyyer et al., 2015), or by a [MASK] token (Wu et al., 2018).",0
1405,Mention replacement Raiman and Miller (2017) augment a question answering training set using an external knowledge base.,0
1405,"In particular, they extract nominal groups in the training set, perform string matching with entities in Wikidata, and then randomly replace them with other entities of the same type.",0
1405,"In order to remove gender bias from coreference resolution systems,  propose to generate an auxiliary dataset where all male entities are replaced by female entities, and vice versa, using a rule-based approach.",0
1405,Swap words Wei and Zou (2019) randomly choose two words in the sentence and swap their positions to augment text classification training sets.,0
1405,Min et al.,0
1405,"( 2020) explore syntactic transformations (e.g., subject/object inversion) to augment the training data for natural language inference.",0
1405,S ?ahin and Steedman (2018) rotate tree fragments around the root of the dependency tree to form a synthetic sentence and augment low-resource language part-of-speech tagging training sets.,0
1405,(2018) train a question answering model with data generated by backtranslation from a neural machine translation model.,0
1405,Kurata et al.,0
1405,(2016) and Hou et al.,0
1405,(2018) use a sequence-to-sequence model to generate diversely augmented utterances to improve the dialogue language understanding module.,0
1405,"convert data from a high-resource language to a lowresource language, using a bilingual dictionary and an unsupervised machine translation model in order to expand the machine translation training set for the low-resource language.",0
1405,"3 Data Augmentation for NER Inspired by these efforts described in Section 2, we design several simple data augmentation for NER.",0
1405,"Note that these augmentation do not rely on any external trained models, such as machine translation models or syntactic parsing models, which are by themselves difficult to train in low-resource domainspecific scenarios.",0
1405,"Label-wise token replacement (LwTR): For each token, we use a binomial distribution to randomly decide whether it should be replaced.",1
1405,"If yes, we then use a label-wise token distribution, built from the original training set, to randomly select another token with the same label.",1
1405,"Thus, we keep the original label sequence unchanged.",1
1405,"Taking the instance in Table 1 as an example, there are five tokens replaced by other tokens which share the same label with the original tokens.",0
1405,"Our second approach is similar to LwTR, except that we replace the token with one of its synonyms retrieved from WordNet.",1
1405,Note that the retrieved synonym may consist of more than one token.,0
1405,"However, its BIO-labels can be derived using a simple rule: If the replaced token is the first token within a mention (i.e., the corresponding label is 'B-EntityType'), we assign the same label to the first token of the retrieved multi-word synonym, and 'I-EntityType' to the other tokens.",0
1405,"For each mention in the instance, we use a binomial distribution to randomly decide whether it should be replaced.",1
1405,"If yes, we randomly select another mention from the original training set which has the same entity type as the replacement.",1
1405,The corresponding BIO-label sequence can be changed accordingly.,1
1405,"For example, in Table 1, the mention 'headache [B-problem]' is replaced by another problem mention 'neuropathic pain syndrome [B-problem I-problem I-problem]'.",0
1405,We first split the token sequence into segments of the same label.,1
1405,"Thus, each segment corresponds to either a mention or a sequence of out-of-mention tokens.",1
1405,"For example, the original sentence in Table 1 is split into five segments: [She did not complain of], [headache], [or], [any other neurological symptoms], [.].",1
1405,"Then for each segment, we use a binomial distribution to randomly decide whether it should be shuffled.",1
1405,"If yes, the order of the tokens within the segment is shuffled, while the label order is kept unchanged.",1
1405,All We also explore to augment the training set using all aforementioned augmentation methods.,1
1405,"That is, for each training instance, we create multiple augmented instances, one per augmentation method.",1
1412,"Research on strategies for producing referring expressions has often investigated the differences (if any) between spoken and written language, but as we will show in Section 2, results have been inconclusive.",0
1412,"Sometimes, claims are simply contradictory, but the more important problem is that usually, the exact ways of measuring the properties of coreference chains are not being made transparent.",0
1412,"In addition, the data that has been used can vary considerably, and it is not always clear how studies can be compared.",0
1412,"Our primary goal here is to shed light on coreference with respect to the spoken/written distinction, by undertaking a careful comparative corpus analysis and explicitly stating our methods of measurement.",0
1412,"The secondary goal is to explore how the medium microblog, specifically Twitter, relates to the spokenwritten spectrum for coreference strategies.",0
1412,"While Twitter has been applied widely for NLP tasks such as sentiment analysis or information extraction, it has received very little attention on the side of coreference so far.",0
1412,"To illustrate, we used an out-of-the-box contemporary coreference resolver (Lee et al., 2018), set to coarse-to-fine inference, and obtained an F1 score of 72.6% on the standard OntoNotes test data (Pradhan et al., 2013), and in contrast a score of only 45.2% on the corpus of Twitter conversations.",0
1412,"In an earlier study, Aktas ?et al.",0
1412,"( 2018) outlined peculiar anaphoric cases in Twitter conversations, such as exophoric references to non-linguistic content in pictures attached to the messages, or mixed personal pronominal references to the same entity due to the nature of multi-user conversations.",0
1412,Our paper goes a step further and situates coreference strategies found in Twitter conversations in a comparative empirical study of coreference in spoken versus written texts.,1
1412,"For the latter, we build on (Aktas ?et al., 2019), who presented a quantitative study on different genre sections of the OntoNotes corpus.",0
1412,"We extend their study by broadening the data base in two directions: We augment the rather small proportion of spoken data in Ontonotes with the Switchboard corpus (Godfrey et al., 1992), and we add the comparison with Twitter texts, thus achieving both broader coverage of speech and a wider range of production media.",1
1412,"Although Switchboard and OntoNotes have previously both been used for investigating coreference, to our knowledge they have not yet been systematically compared.",0
1412,"Accordingly, an important part of our work is in harmonizing the data sets and the underlying annotation schemes, to enable a sensible analysis.",0
1412,"We will show genre-specific distributional patterns of nominal referring expressions in terms of frequency of syntactic categories, heaviness of NP structures, and relative distance between anaphors and antecedents in the text.",1
1412,"Most of our analyses lead to a common ranking of the genres, as discussed in Section 5.",0
1412,"In addition to the theoretical value, we believe that the observed patterns can provide a basis for data-driven design of automated coreference resolution tools that perform better on spoken or Twitter language than the current ""out of the box"" systems do.",0
1412,The following section presents related work.,0
1412,Section 3 introduces data sources and corpus alignment strategies we applied to harmonize the data.,0
1412,"Section 4 presents the measures we used for the comparison, and the results of the analyses, which are further discussed in Section 5, concentrating on the comparison of the genres and production media.",0
1412,Section 6 summarizes and shows directions for future work.,0
144,"In various assessment fields, essay-writing tests have attracted much attention as a way to measure practical higher-order abilities such as logical thinking, critical reasoning, and creative-thinking skills (Hussein et al., 2019).",0
144,"In essay-writing tests, test-takers are required to write essays about a given topic, and human raters grade those essays based on a scoring rubric.",0
144,"However, because the scoring process takes much time and effort, it is hard to grade large numbers of essays (Hussein et al., 2019).",0
144,"Further, subjectivity in human scoring can reduce accuracy (Amorim et al., 2018).",0
144,"Automated essay scoring (AES), which utilizes natural language processing and machine learning techniques to automatically grade essays, is one method for resolving these problems.",0
144,"Many AES methods have been developed over the past decades, and can generally be categorized as feature-engineering and neural-network approaches (Hussein et al., 2019;Ke and Ng, 2019).",0
144,"The featureengineering approach predicts scores using handcrafted features such as essay length or spelling errors (e.g., (Amorim et al., 2018;Dascalu et al., 2017;Mark D. Shermis, 2016;Nguyen and Litman, 2018)).",0
144,The advantages of this approach include interpretability and explainability.,0
144,"However, this approach generally requires extensive effort for engineering effective features to achieve high scoring accuracy for various essays.",0
144,"To obviate the need for feature engineering, a neural-network approach that automatically extracts features using deep neural networks (DNNs) has recently attracted attention.",0
144,"Many DNN-AES models have been proposed and have achieved high accuracy (Alikaniotis et al., 2016;Taghipour and Ng, 2016;Dasgupta et al., 2018;Farag et al., 2018;Jin et al., 2018;Mesgar and Strube, 2018;Wang et al., 2018;Mim et al., 2019;Nadeem et al., 2019).",0
144,"These two approaches can be viewed as complementary rather than competing, because they provide different advantages.",0
144,"Specifically, the neural-network approach can extract dataset-specific features from word sequence patterns, whereas the feature-engineering approach can use existing effective features that are difficult to extract using DNNs from only word sequence information.",0
144,"To obtain both benefits, Dasgupta et al.",0
144,(2018) proposed a hybrid method that integrates both approaches.,0
144,This method is formulated as a DNN-AES model with an additional recurrent neural network (RNN) that processes a sequence of handcrafted sentence-level features.,0
144,"This method provides state-of-the-art accuracy, but has the following drawbacks:",0
144,It cannot incorporate effective essay-level features developed in previous AES researches.,0
144,"It greatly increases the numbers of model parameters and tuning parameters, increasing the difficulty of model training.",0
144,"It has an additional RNN that processes sequences of handcrafted sentence-level features, enabling extension to various DNN-AES models complex.",0
144,"To resolve these problems, we propose a new hybrid method that integrates handcrafted essay-level features into a DNN-AES model.",1
144,"Specifically, our method concatenates handcrafted essay-level features to a distributed essay representation vector, which is obtained from an intermediate layer of a DNN-AES model.",1
144,The advantages of our method are as follows:,0
144,It can incorporate various existing essay-level features for which effectiveness has been shown.,1
144,"The number of required additional parameters is only the number of incorporated essay-level features, and there are no additional hand-tuned parameters.",1
144,"It can be easily applied to various DNN-AES models, because conventional models commonly have a layer that produces a distributed essay-representation vector.",1
144,"Our model is a simple DNN-AES extension, but experimental results on real-world benchmark data show that it significantly improves accuracy.",0
1508,The task of Bilingual Dictionary Induction is defined as finding target language translations of source language words.,0
1508,"It is an important building block in the area of Machine Translation (MT) and it is one of the main tasks for bilingual word embedding evaluation (Mikolov et al., 2013b;Vulic and Korhonen, 2016).",0
1508,"Recent approaches showed that good performance can be achieved relying only on BWEs, which can be built with only a weak bilingual signal, such as a small seed lexicon of a few thousand word pairs (Mikolov et al., 2013b) or common tokens in the source and target languages (Artetxe et al., 2017).",0
1508,"In addition, they can even be built without any bilingual signal  making them the basis of unsupervised MT systems Artetxe et al., 2019).",0
1508,"Standard BDI learns word representations based on approaches that exploit solely word-level information such as word2vec (Mikolov et al., 2013a) or fasttext (Bojanowski et al., 2017) and then map them to a shared BWE space.",0
1508,"Although BWE-based approaches show high BDI performance, they struggle with a subset of hard-to-translate words such as named entities, for which orthographic information should be used instead of semantic information.",0
1508,Several approaches have integrated orthographic information into the BDI system.,0
1508,Heyman et al.,0
1508,(2017) relied on character-level information in their classification based BDI system by using an RNN architecture.,0
1508,Braune et al.,0
1508,(2018) combined orthographic information with BWE-based word similarity information using an ensembling approach.,0
1508,"Both of these approaches showed improved results but they relied on Levenshtein distance to get translation candidates for given source words during prediction, which is not applicable for language pairs with different scripts.",0
1508,"To bridge the gap between languages with different scripts, a transliteration system was employed in (Severini et al., 2020).",0
1508,"They followed the approach of (Braune et al., 2018) but used the transliteration system instead of Levenshtein distance to get candidates used in the ensembling model.",0
1508,"On the other hand, as they also showed, the ensembling approach often fails to decide correctly if a given word has to be transliterated or not; this is because there are only two independent scores available, the score of the (semantic) BWE, and the score of the transliteration model.",0
1508,"In this paper, we present our novel approach to BDI focusing on words that have to be transliterated to another script, which is especially important for low-frequency words, but also relevant for highfrequency named entities.",1
1508,"Our aim is to improve BDI systems in two aspects: (i) eliminating the need for language specific orthographic information, such as is used in Levenshtein distance, and (ii) to be able to better decide when to choose transliteration over semantic translation.",1
1508,We propose a new approach for language pairs with different scripts by combining semantic information with orthographic information.,1
1508,"For the latter we introduce Bilingual Orthographic Embeddings (BOEs) of words, which represent transliteration pairs in the source and target language with similar vectors.",1
1508,We build BOEs using a novel transliteration system trained jointly for both language directions.,1
1508,We refer to this novel system as seq2seqTr.,0
1508,seq2seqTr is also used to extract candidate transliterations for given source words and it is applicable to any language pair as opposed to Levenshtein distance.,1
1508,"To make a more informed decision about which words should be transliterated (which means we shoud primarily trust the BOEs) and which should be translated (which means we should primarily trust the BWEs), we use a classification approach similar to (Heyman et al., 2017).",0
1508,"In contrast to their approach, we use additional features, such as frequency, length, similarity scores, and the ranks assigned by the semantic and character-level submodels, and show that they are necessary to make the right decision.",1
1508,"We test our system on the English-Russian (En-Ru) data provided in the BUCC 2020 shared task (Rapp et al., 2020).",0
1508,"Test dictionaries were released in three frequency categories: high, middle and low.",0
1508,"We evaluate our system on all three sets, both separately and jointly, and show improved performance on all three frequency ranges compared with previous approaches.",0
1508,"Furthermore, we show that our classification system is more robust than the ensembling of (Severini et al., 2020), which required specialized tuning on each frequency set.",0
1508,"Lastly, we conduct a further analysis of the quality of the proposed BOEs by running transliteration mining on the NEWS 2010 shared task data (Kumaran et al., 2010) by using the vector similarity of Bilingual Orthographic Embeddings of words.",0
1508,We show good performance on the task indicating the usefulness of BOEs for other downstream tasks.,0
1535,Grammarians have for a long time (e.g.,0
1535,"ini, 6th Century BCE; Aristotle, 350 BCE) been interested in characterising what is and is not possible within a given specific language.",0
1535,"Famously, Chomsky (1957) used the sentence colorless green ideas sleep furiously and its reversal furiously sleep ideas green colorless to illustrate the difference between grammatical and ungrammatical within English.",0
1535,"In addition, modern linguistics has also been concerned more generally with what is and is not possible within the space of all human languages.",0
1535,"In this article, we investigate the ability of a neural language model to learn structures on the impossible side of this boundary.",1
1535,"That is, we train and test the network on data which has been manipulated to contain unnatural structures.",1
1535,"In contrast, the evaluation of such models has usually focused on the other side of the boundary, i.e.",0
1535,on data gathered from real human behaviour within specific natural languages.,0
1535,"Here, we are interested in the more general question of whether these neural architectures are appropriate models of human language abilities, in terms of their ability to differentiate between possible and impossible structures.",1
1535,"From a theoretical perspective, a number of grammatical formalisms have been proposed as computational models of the space of possible natural languages: from Context-Free Grammars (Chomsky, 1956) to Tree-Adjoining Grammars (Joshi et al., 1969) and Combinatory Categorial Grammars (Steedman, 1987).",0
1535,"One characteristic that all such grammars share is the organisation of linguistic expressions into hierarchical, as opposed to purely linear, structures.",0
1535,Empirical research has also uncovered further design patterns common across many superficially diverse languages.,0
1535,"Although the concept of language universals is not uncontroversial, most linguist would agree that, at the very least, natural languages display strong tendencies and similarities.",0
1535,"For example, languages typically have a preferred word order -e.g.",0
1535,Subject Verb Object vs. Subject Object Verbwhich is correlated with the ordering of other constructions -e.g.,0
1535,prepositions vs. postpositions.,0
1535,"Alongside the observation that children learn their native languages efficiently from limited, noisy input, these shared principles have been been used to support the view that the human language faculties are innately adapted to learn and process one specific set of possible languages.",0
1535,"Or, to put it another way, there are structures that are essentially impossible for these innate faculties to handle.",0
1535,"Recently, however, domain-general neural sequence models, lacking explicit linguistic inductive biases, have demonstrated impressive performance on a range of practical language processing applications.",0
1535,"This, in turn, has led to increasing interest in the question of how effective these architectures are as models of general human linguistic behaviour.",0
1535,"Whereas our interest here is in their behaviour when trained on unnatural structures, most research has so far been interested in their ability to successfully learn the structure of natural languages.",0
1535,"For example, Gulordava et al.",0
1535,"(2018) examined the extent to which a Long Short Term Memory (Hochreiter and Schmidhuber, 1997) network trained on raw text learned about agreement phenomena across a number of languages.",0
1535,"They found that, given the right training data, the model was able to predict long-distance agreement on both natural and semantically nonsensical but syntactically valid sentences.",0
1535,"Similarly, Futrell et al.",0
1535,(2019) investigated the ability of such models to track syntactic state within subordinations and garden path sentences.,0
1535,"Treating their networks as psycholinguistic subjects, they found that given enough data the models responded to subtle cues in a manner comparable to human subjects.",0
1535,"These articles suggest that although an LSTM handles its input in a purely sequential manner, it is able, nonetheless, to learn about phenomena that have traditionally been represented within hierarchical data structures.",0
1535,"Frank and colleagues (Frank, 2009;Frank and Bod, 2011;Frank et al., 2012;Frank et al., 2015) suggest that, in fact, such sequential models have advantages over hierarchical alternatives.",0
1535,"Using evidence from reading time and ERP studies, they argue that the ability to accurately model the probabilities of words in context is more important in a psycholinguistic model than the particular architecture employed.",0
1535,"Given these successes in language learning, Pater (2019) has proposed a complementary role for neural networks alongside generative linguistics.",0
1535,He argues that these architectures may supply the theory of learning that linguistics currently lacks.,0
1535,"In response, Rawski and Heinz (2019) invoke the no-free-lunch theorems (Wolpert and Macready, 1997) and poverty-of-the-stimulus arguments (Chomsky, 1986) to question whether neural models actually have the right inductive biases.",0
1535,"Here, we investigate this question by evaluating a network's ability to learn number agreement within pathological structures unlike any natural language.",1
1535,"While failure to learn natural structures would give important insights into a model's limitations, success on unnatural structures is also revealing, indicating a mismatch between the model's architecture and human linguistic processes.",0
1535,We therefore train and test an LSTM on manipulated English sentences and compare this performance to that on the original unmanipulated corpus.,1
1535,"Based on prior research using unnatural data, we employ three types of modification: reversing word order, inserting a linear sequence of tokens, and shuffling the vocabulary.",1
1535,Introducing reversed constructions alongside the original sequences disrupts a language's preference for a consistent word order and so is an effective way of producing unnatural data.,1
1535,"Moreover, the human experiments described in Section 2 suggest that learners struggle to acquire artificial languages constructed in this way.",0
1535,"Learners also struggle with constructions based on sequential counting, e.g.",0
1535,inserting a word in the nth position regardless of the syntactic context.,0
1535,"Human languages universally base their syntax around hierarchical structures, and so constructions based on counting and linear order are entirely unnatural.",0
1535,"While word order preferences and hierarchical structure are language specific characteristics differentiating natural from unnatural data, machine learning theorists have also proposed domain general methods for generating unnatural data.",0
1535,"In this regard, shuffling provides a useful means for for analysing the limits of learning algorithms.",0
1535,"In particular, fully randomised shuffling of the training labels in a supervised learning task forces an algorithm to resort to memorisation of the training data, supplying insight into its capacity.",1
1535,"In our experiments, we employ a limited form of shuffling, where generalisation is still possible, but which increases the computational load on the model substantially.",0
1535,We describe these experiments and their results in Section 3.,0
1535,Following that we analyse in more detail how knowledge about number agreement is encoded in the weights of the network in Section 4.,0
1535,"First, however, we describe some of the prior work on unnatural data in Section 2.",0
1550,"With the rise of social media, exchange of opinions and news happens faster than ever.",0
1550,"News circulation is therefore less and less bound to traditional print journalism that usually requires extensive research, fact checking and accurate coverage in order to be a reliable news resource.",0
1550,It is relatively easy to share opinions that are either not supported by researched facts or simply wrong.,0
1550,In the worst case a large amount of people can be targeted by propaganda in order to shift societal discussions in favor of a wanted agenda.,0
1550,"Human resources are limited to identify such Fake News, since they cover a wide range of topics and linguistic writing styles (Shu et al., 2017, p.2).",0
1550,Automated Fake News detection (FND) has therefore proven to be an important challenge for NLP researchers in recent years.,0
1550,"To this day, a variety of approaches dealing with FND exists (Khan et al., 2019).",0
1550,"In 2017, the Fake News Challenge Stage 1 was introduced which tackles FND as a stance detection task (Pomerleau and Rao, 2017), where the idea is to determine the stance of a news article to a given headline (Hanselowski et al., 2018, p.1).",0
1550,"We now evaluate the five models BERT (Devlin et al., 2018), RoBERTa (Liu et al., 2019), DistilBERT , ALBERT (Lan et al., 2019) and XLNet  which were developed and enhanced recently.",0
1550,All architectures were pre-trained on large unlabeled corpora using a self-supervised objective and can be fine-tuned on a desired task at hand.,0
1550,Our main focus is to evaluate the necessity of hyperparameter tuning as well as the general performance.,1
1550,"1 Besides this, it is of special interest to examine the differences between auto-encoding (BERT-based models) and auto-regressive architectures (XLNet).",1
1550,"In this context, the term Fake News is defined as a text piece that is verifiably wrong and spread with a malicious intention.",0
1550,"In doing so, other media sources such as video, images or audio are excluded.",0
1550,Since the intention has to be malicious all sorts of entertainment related false news such as hoaxes and april fools are excluded.,0
1550,The definition is similar to the narrow definition that Shu et al.,0
1550,(2017) undertake.,0
1550,2 The Fake News Challenge (FNC-1),0
1550,"In 2017, the Fake News Challenge Stage 1 (FNC-1) was published.",0
1550,"Organizers from industry and academia created an online challenge accessible via http://www.fakenewschallenge.org/ (Pomerleau and Rao, 2017).",0
1550,The FNC-1 is conceptualized as an important pre-step in identifying Fake News and exploring how artificial intelligence tools can be leveraged in combatting them.,0
1550,"Given a certain claim about a topic, what are different news agencies reporting about this claim?",0
1550,"If most news agencies agree with a claim, this can be interpreted as an indicator of the truthfulness of the claim.",0
1550,"On the contrary, if a lot of news disagree with the claim, the claim is likely Fake News.",0
1550,"Statistically speaking this idea is translated into a stance detection task with the claim being treated as a headline and the stance of the article body being either Agree, Disagree, Discuss or Unrelated.",0
1550,FND is thus treated as a classification task with four categories which are interpreted as the stance of an article body towards a given headline claim.,0
1550,"Along with a baseline model, a training and test set was published.",0
1550,According to Hanselowski et al.,0
1550,"(2018), 50 teams participated in the challenge.",0
1566,Scientific documents are a valuable source of information for both physicians and researchers in medical sciences.,0
1566,"Relevant information is continuously produced, more than 3,000 articles are published every day (Tsatsaronis et al., 2012).",0
1566,"Manually finding relevant information in this huge among of data is an enormous challenge (Sarrouti and El Alaoui, 2017).",0
1566,Passage retrieval methods can alleviate the manual scanning of documents by automatically finding a subset of relevant passages that speed-up and improve the search process.,0
1566,Their goal is to return the highest correlated passages that conform to a valid answer for a given question.,0
1566,Metric learning has been broadly used in face identification and other image processing tasks.,0
1566,This approach has a powerful and simple mathematical formulation that allows to produce a compact representation in a metric space that can be used to identify image correspondences.,0
1566,The same idea can be applied to the passage retrieval task where answer passages should share semantic patterns with the question and this can be measured by a metric in an appropriate metric space.,0
1566,"This idea has not been explored in depth in the context of passage retrieval, except for the work of (Bonadiman et al., 2019), where a siamese network was used for learning a metric between questions and candidate answers in an open-domain question answering task on a proprietary dataset.",0
1566,This paper presents a novel deep metric learning method that learns a metric between question and passages bringing close semantically related pairs.,1
1566,Most of the metric learning approaches learn to embed samples in a latent space where a metric (usually Euclidean) captures relationships between samples.,0
1566,The proposed approach directly learns the metric fusing different similarity measures through a siamese convolutional deep learning architecture.,1
1566,"Also, the paper presents a sampling strategy that chooses easy and then hard negative samples in the training phase, improving the overall model performance.",1
1566,The experimental results show that the method is able to induce a metric between questions and passages that helps to discriminate relevant passages from non-relevant passages.,0
1566,"The proposed architecture is similar to a triplet network (because of the three inputs: question, answer passage, non-answer passage) and also to a siamese architecture because it is composed of two convolutional neural networks with shared weights.",1
1566,"However, different from these, it allows to extract important semantic features from several question-passage internal similarity measures that provide a complementary view of their relatedness.",1
1566,"The similarity measures include a structured view of the question and passage, incorporating valuable information that is usually available in close domain problems.",0
1566,"To validate the model performance we carried out a systematic evaluation considering a widely used domain-specific collection, the BioASQ dataset (Tsatsaronis et al., 2012), and comparing it against stateof-the-art models.",0
1566,The results show that the performance of the proposed model outperforms previous approaches with a wide margin.,0
1566,The main contributions of this work are the following:,0
1566,? We formulate a novel deep metric learning architecture which encodes question-passage semantic interactions improving state-of-the-art performance in biomedical passage retrieval.,1
1566,? We develop an informative sample filtering method that helps to identify easy and hard negative samples to be used during training leading to faster convergence and better performance.,1
1566,"It is important to highlighted that the proposed model could be easily implemented, and the number of its parameters is much less than in the state-of-the-art models (Brokos et al., 2018), which have in the order of millions while ours in the order of thousands.",1
1566,"The paper is organized as follows: Section 2 discusses the related work in Biomedical passage retrieval; Section 3 shows the details of the proposed metric learning method; Section 4 present the sampling strategy; Section 5 presents a systematic evaluation of the method; Section 6 discusses the results against the state of the art models; finally, Section 7 exposes some conclusions and discusses our future work ideas.",0
1567,The form-meaning association relating words to their senses is a fundamental component of language.,0
1567,"Hence, the representation of the meaning of words is an important research question in computational linguistics.",0
1567,Processing word meaning allows interpreting larger units such as phases and sentences.,0
1567,"Therefore, computational lexical semantics is, explicitly or implicitly, at the core of higher-level NLP tasks such as textual understanding, information extraction, and automatic summarisation.",0
1567,"Much effort has been put in the manual and semi-automatic construction of resources encoding lexical semantics (i.e., word meaning).",0
1567,"These include semantic lexicons with inventories of possible senses that lexical units can assume, e.g., Wordnet (Miller et al., 1990) and Babelnet (Navigli and Ponzetto, 2012), as well as sense-annotated corpora specifying which of these senses are employed in context, e.g., SemCor (Landes et al., 1998) and Eurosense (Delli Bovi et al., 2017).",0
1567,"Because fine-grained sense distinctions are generally hard to annotate and predict (Navigli, 2009), coarse-grained tagsets, referred to as supersenses, were proposed as alternatives (Ciaramita and Johnson, 2003;Schneider et al., 2016), with positive impact on downstream applications, e.g., (Agirre et al., 2011;Flekova and Gurevych, 2015).",0
1567,"Alternatively, real-numbered vectors can encode contextual co-occurrence, acting as a proxy for a lexical unit's semantics (Harris, 1954).",0
1567,"This principle has guided the development of numerous distributional semantic models, that is, semantic vector representations inferred from corpus co-occurrences (e.g., Landauer and Dumais (1997)).",0
1567,"Advances in neural networks shifted the focus of computational semantics to representation learning, so as to obtain vectors as by-products of neural networks (Mikolov et al., 2013).",0
1567,"In this booming field, a myriad of models have emerged to encode sub-lexical information (Bojanowski et al., 2017), polysemy (Neelakantan et al., 2014), and context (Peters et al., 2018;Devlin et al., 2018).",0
1567,"These models are efficiently learned from corpora, benefiting from high-performance neural architectures and libraries.",0
1567,"Thus, vector representations, rebranded word embeddings, have become the dominant technique to represent lexical units, at the core of state-of-the-art neural approaches.",0
1567,"Traditional static embeddings, such as word2vec and Fasttext, assume that each word's meaning can be represented as a single vector, independently of its context.",0
1567,"While generic and reusable, these models usually conflate the different meanings of a given unit into a single vector (Camacho-Collados and Pilehvar, 2018).",0
1567,"Contextual models, such as ELMo, GPT-2, BERT, and their variants, encode each word's occurrence as a context-dependent vector, assuming that each context corresponds to a different sense (Yarowsky, 1993).",0
1567,"In short, while static models create one generic embedding per lexical unit, contextual models provide a fine-grained distinct representation for each occurrence.",0
1567,"Both models, but especially the latter, are increasingly complex and opaque (Rogers et al., 2020), requiring advanced techniques to help humans understand their strengths and limitations (Jawahar et al., 2019;Serrano and Smith, 2019).",0
1567,"Given this landscape, we introduce SLICE: an alternative semantic model which represents a trade-off between interpretable symbolic senses, static and contextual word embeddings.",1
1567,"We propose a weakly supervised technique to build dense low-dimensional embeddings whose dimensions represent coarsegrained semantic classes i.e., supersenses (Sec.",1
1567,Our lightweight model embeds both lexical units and their contexts into the same semantic space.,1
1567,"Thus, words and their contexts are represented as two compact vectors of directly interpretable scores, one per supersense, automatically learned from a non annotated corpus.",1
1567,Our embeddings are assessed in a word sense disambiguation (WSD) setting (Sec.,0
1567,"Thanks to the model's interpretability, we are able to perform a rich linguistic analysis of WSD results, providing insightful results to understand the model's predictions (Sec.",0
1570,"Research on object naming (Ordonez et al., 2016;Graf et al., 2016;Eisape et al., 2020), especially the linguistic analysis of the naming behavior of humans and computational models, requires natural and reliable naming data.",0
1570,"The collected data should account for language variation, or the fact that an individual object can be called by different names.",0
1570,"At the same time, humans need to agree that the collected name(s) can be used to refer to a given object, that is, disagreements in naming should be evidence for true naming variation.",0
1570,"To achieve both naturalness and control, previous work in Language & Vision (L&V) used data collection methods that prompt speakers to freely talk about or refer to given objects in an image (Kazemzadeh et al., 2014;Yu et al., 2016;Silberer et al., 2020).",0
1570,"Common to the collection setups is their use of images as a proxy for the actual context of language use, and simple bounding boxes to indicate the target, which allows for large-scale data collection, leveraging existing Computer Vision datasets.",0
1570,"However, this kind of simulation also risks introducing confounding factors, such as referential uncertainty (uncertainty about which object is the target) or visual uncertainty (when objects are difficult to recognize).",0
1570,Silberer et al.,0
1570,"(2020) followed this setting to create ManyNames, a large-scale dataset of naming data for objects from 7 different domains (e.g., animals, people, food) 1 .",0
1570,"It builds upon object annotations in Visual Genome (Krishna et al., 2016), and provides up to 36 free name annotations for each of 25K objects in images.",0
1570,"To crowdsource the names, Silberer et al.",0
1570,presented subjects with an image and asked them to name the target object that was marked by a bounding box.,0
1570,"The authors showed tentative evidence that free name annotations result in quite a bit of naming variation, with an average of roughly 3 names per object.",0
1570,"However, they also observe the presence of naming errors in the data that prevented them from drawing conclusions about naming behavior, in humans and in computational models.",0
1570,"Our own data inspection indeed showed errors such as subjects naming a different object from the one highlighted by the box, or the right object but with a clearly incorrect name (see Figure 2 for examples).",0
1570,"In this work, we assess the factors that affect the collection of valid naming data in the typical L&V setup, and test whether these factors are similarly reflected in the effectiveness of a L&V object labeling model.",1
1570,"To obtain valid data, we verified ManyNames via crowdsourcing.",0
1570,"First, we analyze the verification data and show that a) the main problem are referential mistakes (subjects naming an object other than the target); b) while naming variation is reduced when removing noise, it is still substantial, as objects in ManyNames have an average of 2.2 names after noise removal; c) both the effect of confounding factors and the resulting naming variation are domain-dependent.",0
1570,"Based on our analysis, we obtain a more reliable and valid version of the dataset, ManyNames v2, which we will publicly release upon publication.",0
1570,We then introduce a diagnostic evaluation that uses ManyNames for the analysis of L&V object naming models.,1
1570,"This evaluation moves beyond the single-label setup that is common in computer vision and L&V by considering not only a single label, but all the valid names in ManyNames.",1
1570,"Moreover, it provides valuable information by establishing whether the name predicted by the model coincides with the most frequent human response, a less frequent but still valid name, or a mistake (and, in the latter case, of which kind).",1
1570,"We showcase the potential of this evaluation method by analyzing the performance of a popular L&V model, Bottom-up (Anderson et al., 2018), on object naming.",1
1570,"The model is trained towards object detection on Visual Genome, which provides a single ground-truth name for each object.",0
1570,"To the best of our knowledge, Bottom-up has not been tested directly in terms of its naming predictions.",0
1570,"Our experimental results show that single-label evaluation greatly underestimates the naming capabilities of Bottom-Up, which actually come close to our estimated human upper bound (88% vs. 91%).",1
1570,"However, its effectiveness varies across domains.",1
1570,"We furthermore show that the aspects that are confusing for models overlap with those that are confusing for humans, but they are not identical.",1
1570,"Differences exist particularly in domains that that are very familiar and relevant in human daily life (people, clothing and food), which suggests that the gap between humans and models regarding understanding and modeling human language use in and about the real world is larger than a superficial look may suggest.",1
1598,"Unsupervised pretraining models, such as GPT and GPT-2 (Radford et al., 2018;Radford et al., 2019), ELMo (Peters et al., 2018), and BERT (Devlin et al., 2019) yield state-of-the-art performance on a wide range of natural language processing tasks.",0
1598,All these models rely on language modeling (LM) objectives that exploit the knowledge encoded in large text corpora.,0
1598,"BERT (Devlin et al., 2019), as one of the current state-of-the-art models, is pretrained on a joint objective consisting of two parts: (1) masked language modeling (MLM), and (2) next sentence prediction (NSP).",0
1598,"Through both of these objectives, BERT still consumes only the distributional knowledge encoded by word co-occurrences.",0
1598,"While several concurrent research threads are focused on making BERT optimization more robust  or on imprinting external world knowledge on its representations (Sun et al., 2019a;Zhang et al., 2019;Sun et al., 2019b;Liu et al., 2020;Peters et al., 2019;Wang et al., 2020, inter alia), no study yet has been dedicated to mitigating a severe limitation that contextualized representations and unsupervised pretraining inherited from static word embeddings: every model that relies on distributional patterns has a tendency to conflate together pure lexical semantic similarity with broad topic relatedness (Schwartz et al., 2015;Mrk?i? et al., 2017).",0
1598,"In the past, a plethora of models have been proposed for injecting linguistic constraints (i.e., lexical knowledge) from external resources to static word embeddings (Faruqui et al., 2015;Wieting et al., 2015;Mrk?i? et al., 2017;Ponti et al., 2018, inter alia) in order to emphasize a particular lexical relation in a specialized embedding space.",0
1598,"For instance, lexically informed word vectors specialized for pure semantic similarity result in substantial gains in a number of downstream tasks where such similarity plays an important role, e.g., in dialog state tracking (Mrk?i? et al., 2017;Ren et al., 2018) or for lexical simplification .",0
1598,"Existing specialization methods are, however, not directly applicable to unsupervised pretraining models because they are either (1) tied to a particular training objective of a static word embedding model, or (2) predicated on the existence of an embedding space in which pairwise distances can be modified.",0
1598,"In this work, we hypothesize that supplementing unsupervised LM-based pretraining with clean lexical information from structured external resources may also lead to improved performance in language understanding tasks, especially in tasks where distinguishing between pure semantic similarity and conceptual relatedness is paramount.",0
1598,"We propose a novel method to inject linguistic constraints, available from lexico-semantic resources like WordNet (Miller, 1995) and BabelNet (Navigli and Ponzetto, 2012), into unsupervised pretraining models, and steer them towards capturing word-level semantic similarity.",1
1598,"To train Lexically Informed BERT (LIBERT), we (1) feed semantic similarity constraints to BERT as additional training instances and (2) predict lexico-semantic relations from the constraint embeddings produced by BERT's encoder (Vaswani et al., 2017).",1
1598,"In other words, LIBERT adds lexical relation classification (LRC) as the third pretraining task to BERT's multi-task learning framework.",1
1598,"We compare LIBERT to a lexically blind ""vanilla"" BERT on the GLUE benchmark (Wang et al., 2018) and report their performance on corresponding development and test portions.",0
1598,"LIBERT yields performance gains over BERT on 9/10 GLUE tasks (and is on a par with BERT on the remaining one), with especially wide margins on tasks involving complex or rare linguistic structures such as Diagnostic Natural Language Inference and Linguistic Acceptability.",0
1598,"Moreover, we assess the robustness and effectiveness of LIBERT on 3 different datasets for lexical simplification (LS), a task proven to benefit from word-level similarity specialization .",0
1598,We report LS improvements of up to 8.2% when using LIBERT in lieu of BERT.,0
1598,"For direct comparability, we train both LIBERT and BERT from scratch, and monitor the gains from specialization across iterations.",0
1598,"Interestingly, these do not vanish over time, which seems to suggest that our specialization approach is suitable also for models trained on massive amounts of raw text data.",0
1617,Metaphors are prevalent in our everyday language even without our consciousness of its presence as we speak and write.,0
1617,"It induces the unknown using the known, explains the complex using the simple, and helps us to emphasize the relevant aspects of meaning resulting in effective communication.",0
1617,"In general, metaphor involves certain concept transfer from one domain (Source) to another (Target), as in 'sweet voice' (using taste to describe sound).",0
1617,Lakoff (1980) describes metaphor as a cognitive mechanism (a property of language) reflected by our conceptual system for structuring our understanding of the world.,0
1617,"It is a fundamental way to relate our physical and familiar social experiences to a multitude of other subjects and contexts (Lakoff and Johnson, 2008).",0
1617,"To better understand the intrinsic properties of metaphors and to provide an in-depth analysis to this phenomenon, we propose a linguistically-enriched deep learning model extending one published work (WAN et al., 2020) at ACL Figlang 2020 workshop by incorporating the modality norms into attention-based BiLSTM.",1
1617,"As a continuation of their work, we conduct the current research to further testify the effectiveness of leveraging conceptual norms for metaphor detection by incorporating the modality norms into an attention-based neural network.",1
1617,"For standard reference, we adopt the dataset of the first and second shared tasks of metaphor detection on verbs of the VUA corpus  1 .",0
1617,Details about the experiment are given in Sections 3-5.,0
1644,"A very comprehensive definition of text readability was given by (Dubay, 2007) as the ease of reading a text created by the choice of content, style, structure and organization that meets prior knowledge, reading ability, interest and motivation of the audience.",0
1644,"Tracking the automation of readability back to its origin, we find the first readability formulas a century ago, in the United States, with the aim of helping to select reading material for classes by teachers, librarians, and scholars (Davison and Green, 1988)  (Bohn, 1990).",0
1644,"At that time, it was considered that complexity could be inferred by surface-level metrics of words and sentences, based on the frequency and size (number of letters) of the words and on the average number of words per sentence.",0
1644,"Since then, readability analysis has become a large area of multidisciplinary research, with an ever growing literature, related tasks (e.g., text simplification task (Vajjala and Meurers, 2014a) and text summarization task (Vodolazova and Lloret, 2019)), and has gained new computational approaches in this century with the use of Natural Language Processing (NLP) and Machine Learning methods (Collins-Thompson, 2014).",0
1644,"Traditionally, the task has been applied on the text level, assigning a grade (or level of proficiency ranking) for an entire document.",0
1644,"However, in a document classified as simple, complex sentences can occur, just as there are simple sentences in a complex document.",0
1644,"A sentence is an important unit that brings, in most cases, enough information for inference and analysis of its complexity.",0
1644,"Although it is possible to use the same approach to assess the complexity of texts at the sentence level, (Dell'Orletta et al., 2014) demonstrated that a greater number of features are needed for readability prediction at the sentence level.",0
1644,"The work of (Gonzalez-Gardu?o and S?gaard, 2018) has achieved state-of-the-art performance in readability prediction of English sentences, using multi-task learning and eye-tracking measures.",0
1644,"This paper presents a thorough evaluation of sentence readability prediction in Brazilian Portuguese (BP), starting with the evaluation of single-task methods, followed by a replication of the work developed by (Gonzalez-Gardu?o and S?gaard, 2018) and, in the end, we propose a new model based on sequential transfer learning approach (Ruder et al., 2019) which achieved state-of-the-art performance in readability prediction of BP sentences.",1
1644,Section 2 presents a literature review of the main works in readability prediction at sentence level (RPSL).,0
1644,Section 3 details the corpora and metrics used and Section 4 presents the models evaluated and experimental results.,0
1644,"Section 5 presents an analysis of the main errors of our best model, followed by a revision of the evaluation dataset and final results of our final best model.",0
1644,Section 6 brings the conclusions and future works.,0
1661,Dependency parsing is the task of assigning a syntactic structure to a sentence by linking its words with binary asymmetrical typed relations.,0
1661,"In addition to syntactic information, dependency representations encode some semantic aspects of the sentence which make them important to downstream applications including sentiment analysis (Tai et al., 2015) and information extraction (Miwa and Bansal, 2016).",0
1661,In this paper we are interested in Arabic dependency parsing for which two formalisms have been developed (See 2).,0
1661,"The first is the Columbia Arabic Treebank (CATiB) representation (Habash and Roth, 2009), which is inspired by Arabic traditional grammar and which focus on modeling syntactic and morpho-syntactic agreement and case assignment.",0
1661,"The second is the Universal Dependency (UD) representation (Taji et al., 2017), which has relatively more focus on semantic/thematic relations, and which is coordinated in design with a number of other languages (Nivre et al., 2017).",0
1661,"While previous work on Arabic dependency parsing (Marton et al., 2013;Taji et al., 2017) tackled these formalisms separately, we argue that they stand to benefit from multitask learning (MTL) (Caruana, 1993).",0
1661,MTL allows for more training data to be exploited while benefiting from the structural or statistical similarities between the tasks.,0
1661,We therefore propose to learn CATiB and UD dependency trees jointly on the same input sentences using parallel treebanks.,1
1661,Deep neural networks are particularly suited for multitask scenarios via straightforward parameter and representation sharing.,0
1661,Some hidden layers can be shared across all tasks while output layers are kept separate.,0
1661,"In fact, most deep learning architectures for language processing start with sequential encoding components such as BiLSTMs or Transformer layers which can be readily shared across multiple tasks.",0
1661,"This approach is widely applicable even to tasks that use very different formalisms and do not have parallel annotations (S?gaard and Goldberg, 2016;Hashimoto et al., 2017).",0
1661,"This type of sharing has also been shown to benefit (semantic and syntactic) dependency parsing, both transition-based (Stymne et al., 2018;Kurita and S?gaard, 2019) and graph-based (Sato et al., 2017;Lindemann et al., 2019).",0
1661,"In addition to simple parameter sharing, joint inference across multiple tasks has been shown to be beneficial.",0
1661,Peng et al.,0
1661,"(2017) perform decoding jointly across multiple semantic dependency formalisms with cross-task factors that score combinations of substructures from each (Peng et al., 2017).",0
1661,Joint inference however comes with increased computational cost.,0
1661,"To address this issue, we introduce a multitask learning model for dependency parsing ( 3.2).",1
1661,"This model is based on greedy arc selection similar to the neural easy-first approach proposed in (Kiperwasser and Goldberg, 2016) ( 3).",1
1661,We use tree-structured LSTMs to encode substructures (partial trees) in each formalism which are then concatenated across tasks and scored jointly.,1
1661,"Hence, we model interactions between substructures across tasks while keeping computational complexity low thanks to the easy-first framework.",1
1661,"Furthermore, this approach enables the sharing of various components between tasks, a richer sharing than the mere sequential encoder sharing found in most multitask systems (Kurita and S?gaard, 2019).",1
1661,Our multitask architecture outperforms the single-task parser on both formalisms ( 4.3).,0
1661,"Beside efficiency, the tree-structured LSTM easy-first framework provides several advantages which makes it appealing in our settings.",0
1661,New arc selection decisions are conditioned on encoded representations of partially parsed structures in both formalisms with the latest information at each step.,1
1661,"Since some word attachments are harder to find in one formalism than the other (longer range, ambiguous relations, etc.",0
1661,"), we suppose that looking at the substructure involving such a word in one formalism may help make better decisions in the other.",0
1661,"We do not need to postulate any priority between the tasks nor that all attachment decisions must be taken jointly which is computationally expensive, we leave the exact flow of information to be learned by the model.",0
1661,"Additionally, (Kurita and S?gaard, 2019) showed that even when no easy-first strategy is hard-wired into their multitask semantic dependency parser, it gets nevertheless learned from the data in a reinforcement learning framework.",0
1661,Some possible enhancement to our model are explored in 6.,0
1661,"Summary of contributions In this paper, (i) we propose a new multitask dependency parsing algorithm, based on easy-first hierarchical tree LSTMs, capable of decoding a sentence into multiple formalisms; (ii) we show that our joint system outperforms the single-task baseline on both CATiB and UD Arabic dependency treebanks; and (iii) demonstrate experimentally that linguistic information available in each formalism helps make better predictions for the other by showing that the parser learns to leverage information from each dimension to parse the other dimension better.",1
1661,2 Linguistic Background: CATiB vs. UD,0
1661,The CATiB and the Arabic UD treebanks are currently the two largest Arabic dependency treebanks.,0
1661,"As both treebanks are in dependency representations, that leads to them sharing a number of similarities.",0
1661,"However, there are a few differences between the two treebanks stemming from the granularity of their tag sets and their specific definitions of dependencies.",0
1661,"Granularity of tags One of the design features of CATiB is fast annotation (Habash and Roth, 2009), hence it has only six POS tags and eight dependency relations.",0
1661,"On the other hand, UD aims to accommodate constructions in universal languages (Nivre et al., 2017), and therefore have finer-grained tagsets with 17 POS tags and 37 basic dependency relations.",0
1661,"Naturally, a tag in CATiB, whether a POS tag or a dependency relation, may correspond to a number of tags in UD.",0
1661,Figure 1 illustrates that mapping through a number of examples.,0
1661,"UD's noun (NOUN), adjective (ADJ), and number (NUM) tags correspond to CATiB's nominal (NOM) tag.",0
1661,"Similarly, when it comes to modifiers in UD, if they are headed by a verb then they are oblique nominals (OBL), if they are headed by a noun then they can be nominal modifiers (NMOD), adjectival modifiers (AMOD), or numeric modifiers (NUMMOD), depending on the modifier's POS tag.",0
1661,"However, in CATiB, they are all modifiers (MOD).",0
1661,"On the other hand, the Idafa relation (IDF) in CATiB is also a nominal modifier in UD, but for this particular structure UD uses the possessive subtype (NMOD:POSS) to distinguish it from other nominal modifiers.",0
1661,The philosophical difference between CATiB and UD is in how they define a dependency relation.,0
1661,"CATiB focuses on modeling the assignment of case to make the tree structures closer to traditional Arabic grammar analysis (Habash and Roth, 2009).",0
1661,"As a result, function words tend to head their phrase structures.",0
1661,"On the other hand, UD aims to minimize the differences between languages with different morphosyntactic structures, and therefore focus on the meaning (Nivre, 2016).",0
1661,This often makes content words the heads of phrase structures.,0
1661,"We can see some of those differences in the examples in Figure 1, the most prominent of which may be prepositional phrase constructs.",0
1661,"In CATiB, particles head these phrases since they modify the case assignment of the words that follow them.",0
1661,"In contrast, particles in UD attach low under the content head of these phrases to keep the focus on the semantic meaning of the sentences.",0
1661,"Due to these differences and similarities in representation, we hypothesize that parsing the two treebank formalism along side each other will help improve both parsing outcomes.",0
1681,Machine translation is a complex task which requires diverse linguistic knowledge.,0
1681,"The seemingly straightforward translation of the English pronoun it into German requires knowledge at the syntactic, discourse and world knowledge levels for proper pronoun coreference resolution (cr).",0
1681,"A German pronoun can have three genders, determined by its antecedent: masculine (er), feminine (sie) and neuter (es).",0
1681,"Previous work (Hardmeier and Federico, 2010;Miculicich Werlen and Popescu-Belis, 2017;Mller et al., 2018) proposed evaluation methods for pronoun translation.",0
1681,This has been of special interest in context-aware nmt models that are capable of using discourse-level information.,0
1681,"Despite promising results, the question remains: Are transformers (Vaswani et al., 2017) truly learning this task, or are they exploiting simple heuristics to make a coreference prediction?",0
1681,"To empirically answer this question, we extend ContraPro (Mller et al., 2018)-a contrastive challenge set for automatic EnglishGerman pronoun translation evaluation-by making small adversarial changes in the contextual sentences.",1
1681,Our adversarial attacks on ContraPro show that context-aware Transformer nmt models can easily be misled by simple and unimportant changes to the input.,1
1681,"However, interpreting the results obtained from adversarial attacks can be difficult.",0
1681,"The results indicate that nmt uses brittle heuristics to solve cr, but it is not clear what those heuristics are.",0
1681,"In general, it is challenging to design attacks based on modifying ContraPro that can test specific phenomena that may be of interest.",0
1681,"For this reason, we propose an independent set of templates for coreferential pronoun translation evaluation to systematically investigate which heuristics are being used.",1
1681,"Inspired by previous work on cr (Raghunathan et al., 2010;Lee et al., 2011), we create a number of templates tailored to evaluating the specific steps of an idealized cr pipeline.",1
1681,"We call this collection Contracat ( ), Contrastive Coreference Analytical Templates.",1
1681,The templates are constructed in Start: Original sentence,0
1681,The cat and the actor were hungry.,0
1681,was hungrier.,0
1681,Step 1: Markable Detection,0
1681,The cat and the actor were hungry.,0
1681,was hungrier.,0
1681,Step 2: Coreference Resolution,0
1681,The cat and the actor were hungry.,0
1681,It ( ) was hungrier.,0
1681,Step 3: Language Translation Der Schauspieler und die Katze waren hungrig.,0
1681,Er / Sie ( ) / Es war hungriger.,0
1681,Table 1: A hypothetical cr pipeline that sequentially resolves and translates a pronoun.,0
1681,"a completely controlled manner, enabling us to easily create large number of coherent test examples and provide unambiguous conclusions about the cr capabilities of nmt.",0
1681,The procedure we used in creating the templates can be adapted to many language pairs with little effort.,0
1681,Our results suggest that transformer models do not learn each step of a hypothetical cr pipeline.,1
1681,We also present a simple data augmentation approach using fine-tuning.,0
1681,"The experimental results show that this approach improves scores and robustness on some of our metrics, but it does not fundamentally change the way cr is being handled by nmt.",0
1681,We will publicly release ContraCAT and the adversarial modifications to ContraPro.,0
1694,"In this paper, we analyze the use of Patronizing and Condescendig Language (PCL) towards vulnerable communities in the media.",0
1694,An entity engages in PCL when its language use shows a superior attitude towards others or depicts them in a compassionate way.,0
1694,This effect is not always conscious and the intention of the author is often to help the person or group they refer to (e.g.,0
1694,"by raising awareness or funds, or moving the audience to action).",0
1694,"However, these superior attitudes and a discourse of pity can routinize discrimination and make it less visible (Ng, 2007).",0
1694,"Moreover, general media publications reach a large audience and we believe that unfair treatment of vulnerable groups in such media might lead to greater exclusion and inequalities.",0
1694,"The modelling of PCL, and especially PCL towards vulnerable communities, has not yet been considered in NLP, to the best of our knowledge.",0
1694,"While there has been substantial work on modelling language that purposefully undermines others, e.g.",0
1694,"offensive language or hate speech (Zampieri et al., 2019;Basile et al., 2019), the use of PCL in the media is commonly unconscious.",0
1694,PCL is often also subtler and more subjective than the types of discourse that are typically targeted in NLP.,0
1694,"Within a broader setting, there has been some work on PCL which is concerned with the communication between two parties, where one is patronized by the other, such as in social media interactions.",0
1694,"In particular, Wang and Potts (2019) recently published the Talkdown corpus for condescension detection in comment-reply pairs from Reddit.",0
1694,"In this work, the authors highlight the difficulty of the task and the need for a high-quality dataset annotated by experts, which is the approach we take for studying PCL towards vulnerable communities.",1
1694,"In particular, to encourage more research on detecting PCL language, we introduce the Don't Patronize Me!",1
1694,"This dataset contains around 5,000 paragraphs extracted from news stories, which have been annotated to indicate the presence of PCL at the text span level.",1
1694,"The paragraphs were selected to cover English language news sources from 20 different countries, covering different types of vulnerable communities (e.g.",1
1694,"homeless people, immigrants and poor families).",1
1694,"We furthermore propose a taxonomy of PCL categories, focused on PCL towards vulnerable communities.",1
1694,Each of the PCL text spans from our dataset has been annotated with a category label from this taxonomy.,1
1694,"Finally, we also provide some analysis of the dataset.",0
1694,"Among others, we find that even simple baselines are able to detect PCL to some extent, which suggests that this task is feasible for NLP systems, despite the subtle nature of PCL.",0
1694,"On the other hand, we also find that the considered models, including approaches based on BERT (Devlin et al., 2019), struggle to detect certain categories of PCL, suggesting that there is still considerable room for improvement.",0
1694,"In particular, while some forms of PCL can be detected by identifying relatively simple linguistic patterns, many other cases seem to require a non-trivial amount of world knowledge.",0
1697,Automatic evaluation metric plays a vital role in evaluating system performance for the task of document summarization.,0
1697,"Challenges remain in the design of an ideal evaluation metric and the off-the-shelf metrics have their own drawbacks (Schluter, 2017;Kryscinski et al., 2019).",0
1697,"The widely adopted metrics, e.g.",0
1697,"ROUGE (Lin, 2004), are reference-based in that they compare the output of some summarizer (namely peer summary) with one or multiple human-authored summaries (namely reference/model summary).",0
1697,"The reference-free metrics are still not mature enough to be utilized for evaluation in a real world setting since their correlations with human judgments have been reported to fall far behind reference-based metrics especially for multi-document summarization (Peyrard et al., 2017;Gao et al., 2020)  1 .",0
1697,"In this paper, we consider a new protocol of reference-based summarization metrics by rethinking the role of source document which is indeed a lost treasure neglected by most previous works.",1
1697,"Furthermore, a specific implementation of the protocol (i.e.",0
1697,anchored version of ROUGE) will be discussed.,0
1697,"The reference-based metrics that already exist typically pursue a kinda computation of overlap between peer and reference summary either at a lexical level (Lin, 2004) or at a semantic level (Ng and Abrecht, 2015;Sun and Nenkova, 2019;Zhang et al., 2019a).",0
1697,"However, to our knowledge, few of them consider the impact of source document (or documents in multi-document summarization) to the computation.",0
1697,This goes against common sense as source document is the true information source of both summaries and can be utilized to boost the discriminative power of metrics.,0
1697,"Therefore, we advance a new protocol of reference-based metrics for the evaluation of document summarization.",1
1697,"More specifically, the direct participation of source document is a necessity to compute any reference-based metric for document summarization.",1
1697,This makes source document endorse a certain metric and the advantage lies with the ability to fact-check the information of peer summary based on the information pool (i.e.,1
1697,source document).,0
1697,The protocol change is illustrated in Fig.,0
1697,Metrics designed under the new protocol are called "active metrics" since they will be able to refer to the source.,1
1697,"In a word, the new protocol has introduced a key dimension that can nurture reference-based summarization metrics.",1
1697,"For a verification purpose, we propose an anchored version of ROUGE metric under the new protocol.",1
1697,The anchors here mean a set of lexical items (called particles) in source document corresponding to a certain particle in the summary.,1
1697,"Utilizing anchor set in the computation of ROUGE can introduce a weighted scheme that focuses more on key particles, as will be detailed in the next section.",0
1697,"2 A Specific Implementation: Anchored ROUGE Following the new protocol, ROUGE metric can be revised by introducing anchor set for each particle (i.e.",1
1697,lexical item such as n-gram and skipping bigram) in both peer and reference summaries.,0
1697,"The anchor set for a particle in the summary comprises k particles in source document, each of which is a good match for the summary particle.",1
1697,"In other words, anchor set serves as the grounds of summary particles.",1
1697,"We build the anchor set A s for summary particle s following the two steps: (1) compute the cosine similarity of embedding vectors of s and d with d being any arbitrary document particle (s and d should be of the same lexical form such as bigram); (2) extract top-k document particles based on similarity to form the anchor set, i.e.",1
1697,"A s = {d s1 , d s2 , ..., d sk }.",1
1697,"Also, we record the similarity as the strength of anchor and denote the strength between s and d si as q si (1  i  k).",1
1697,The embedding vector of the particle in this paper is obtained by averaging the contextualized embeddings of all tokens occurring in the particle.,1
1697,"Specifically, in the following experiment, we will sum the last four hidden layers of the pretrained uncased BERT Base model 2 (Devlin et al., 2019) to get the embedding for each token (dimension of embedding vector is 768).",1
1697,An example of anchor set can be found in Fig.,0
1697,The anchored version of ROUGE metric can be defined as follows once all the anchor sets for summary particles (both in peer and reference) have been built.,0
1697,We calculate the union of anchor sets for all particles in reference summary and denote it as C ref .,1
1697,1 gives the formula of anchored ROUGE and function T is defined by Eqn.,0
1697,"Notice that notation ""RefSumm"" is a collection of reference summaries, w d is the count of particle d (with stemming) occurring in source document and  is Kronecker delta function (assigned to 1 only when two relevant variables are equal).",0
1697,"The above anchored metric has based the computation on anchor sets which reside in source document, as compared with traditional ROUGE metric (Lin, 2004).",0
1697,"Function T replaces the count of summary particle, which is adopted in traditional ROUGE, and sums the weighted contributions from different summary particles (the weight coefficient is the anchor strength q si as shown in Eqn.",1
1697,"1, w d assigns a larger weight to more significant particle d. The min function is utilized to compute the matching degree based on document particle d (thus the overall metric will be less than one), which refines the measure of matching count in traditional ROUGE.",1
1697,"Based on these manipulations, anchored ROUGE is endorsed by source document, whose evaluation efficacy will be tested in the next section.",0
1697,"= refRefSumm dC ref w d ? min(T (d, peer), T (d, ref)) refRefSumm dC ref w d ? T (d, ref) ,(1)",0
1697,"T (d, summ) = ssumm i=k i=1 d si As  d,d si ? q si , for summ  {peer, ref}.",0
1697,(2) R-1 R-2 R-1-WE R-2-WE BERTScore S 3 f ull S 3,0
1697,3 Evaluation Efficacy of Anchored ROUGE Datasets.,0
1697,"We select two datasets of topic-focused multi-document summarization (MDS), i.e.",0
1697,"TAC 2008 3 and TAC 2009 4 , for two main reasons: (1) MDS is more challenging than single document summarization and summarizers tend to behave more differently for evaluation, which fits the purpose to examine various metrics; (2) multiple reference summaries are offered, which makes it possible to perform robustness test (see Table 2).",0
1697,"The two datasets consist of 48 and 44 topics, respectively, each of which has a set of 10 source documents and 4 reference summaries (n is 4).",0
1697,"We only use document set A of official datasets in line with (Louis and Nenkova, 2013;Gao et al., 2020).",0
1697,"Additionally, TAC 2008 has 57 peer summaries for each topic while TAC 2009 has 55.",0
1697,"All summaries are at most 100 words and every peer summary has a Pyramid score (Passonneau et al., 2005), which serves as the human judgment.",0
1697,For tuning the anchor set size (i.e.,0
1697,"2), another dataset (DUC 2007 5 ) will be used.",0
1697,Comparing metrics.,0
1697,These reference-based metrics are involved in the experiment.,0
1697,"(1) ROUGE (Lin, 2004): a traditional metric for counting lexical-level overlap.",0
1697,Two variants are considered based on either unigram (R-1) or bigram (R-2).,0
1697,"( 2) ROUGE-WE (Ng and Abrecht, 2015): a metric based on word2vec embeddings (Mikolov et al., 2013) to compute semantic similarity.",0
1697,ROUGE-WE with unigram (R-1-WE) and bigram (R-2-WE) are computed.,0
1697,"(3) BERTScore (Zhang et al., 2019a): a direct metric computing token similarity with BERT embeddings.",0
1697,"(4) S 3 f ull and S 3 best (Peyrard et al., 2017): two learned metrics that combine different sets of existing metrics.",0
1697,"( 5) Mover (Zhao et al., 2019): a contextualizedembedding-based metric using Word Mover's Distance (Kusner et al., 2015).",0
1697,We report its best version with BERT embeddings and certain methods for fine-tuning and aggregation of embeddings according to the original paper.,0
1697,( 6) ROUGE-anchored: our metric proposed under the new principle as formulated in Sec.,0
1697,"Similar to ROUGE, we consider two variants with particle granularities being unigram (AncR-1) and bigram (AncR-2).",0
1697,Tuning on DUC 2007 sets the anchor set size to 5.,0
1697,"Following the convention, we compute the average summary-level correlation with human judgments for each metric in terms of three correlation coefficients: Pearson r, Spearman  and Kendall  .",0
1697,Main results.,0
1697,"As shown in Table 1, the overall correlation results prove the superiority of our anchored ROUGE metric.",0
1697,"On both datasets, anchored ROUGE has achieved the highest correlations according to all three correlation coefficients.",0
1697,"More specifically, AncR-1 and AncR-2 have a correlation higher than  Table 2: Correlations computed with n references.",0
1697,their traditional counterparts (i.e.,0
1697,"R-1 and R-2) and the gaps are over 2.5 and 1.3 percent, respectively.",0
1697,"Even the most recent metric based on advanced contextualized embeddings, i.e.",0
1697,"Mover, has fallen behind our metric (by over one percent as compared with AncR-1 on TAC 2008 and AncR-2 on TAC 2009).",0
1697,"For a more convincing comparison, we have conducted the pairwise Williams significance test recommended by (Graham, 2015) between our metric (more precisely AncR-1 on TAC 2008 and AncR-2 on TAC 2009) and other competitors and the result shows that the increases in correlations of our metric over others except the supervised metric S 3 best are statistically significant (p-value < 0.05).",0
1697,Hyperparameter effect & Robustness.,0
1697,Two extra tests have been performed to further analyze our metric.,0
1697,Effects of anchor set size k on Pearson correlations are illustrated in Fig.,0
1697,"3, indicating that an anchor set with a proper size is needed to establish the efficacy of our metric.",0
1697,The correlations deteriorate when k is less than three and we see no substantial improvements with an extremely large k that causes more intensive computation.,0
1697,Effect of the number of reference summaries is shown in Table 2.,0
1697,We have used all available references to compute metrics when n is equal to four and used n randomly selected references with a smaller n (note that the average of 4 n results is reported).,0
1697,"The observation is that our metric is relatively robust to n and it demonstrates that our metric is less prone to the reference noise observed in (Kryscinski et al., 2019) or the reference bias introduced when very few reference summaries are available (Hermann et al., 2015;Grusky et al., 2018).",0
1752,"Language is a constantly changing system by nature as it is also a method of communication and if we consider language as a social instrument, it should meet the needs of its speakers and it changes according to changes in society and the ever-changing world.",0
1752,"As language units, words also evolve over time, undergoing diachronic (or temporal) semantic shifts which influence the relations between their forms and meanings (Traugott and Dasher, 2001).",0
1752,"Tracing semantic shifts can be important either as an independent linguistic research or for subsequent practical applications, for example, in socio-linguistic research.",0
1752,Manual analysis of such changes require a lot of time and labor for specialists even after the appearance of large representative corpora as it is needed to look through a lot of examples and lexicographic resources which often do not record current lexical changes in language due to limited resources.,0
1752,"Thus, researchers are trying to model these processes using advanced computational approaches often based on distributional semantics and dense word embeddings Tang, 2018).",0
1752,"However, this research is still mostly conducted for English: often simply because manually annotated test data is not available for a particular language.",0
1752,"Recently, consistently annotated lexical semantic change test sets for other languages started to appear; see, for example, (Schlechtweg et al., 2020).",0
1752,"In this paper, we continue this vein of work by presenting RuSemShift.",0
1752,"RuSemShift is the first historical semantic change dataset for Russian annotated in the DURel framework (Schlechtweg et al., 2018) using a large crowd-sourcing platform, instead of personal intuitions of particular researchers.",1
1752,It allows to evaluate semantic change detection systems by their ability to estimate the lexical shifts which occurred to Russian words either after 1917 (the fall of the Russian Empire) or after 1990 (the fall of the Soviet Union).,1
1752,The rest of the paper is organized as follows: in Section 2 we put our research in the context of the related work.,0
1752,In Section 3 we explain the process of choosing words for annotation and the corpora that were used as sources of contexts.,0
1752,Section 4 describes the annotation itself.,0
1752,In Section 5 we evaluate several semantic change detection algorithms (based on static and contextualized embeddings) on RuSemShift to check its sanity.,0
1752,Section 6 summarizes our contributions and outlines future research.,0
1775,Various theoretical and empirical works have addressed the question of which factors play a role in the choice of referring expressions.,0
1775,"One of the main ideas in this tradition (henceforth; the linguistic tradition) is that there is a direct relationship between the ""prominence"" (in a broad sense) that a referent has at a given point in the discourse, and the form that is used to refer to the referent at that point.",0
1775,"If a referent is highly prominent, a short anaphoric form (e.g.",0
1775,"a pronoun) suffices; if it is less prominent, longer forms with more semantic content are used.",0
1775,"Prominence has been argued to be influenced by various factors such as recency and frequency of mention (Ariel, 1990), grammatical function (Brennan, 1995), animacy (Fukumura and van Gompel, 2011), distance (McCoy and Strube, 1999) and competition between the referents (Arnold and Griffin, 2007).",0
1775,"Reference production is also one of the most-studied topics in Natural Language Generation (Gatt and Krahmer, 2018), where it is known as Referring Expression Generation (Krahmer and van Deemter, 2019).",0
1775,"A key part of the REG problem is deciding which form (e.g., proper name, definite description or pronoun) is employed to refer to a target referent at a given point in the discourse.",0
1775,Henceforth we call this task Selection of Referential Form (SRF).,0
1775,"SRF models come in many shapes and forms, with feature-based machine learning models playing a dominant role.",0
1775,"However, the feature sets employed by these models can differ considerably from one model to the next, and although ""linguistic"" features akin to the ones employed in the linguistic tradition are often used, other types of features, which are harder to interpret linguistically, are frequent as well.",0
1775,"Our aim in this paper is to examine feature-based SRF models from a linguistic perspective: We will conduct a systematic evaluation of these models, asking what features make these models work best.",1
1775,"Having done this, we propose a ""consensus"" feature set; we consider this to be useful because it will enable linguists to compare these algorithms to their own theories and insights, so that features may be enhanced or replaced by other features in the future.",1
1775,"Finally, we compare the features in our consensus feature set against the factors considered to be important in the linguistic tradition.",1
1775,"An important question in any systematic evaluation is how the objects of study (in our case, SRF feature sets) are selected.",0
1775,We have proceeded as follows:,0
1775,"? We selected all SRF algorithms submitted to GREC (Belz et al., 2010) and extracted the feature sets used by these algorithms.",0
1775,"GREC was a Shared Task Evaluation task that still forms a natural starting point, because it attracted all the main SRF algorithms that existed at the time.",0
1775,"? We re-implemented all features that were obtained in this way, following the method detailed in section subsection 3.2.",0
1775,"Note that our systematic evaluation does not include methods that are based on Deep Learning (e.g., ; Cao and Cheung (2019)) since, at the current state of the art, these do not yet offer much opportunity for linguistic interpretation.",0
1775,Our focus is on interpretable linguistic features.,0
1775,"To perform our actual evaluation, two further choices need to be explained and motivated: we had to define an exact task (because the SRF task as such can be defined in different ways), and we had to specify a corpus.",0
1775,These two choices will be explained in section 3.,0
1775,The paper will conclude with a proposed consensus feature set and a discussion of the extent to which the features in it are linguistically interpretable (section 4).,0
1781,Podcasts come in many formats and levels of formality.,0
1781,Episodes appear on a regular or irregular cadence.,0
1781,They can be formal news journalism or conversational chat; fiction or non-fiction.,0
1781,"They are sharply growing in popularity (Whitner, 2020) and yet have been relatively little studied.",0
1781,"This medium opens up a rich palette of questions and issues for research in speech and language technology, linguistics, information access, and media studies.",0
1781,"To facilitate research into podcasts, we have produced a corpus of podcast episodes, comprising nearly 60,000 hours of speech.",1
1781,"This is orders of magnitude larger than previous transcribed speech datasets, and contains a rich variety of genres, subject matter, speaking styles, and structural formats.",1
1781,Our contributions are four-fold:,0
1781,"? The largest corpus 1 of transcribed speech data, from a new and understudied domain,",1
1781,"? A set of labeled data for retrieval and summarization on this corpus,",1
1781,"? Benchmarking results for retrieval and summarization tasks using standard baselines,",0
1781,"? An analysis of the data and benchmarking results, highlighting domain differences from vanilla versions of these tasks to motivate areas of future research.",0
1796,"In organizational settings where team members interact with each other, commitments and requests are constantly exchanged through communications.",0
1796,"For example, a team leader may request a team member to accomplish a task via email.",0
1796,Team members may chat with each other to make commitments about how the tasks are assigned within the team.,0
1796,"Efficient team collaboration, including creating todo lists and meetings, presumes that the tasks are clear.",0
1796,How can NLP support such uses?,0
1796,We define a task as a verb phrase that specifies a single action to be carried out.,0
1796,"One or more tasks could arise in a message from emails or chats, from a sender to a receiver.",0
1796,The root verb of such a verb phrase is its main verb.,0
1796,"Therefore, identifying the main verbs is essential to identifying tasks.",0
1796,"In simple sentences of commitments or requests like the following ones, each of them contains only one verb, which can easily be identified as a main verb (in bold): (1) I will send the QA later today and (2) please reschedule the meeting to next week.",0
1796,"However, in other cases main verbs may be difficult to identify.",0
1796,"For example, each of the following sentences contain multiple verbs, and the main verb of a task (in bold) is not obvious (other verbs are underlined): (1) let me know what I need to do to be ready; (2) go ahead and start working on this; (3) Jeff, before you arrange a meeting, we should think about a few things.",0
1796,; and (4) I would like to meet to discuss or appeal to Greg.,0
1796,Approaches of detecting tasks from emails and chats exist.,0
1796,Bennett and Carbonell (2005) use supervised machine learning classifiers for detecting whether a sentence includes a task.,0
1796,Lampert et al.,0
1796,(2010) provide a binary classifier to detect requests.,0
1796,Kalia et al.,0
1796,(2013) propose binary classifiers to detect the operational forms of commitments of different types.,0
1796,Wang et al.,0
1796,"(2019) categorize commitments into three types-Request Information, Schedule Meeting, and Promise Action-and train a deep learning model to identify them.",0
1796,Lin et al.,0
1796,"(2018) identifies actions, such as reply-yesno, reply-ack, and investigate, using a reparametrized long short-term memory (LSTM) network.",0
1796,Mukherjee et al.,0
1796,(2020) apply a sequence-to-sequence model to generate todo lists based on commitments expressed in emails.,0
1796,There are two important limitations to existing contributions.,0
1796,"One, the majority of them are limited to detecting whether a sentence contains a task (binary classification) without identifying the specific task.",0
1796,"Two, existing studies leverage supervised approaches to identify tasks, which may not be easily generalized to domain-specific task detection for which manually annotated datasets are not available.",0
1796,"Manual annotation of tasks can be cumbersome, especially for domain-specific datasets.",0
1796,"Therefore, our objective is to extract main verbs of tasks in an unsupervised fashion.",0
1796,"To this end, we propose Li?, an unsupervised approach to identify specific tasks from sentences.",1
1796,"To extract specific tasks, Li? identifies the main verb present in a sentence by jointly modeling the syntactic and semantic information in it.",1
1796,We have evaluated Li? on an email dataset and a chat dataset.,0
1796,Li? achieves an F1-score of 80% for the email dataset and 89% for the chat dataset.,0
1796,These results show an improvement over the state-of-the-art supervised baselines.,0
1805,"Recently, neural machine translation (NMT) has demonstrated impressive performance improvements and became the de-facto standard (Sutskever et al., 2014;Bahdanau et al., 2015;Vaswani et al., 2017).",0
1805,"However, like other neural methods, NMT is data-hungry.",0
1805,"This makes it challenging when we train such a model in low-resource scenarios (Koehn and Knowles, 2017).",0
1805,Researchers have developed promising approaches to low-resource NMT.,0
1805,"Among these are data augmentation (Sennrich et al., 2016a;Fadaee et al., 2017), transfer learning (Zoph et al., 2016;Kocmi and Bojar, 2018), and pre-trained models (Peters et al., 2018;Devlin et al., 2019).",0
1805,But these approaches rely on external data other than bi-text.,0
1805,"To date, it is rare to see work on the effective use of bilingual data for low-resource NMT.",0
1805,"In general, the way of feeding samples plays an important role in training neural models.",0
1805,A good instance is that it is popular to shuffle the input data for robust training in state-of-the-art systems.,0
1805,"More systematic studies on this issue can be found in recent papers (Bengio et al., 2009;Kumar et al., 2010;Shrivastava et al., 2016).",0
1805,"For example, Arpit et al.",0
1805,(2017) have pointed out that deep neural networks tend to prioritize learning "easy" samples first.,0
1805,"This agrees with the idea of curriculum learning (Bengio et al., 2009) in that an easy-to-hard learning strategy can yield better convergence for training.",0
1805,"In NMT, curriculum learning is not new.",0
1805,"Several research groups have applied it to large-scale translation tasks although few of them discuss the issue in a low-resource setup (Zhang et al., 2018;Platanios et al., 2019;.",0
1805,The first question here is how to define the "difficulty" of a training sample.,0
1805,Previous work resorts to functions that produce a difficulty score for each training sample.,0
1805,This score is then used to reorder samples before training.,0
1805,But the methods of this type enforce a static scoring strategy and somehow disagrees with the fact that the sample difficulty might be changing when the model is updated during training.,0
1805,Another assumption behind curriculum learning is that the difficulty of a sample should fit the competence of the model we are training.,0
1805,"Researchers have implicitly modeled this issue by hand-crafted curriculum schedules (Zhang et al., 2018) or simple functions (Platanios et al., 2019), whereas there has no in-depth discussion on it yet.",0
1805,"In this paper, we continue the line of research on curriculum learning in low-resource NMT.",0
1805,We propose a dynamic curriculum learning (DCL) method to address the problems discussed above.,1
1805,The novelty of DCL is two-fold.,0
1805,"First, we define the difficulty of a sample to be the decline of loss (i.e., negative log-likelihood).",1
1805,"In this way, we can measure how hard a sentence can be translated via the real objective used in training.",1
1805,"Apart from this, the DCL method explicitly estimates the model competence once the model is updated, so that one can select samples that the newly-updated model has enough competence to learn.",1
1805,DCL is general and applicable to any NMT system.,1
1805,"In this work, we test it in a Transformer-based system on three low-resource MT benchmarks and different sized data selected from the WMT'16 En-De task.",0
1805,Experimental results show that our system outperforms the strong baselines and several curriculum learning-based counterparts.,0
1805,2 Related work 2.1 Low-Resource NMT Koehn and Knowles (2017) show that NMT systems result in worse translation performance in lowresource scenarios.,0
1805,Researchers have developed promising approaches to this problem which mainly focus on introducing external knowledge to improve low-resource NMT performance.,0
1805,"Data augmentation (Sennrich et al., 2016a;Fadaee et al., 2017) alleviates this problem by generating pseudo parallel data.",0
1805,"A large amount of auxiliary parallel corpus from other related language pairs can be used to pre-train model parameters and transfer to target language pair (Zoph et al., 2016;Chen et al., 2017;Gu et al., 2018a;Gu et al., 2018b;Kocmi and Bojar, 2018).",0
1805,"Pre-trained language models trained with a large amount of monolingual data (Peters et al., 2018;Devlin et al., 2019) improve the quality of NMT model significantly (Clinchant et al., 2019;Zhu et al., 2020).",0
1805,"However, these approaches rely on a large number of external resources or conditions, e.g., the auxiliary parallel corpus related to the source or target language, or a large amount of monolingual data.",0
1805,"In this paper, we explore the effective use of bilingual data for low-resource NMT.",0
1807,The goal of text simplification is to transform text into a variant that is more broadly accessible to a wide variety of readers while preserving the content.,0
1807,"While this has been accomplished using a range of approaches (Shardlow, 2014), most text simplification research has focused on fully-automated approaches (Xu et al., 2016;Zhang and Lapata, 2017;Nishihara et al., 2019).",0
1807,"However, in some domains, such as healthcare, using fully-automated text simplification is not appropriate since it is critical that the important information is preserved fully during the simplification process.",0
1807,"For example, Shardlow et al.",0
1807,(2019) found that fully automated approaches omitted 30% of critical information when used to simplify clinical texts.,0
1807,"For these types of domains, instead of fully-automated approaches, interactive text simplification tools are better suited to generate more efficient and higher quality simplifications (Kloehn et al., 2018).",0
1807,Autocomplete tools suggest one or more words during text composition that could follow what has been typed so far.,0
1807,"They have been used in a range of applications including web queries (Cai et al., 2016), database queries (Khoussainova et al., 2010), texting (Dunlop and Crossan, 2000), e-mail composition , and interactive machine translation, where a user translating a foreign sentence is given guidance as they type (Green et al., 2014).",0
1807,Our work is most similar to interactive machine translation.,0
1807,"Autocomplete tools can speed up the text simplification process and give full control over information preservation to users, which is required in some domains, such as health and medical.",0
1807,"In this paper, we explore the application of pretrained neural language models (PNLMs) to the autocomplete process for sentence-level medical text simplification.",0
1807,"Specifically, given (a) a difficult sentence a user is trying to simplify and (b) the simplification typed so far, the goal is to correctly suggest the next simple word to follow what has been typed.",0
1807,Table 1 shows an example of a difficult sentence along with a simplification that the user has partially typed.,0
1807,"An autocomplete model predicts the next word to assist in finishing the simplification, in this case a verb like ""take"", which might be continued to a partial simplification of ""take up glucose"".",0
1807,"By suggesting the next word, the autocomplete models provide appropriate guidance while giving full control to human experts in simplifying text.",0
1807,We explore this task in the health and medical domain where information preservation is a necessity.,0
1807,Difficult Lowered glucose levels result both in the reduced release of insulin from the beta cells and in the reverse conversion of glycogen to glucose when glucose levels fall.,0
1807,This insulin tells the cells to Table 1: An example text simplification Autocomplete task.,0
1807,The user is simplifying the difficult sentence on top and has typed the words on the bottom so far.,0
1807,The example is taken from a medical parallel English Wikipedia sentence pair in Table 2.,0
1807,Difficult Lowered glucose levels result both in the reduced release of insulin from the beta cells and in the reverse conversion of glycogen to glucose when glucose levels fall.,0
1807,This insulin tells the cells to take up glucose from the blood.,0
1807,We make three main contributions:,0
1807,"We introduce a new parallel medical data set consisting of aligned English Wikipedia and Simple Wikipedia sentences, which is extracted from the commonly used general Wikipedia parallel corpus (Kauchak, 2013).",1
1807,The resulting medical corpus has 3.3k sentence pairs.,0
1807,"This corpus is larger than previously generated corpora (by over 1k sentence pairs) and has stricter quality control (Van den Bercken et al., 2019).",1
1807,"Our corpus requires a medical sentence to contain 4 or more medical words and belong to medical titles as compared to the no title requirement and needing to contain only 1 medical word, as described in Van den Bercken et al.",1
1807,We examine the use of PNLMs for the autocomplete task on sentence-level text simplification and provide an initial analysis based on four recent models on this new medical corpus.,0
1807,"In traditional autocomplete tasks, only the text being typed is available.",0
1807,"For text simplification, the additional context of the difficult sentence is also available.",0
1807,We show how this additional information can be integrated into the models to improve the quality of the suggestions made.,1
1807,We introduce an ensemble model that combines the output of the four PNLMs and outperforms all of the individual models.,1
1807,The ensemble approach is not application specific and may be used in other domains where PNLMs have been employed.,0
1825,"Automatic metrics play a significant role in summarization evaluation, profoundly affecting the direction of system optimization.",0
1825,"Due to its importance, evaluating the quality of evaluation metrics, also known as meta-evaluation has been a crucial step.",0
1825,"Generally, there are two meta-evaluation strategies: (i) assessing how well each metric correlates with human judgments (Lin, 2004;Ng and Abrecht, 2015;Louis and Nenkova, 2013;Peyrard et al., 2017), which requires procuring manual annotations that are expensive and time-consuming, and (ii) measuring the correlation between different metrics (Peyrard, 2019), which is a human judgment-free method.",0
1825,"In this work, we focus on the latter and ask two research questions: RQ1: How do automated metrics correlate when ranking summaries in different scoring ranges (low, average, and high)?",1
1825,We revisit the experiments of Peyrard (2019) which concludes that automated metrics strongly disagree for ranking high-scoring summaries.,1
1825,2 We find that the scoring range has little effect on the correlation of metrics.,1
1825,It is rather the width of the scoring range which affects inter-metric correlation.,1
1825,"Specifically, we observe that metrics agree in ranking summaries from the full scoring range but disagree in ranking summaries from low, average, and high scoring ranges when taken separately.",1
1825,RQ2: Which other factors affect the correlations of metrics?,1
1825,"In addition to the width of the scoring range, we analyze three properties of a reference summary on inter-metric correlation -Ease of Summarization, Abstractiveness and Coverage.",1
1825,"Overall we find that for highly extractive document-reference summary pairs, inter-metric correlation is high whereas metrics disagree when ranking summaries of abstractive document-reference summary pairs.",1
1825,"We summarize our contributions as follows: (1) We extend the analysis of Peyrard (2019) and find that not only do metrics disagree in the high scoring range, they also disagree in the low and medium scoring range.",1
1825,(2) We perform our analysis on the popular CNN/Dailymail dataset using traditional lexical matching metrics like ROUGE as well as recently popular semantic matching metrics like BERTScore and MoverScore.,1
1825,"(3) Apart from the width of the scoring range, we analyze three linguistic properties of reference summaries which affect inter-metric correlations.",1
1828,"While cascade automatic speech-to-text translation (ST) systems operate in two steps: source language automatic speech recognition (ASR) and source-to-target text machine translation (MT), recent works have attempted to build end-to-end ST without using source language transcription during decoding (Brard et al., 2016;Weiss et al., 2017;Brard et al., 2018).",0
1828,"After two years of extensions to these pioneering works, the last results of the IWSLT 2020 shared task on offline speech translation (Ansari et al., 2020) demonstrate that end-to-end models are now on par (if not better) than their cascade counterparts.",0
1828,"Such a finding motivates even more strongly the works on multilingual (one-to-many, many-to-many, many-to-one) ST Inaguma et al., 2019; for which end-to-end models are well adapted by design.",0
1828,"Moreover, of these two approaches: cascade proposes a very loose integration of ASR and MT (even if lattices or word confusion networks were used between ASR and MT before end-to-end models appeared) while most end-to-end approaches simply ignore ASR subtask, trying to directly translate from source speech to target text.",0
1828,We believe that these are two edge design choices and that a tighter coupling of ASR and MT is desirable for future end-to-end ST applications.,0
1828,This paper addresses multilingual ST and investigates more closely the interactions between speech transcription (ASR) and speech translation (ST) in a multilingual end-to-end architecture based on Transformer.,1
1828,"While those interactions were investigated as a simple multi-task framework in (Anastasopoulos and Chiang, 2018) for a bilingual case, we propose a dual-decoder with an ASR decoder tightly coupled with an ST decoder and evaluate its effectiveness on one-to-many ST. Our model is inspired by , but the interaction between ASR and ST decoders is much tighter.",1
1828,"1 Finally, our experiments show that our model outperforms theirs on the MuST-C benchmark .",0
1828,Our contributions are summarized as follows:,0
1828,"(1) a new model architecture for joint ASR and multilingual ST, (2) an integrated beam search decoding strategy which jointly transcribes and translates, and that is extented to a wait-k strategy where the ASR hypothesis is ahead of the AST hypothesis by k tokens and vice-versa, (3) competitive performance on MuST-C dataset in the multilingual setting and improvements on previous joint ASR/ST work.",1
1828,"Multilingual ST Multilingual translation (Johnson et al., 2016) consists in translating between different language pairs with a single model, thereby improving maintainability and the quality of low resource language pairs.",1
1828,adapt this method to one-to-many multilingual speech translation by adding a language embedding to each source feature vector.,1
1828,The authors also observe that using the source language (English) as one of the target languages improves performance.,0
1828,Inaguma et al.,0
1828,(2019) simplify the previous approach by pre-pending a target language token to the decoder and apply it to one-to-many and many-to-many speech translation.,0
1828,They do not investigate many-to-one due to the lack of a large corpus for this.,0
1828,"To fill this void,  release the CoVoST corpus for ST from 11 languages into English and demonstrate the effectiveness of many-to-one ST.",0
1828,Joint ASR and ST Joint ASR and ST decoding was first proposed by Anastasopoulos and Chiang (2018) through a multi-task learning framework.,0
1828,Chuang et al.,0
1828,(2020) improved multitask ST by utilizing word embedding as an intermediate level instead of text.,0
1828,"A two-stage model that performs first ASR and then passes the decoder states as input to a second ST model was also proposed (Sperber et al., 2019).",0
1828,Their architecture is closer to cascaded translation while maintaining end-to-end trainability.,0
1828,Our work is closely related to that of  who propose an interactive attention mechanism which enables ASR and ST to be performed synchronously.,0
1828,Both ASR and ST decoders do not only rely on their previous outputs but also on the outputs predicted in the other task.,0
1828,We highlight three differences between their work and ours: (a) we propose a more general framework in which  is only a special case; (b) tighter integration of ASR and ST is proposed in our work; and (c) we experiment in a multilingual ST setting while previous works on joint ASR and ST only investigated bilingual ST.,1
1883,Ontologies have been widely used in lexical semantics to organize and represent knowledge.,0
1883,"Carefully built by experts, they contain semantically meaningful information in the form of relations between concepts.",0
1883,"However, being manually constructed, they struggle to assimilate new information.",0
1883,"Compared to ontologies, distributed representations are fully automated and can be fine-tuned for new tasks.",0
1883,"Despite their exceptional performance, most distributional methods do not have an explicit semantic interpretation.",0
1883,"The resulting representations encode a tremendous amount of information, but afford no way to interpret what this information is and how it relates to the concept.",0
1883,"Thus, one cannot choose which type of information is useful for a specific task, unless one has a lot of data and resources to fine-tune.",0
1883,"Although a few approaches have tried to bridge the gap between semantics and distributed representations (Faruqui et al., 2015;Mrk?i? et al., 2017), (1) they only encode information from ontologies, which are not extensible, and (2) the final representations are still not semantically meaningful.",0
1883,"Motivated by these problems, we introduce a novel hybrid representation called Definition Frames (DF), which encode semantic information extracted from definitions.",1
1883,"DFs are matrix representations, where each row corresponds to a particular relation.",1
1883,"The set of the relations used is based on the Qualia structure suggested in (Boguraev and Pustejovsky, 1990), and they are extracted automatically from definitions via a domain-adaptation approach.",1
1883,"To the best of our knowledge, DF is the first hybrid representation, combining an explicit structure through semantically meaningful rows, while still being decomposed into distributional vectors.",1
1886,"Fake news have become a problem of paramount relevance in our society, due to their large diffusion in public discourse, especially on social media, and their alarming effects on our lives (Lazer et al., 2018).",0
1886,"Several works show that fake news played a role in major events such as the US Presidential Elections (Allcott and Gentzkow, 2017), stock market trends (Rapoza, 2017), and the Coronavirus disease outbreak (Shimizu, 2020).",0
1886,"In NLP a considerable amount of work has been dedicated to fake news detection, i.e., the task of classifying a news as either real or fake -see Zhou and Zafarani (2018), Kumar and Shah (2018) and Oshikawa et al.",0
1886,(2018) for overviews.,0
1886,"While initial work focused uniquely on the textual content of the news (Mihalcea and Strapparava, 2009), subsequent research has considered also the social context in which news are consumed, characterizing, in particular, the users who spread news in social media.",0
1886,"In line with the results reported in other classification tasks of user-generated texts (Del Tredici et al., 2019;Pan and Ding, 2019), several studies show that leveraging user representations, together with news' ones, leads to improvements in fake news detection.",0
1886,"In these studies, user representations are usually computed using informative but costly features, such as manually assigned credibility scores (Kirilin and Strube, 2018).",0
1886,"Other studies, which leverage largely available but scarcely informative features (e.g., connections on social media), report less encouraging results (Zubiaga et al., 2016).",0
1886,Our work also focuses on users.,0
1886,"We build on psychological studies that show that some people are more prone than others to spread fake news, and that these people usually share a set of cognitive and social factors, such as personality traits, beliefs and ideology (Pennycook et al., 2015;Pennycook and Rand, 2017).",0
1886,"Also, we rely on studies showing a relation between these factors and language use, both in Psychology and Linguistics (Pennebaker et al., 2003;De Fina, 2012) and in NLP (Plank and Hovy, 2015;Preot ?iuc-Pietro et al., 2017).",0
1886,"We therefore propose to leverage user-generated language, an abundant resource in social media, to create user representations based solely on users' language production.",1
1886,"We expect, in this way, to indirectly capture the factors characterizing people who spread fake news.",1
1886,We implement a model for fake news detection which jointly models news and user-generated texts.,1
1886,"We use Convolutional Neural Networks (CNNs), which were shown to perform well on text classification tasks (Kalchbrenner et al., 2014) and are highly interpretable (Jacovi et al., 2018), i.e., they allow us to extract the informative linguistic features of the input texts.",1
1886,"We test our model on two public English datasets for fake news detection based on Twitter data, both including news and, for each news, the users who spread them on Twitter.",0
1886,"We leverage two kinds of user-generated language, i.e., past tweets and self-description.",0
1886,"In line with our expectations, model performance improves when language-based user representations are coupled with news representations, compared to when only the latter are used.",0
1886,"Moreover, the model achieves high results when leveraging user-generated texts only to perform the task.",0
1886,"We use the linguistic features returned by the model to analyze the language of fake news spreaders, showing that it has distinctive features related to both content, e.g., the large usage of words related to emotions and topics such as family and religion, and style, e.g., a peculiar usage of punctuation.",1
1886,"Importantly, these features are largely independent from the domain of the dataset, and stable across datasets.",1
1886,"Moreover, we find that the two kinds of user-generated language we consider provide partially overlapping information, but with some relevant differences.",1
1886,"Finally, we consider the relation between the language produced by the users and their connections in the social graph.",1
1886,"In particular, we investigate the Echo Chamber effect, i.e, the situation in which the ideas expressed by a user are reinforced by their connections (Jamieson and Cappella, 2008).",1
1886,"In previous NLP work, the effect has been studied by observing whether users connected in the social graph post the same content, usually defined as a link to a web page from a manually compiled list (Garimella et al., 2018;Choi et al., 2020).",0
1886,"We propose to define the content produced by the users based on their linguistic production, and to compute the Echo Chamber effect as a function of the similarity between the content of connected users and their distance in the social graph.",1
1886,"By applying our methodology, we show that the Echo Chamber effect is at play, to different extent, in both the datasets under scrutiny.",0
1886,Modelling user-generated data requires careful consideration of the possible ethical aspects related to the treatment of such data.,0
1886,We provide an ethics statement in the Supplementary Material with details on how we have dealt with these aspects.,0
1912,Simple factoid question answering over a knowledge base is an important task in natural language understanding.,0
1912,"Although it only deals with factoid questions about a single entity and a predicate, they cover much of the real user queries (Dai et al., 2016), and also, accurate mapping of these is a critical subproblem in more general semantic parsing-based complex query generation (Berant et al., 2013;Bao et al., 2016;Reddy et al., 2016;Trivedi et al., 2017).",0
1912,"SimpleQuestions (Bordes et al., 2015) is the largest and most popular dataset on this task.",0
1912,"It was recently argued that this task, given abundant training data, is nearly solved with standard techniques in machine learning (Petrochuk and Zettlemoyer, 2018;Mohammed et al., 2018).",0
1912,"In this paper, we present a thorough empirical analysis to assess whether the success on one particular dataset indicates the success of the task itself in general.",1
1912,"To this end, we evaluate the behaviors of four existing QA systems targeting SimpleQuestions (Mohammed et al., 2018;Yu et al., 2017;, across four different datasets (Cai and Yates, 2013;Yih et al., 2016;Bordes et al., 2015;Jiang et al., 2019), under different conditions.",1
1912,One of our research goals was to evaluate the robustness of a model trained on a single dataset against questions that are outside of the distribution of the training data.,1
1912,"Such robustness evaluation is recently actively studied in other language understanding tasks (Jia and Liang, 2017;Naik et al., 2018;McCoy et al., 2019) but little effort has been made on question answering over a knowledge base, though, in practice, it would be critical because a practical system has to be robust on real user queries, which may be outliers in the training data.",0
1912,"Our experiments suggest that, while SimpleQuestions is the largest, the examples are too simple and the success on it does not indicate a progress in factoid question answering in general.",1
1912,"For example, we show that, under the same training data size, the system's accuracy on SimpleQuestions gets about 10 points higher than that on WebQuestions.",1
1912,"Although the simplicity of SimpleQuestions is pointed out in past work (Jiang et al., 2019), our work provides an empirical evidence that this is indeed the case with a careful comparison and manual analysis using the standardized datasets.",0
1912,"Given our analysis, we suggest two possible future directions.",0
1912,"One is to invent a clever novel data creation method that would be scalable, while avoiding bias as much as possible.",0
1912,"In this respect, we point out that a recent attempt by FreebaseQA (Jiang et al., 2019) is not successful, and that significant bias still exists.",0
1912,Another is to exploit useful information from the large dataset of SimpleQuestions in a better way.,0
1912,"In the last analysis, we perform an experiment on this idea by training on a union of the datasets (Talmor and Berant, 2019).",0
1927,"The interpretation of modal expressions is essential to meaningful human-robot dialogue: the ability to convey information about objects and events that are displaced in time, space, and actuality allows the human and robot to align their environmental perceptions and successfully collaborate (Liu and Chai, 2015).",0
1927,"As an example, if a robot is sent to a remote location on a search and navigation mission, modally interpreted expressions such as ""Tell me what you see"" (uttered by the human) and ""I can't see because of smoke"" (uttered by the robot) are vital to information exchange.",0
1927,"Similarly, a robot that has abilities to navigate obstacles (for example, by jumping or using LIDAR) can inform the human of this.",0
1927,"The learning of modal expressions for automatic understanding and use nevertheless presents a conversational paradox: while these expressions serve to communicate and align world knowledge, there is no obvious manner to ground them in the shared environment.",0
1927,"Whereas objects and actions can be pointed to or modeled for grounded learning, modal expressions are grounded in the linguistic signal itself.",0
1927,"Nevertheless, a basic understanding of modal meaning would allow non-human agents to reason about the possible uses of objects and better assess how certain actions and behaviors impact the task at hand.",0
1927,"In this paper, we document the range and nature of modally interpreted expressions used in human-robot dialogue with the goal to make the interpretation of such expressions easily automated in the future.",1
1927,"We hypothesize that certain readings and scope preferences for modal operators are more salient in human-robot dialogue because of the unique makeup of the common ground (Poesio, 1993).",1
1927,"We provide a mapping from formal semantic theories of modality related to participant beliefs and updates of the common ground (Portner, 2009), to a practical model of speech acts that translates into robot action for search and navigation task-oriented dialogue and an automated NLU and NLG system (Bonial et al., 2020).",1
1927,"This mapping is formalized in an annotation scheme in which the use of modal expressions is mapped to their effect in dialogue, providing a model for the robot to learn the meaning of modal expressions (Chai et al., 2018).",1
1927,"Our annotation task reveals surprisingly high inter-annotator agreement for a complex scheme; results indicate that our data is highly repetitive in the natural language used, and yet the interpretation of modal expressions is quite diverse and worth investigating further to foster effective human-robot communication in situated, task-oriented settings.",1
1927,The paper is structured as follows.,0
1927,In Section 2 we introduce the SCOUT corpus for our annotation and situate formal semantic theories of modality in the context of human-robot dialogue.,0
1927,"We describe our annotation scheme in Section 3, which covers both the type of modality used in an expression, and the speech act the expression conveys.",0
1927,"We describe our results in Section 4, discussing implications for modal interpretation in human-robot dialogue and some linguistic issues that arose during the annotation process.",0
1927,"In Section 5 we consider the implications of our results for a theory of modality and common ground in human-robot dialogue, before concluding in Section 6.",0
1927,2 Background and Related Work,0
2027,"Identification and analysis of emotions in user-generated data in social media like Twitter, Facebook, Reddit, etc., is essential in understanding the daily trends and human behavior.",0
2027,"Emotion prediction aims at identifying and analyzing such emotions like 'Happy, 'Sad,' 'Angry,' 'Fear,' 'Surprise,' and 'Disgust' types present in the text.",0
2027,"Original works were focussed more on monolingual text (Alm et al., 2005;Chen et al., 2010) due to the large-scale availability of monolingual texts.",0
2027,India has twenty-three significant languages with over seven hundred and twenty dialects.,0
2027,"The majority of people are multilingual, and they tend to mix words from different languages in speech and written text.",0
2027,This method of interchanging languages is commonly addressed by terms 'Code-switching' and 'Code-mixing' as described by Lipski (1978).,0
2027,Code-mixing refers to the use of words from different languages in the same sentence.,0
2027,Code-switching refers to the use of words or phrases from different languages within the same speech context.,0
2027,We can understand the difference between code-switching and code-mixing from the positions of altered elements.,1
2027,"Code-mixing refers to the intrasentential modification of codes, whereas code-switching refers to the intersentential modification of codes.",1
2027,We observe code-switching and code-mixing frequently on social media platforms.,1
2027,"Since the available resources are limited for Kannada-English codemixed text, we primarily focus on the creation of corpus and annotating the code-mixed tweets with their respective emotions, in this paper.",1
2027,Here are some examples from a corpus of code-mixed Kannada-English generated from Twitter data and its translation in English.,0
2027,"T1: ""Nam placement officer helidda ee thara helidre ond company lu kelsa sigalla anta...I had 2 offers before I left college, ondu IT innondu core..."" Translation: ""Our placement officer said,'if you talk like this, you wont get a single job.'",0
2027,I had 2 offers before i left college.,0
2027,"One was IT, the other was core.""",0
2027,T2: "Eshwarappa avarey neevu petrol bunk ge hogilla ansuthe.,0
2027,"me nimmannu karkondu hogthini"" Translation: ""Eshwarappa, it looks like you did not go to the petrol bunk.",0
2027,I will take you there.",0
207,"In a way, a PQ is like clickbait, except that it is not lying to people.",0
207,Discovering what keeps readers engaged is an important problem.,0
207,We thus propose the novel task of automatic pull quote (PQ) selection accompanied with a new dataset and insightful analysis of several motivated baselines.,1
207,"PQs are graphical elements of articles with thought provoking spans of text pulled from an article by a writer or copy editor and presented on the page in a more salient manner (French, 2018), such as in Figure 1.",0
207,PQs serve many purposes.,0
207,"They provide temptation (with unusual or intriguing phrases, they make strong entrypoints for a browsing reader), emphasis (by reinforcing particular aspects of the article), and improve overall visual balance and excitement (Stovall, 1997;Holmes, 2015).",0
207,"PQ frequency in reading material is also significantly related to information recall and student ratings of enjoyment, readability, and attractiveness (Wanta and Gao, 1994;Wanta and Remy, 1994).",0
207,"The problem of automatically selecting PQs is related to the previously studied tasks of headline success prediction (Piotrkowicz et al., 2017;Lamprinidis et al., 2018), clickbait identification (Potthast et al., 2016;Chakraborty et al., 2016;Venneti and Alam, 2018), as well as key phrase extraction (Hasan and Ng, 2014) and document summarization (Nenkova and McKeown, 2012).",0
207,"However, in Sections 5.4 and 5.5 we provide experimental evidence that performing well on these previous tasks does not translate to performing well at PQ selection.",1
207,Each of these types of text has a different function in the context of engaging a reader.,0
207,The title tells the reader what the article is about and sets the tone.,0
207,Clickbait makes unwarranted enticing promises of what the article is about.,0
207,Key phrases and summaries help the reader decide whether the topic is of interest.,0
207,And PQs provide specific intriguing entrypoints for the reader or can maintain interest once reading has begun by providing glimpses of interesting things to come.,0
207,"With their unique qualities, we believe PQs satisfy important roles missed by these popular existing tasks.",0
207,"Do not worry about these: using lots of adjectives, adverbs, or nouns being ""exciting"" trying to summarize the article having a positive or negative sentiment In this work we define PQ selection as a sentence classification task and create a dataset of articles and their expert-selected PQs from a variety of news sources.",0
207,"We establish a number of approaches with which to solve and gain insight into this task: (1) handcrafted features, ( 2) n-gram encodings, (3) Sentence-BERT (SBERT) (Reimers and Gurevych, 2019) embeddings combined with a progression of neural architectures, and (4) cross-task models.",1
207,"Via each of these model groups, we uncover interesting patterns (summarized in Figure 2).",0
207,"For example, among handcrafted features, sentiment and arousal are surprisingly uninformative features, overshadowed by presence of quotation marks and reading difficulty.",0
207,Analysing individual SBERT embedding dimensions also helps understand the particular themes that make for a good PQ.,1
207,We also find that combining SBERT sentence and document embeddings in a mixture-of-experts manner provide the best performance at PQ selection.,1
207,The suitability of our models at PQ selection is also supported via human evaluation.,0
207,The main contributions are:,0
207,"We describe several motivated approaches for the new task of PQ selection, including a mixture-ofexperts approach to combine sentence and document embeddings (Section 3).",1
207,We construct a dataset for training and evaluation of automatic PQ selection (Section 4).,1
207,"We inspect the performance of our approaches to gain a deeper understanding of PQs, their relation to other tasks, and what engages readers (Section 5).",0
207,2 summarizes these findings.,0
2080,"Abusive language in online communities has become a significant societal problem (Nobata et al., 2016) and online abusive language detection (ALD) aims to identify any type of insult, vulgarity, or profanity that debases a target or group online.",0
2080,"It is not only limited to detecting offensive language (Razavi et al., 2010), cyberbullying (Xu et al., 2012), and hate speech (Djuric et al., 2015), but also to more nebulous or implicit forms of abuse.",0
2080,"Many social media companies and researchers have utilised multiple resources, including machine learning, human reviewers and lexicon-based text analytics to detect abusive language (Waseem, 2016;Qian et al., 2018).",0
2080,"However, none of them can perfectly resolve the ALD task because of the difficulties of moderating user content and in classifying ambiguous posts (Metz and Issac, 2019).",0
2080,"On the technical side, previous ALD models were developed on only a few subtasks (e.g.",0
2080,"hate speech, racism, sexism) in a single domain (like Twitter), and each specialised model is not successfully transferable to general ALD in different online communities.",0
2080,"Our research question is, ""What would be the best generic ALD model that can be used for different types of abusive language detection sub-tasks and in different online communities?""",1
2080,"To solve this, we found that Waseem et al.",0
2080,"(2017) reviewed the existing online abusive language detection literature, and defined a generic abusive language typology that can encompass the targets of a wide range of abusive language subtasks in different types of domain.",0
2080,The typology is categorised in the following two aspects: 1) Target aspect: The abuse can be directed towards either a) a specific individual/entity or b) a generalised group.,0
2080,"This is an essential sociological distinction as the latter refers to a whole category of people, like a race or gender, rather than a specific individual or organisation; 2) Content aspect: The abusive content can be explicit or implicit.",0
2080,"Whether directed or generalised, explicit abuse is unambiguous in its potential to be damaging, while implicit abusive language does not immediately imply abuse (through the use of sarcasm, for example).",0
2080,"For example, assume that we have a tweet ""F***"".",0
2080,"""You are sooo sweet like other girls"".",0
2080,"It includes all those aspects; the directed target (""yourself""), the generalised target (""girls""), the explicit content (""F***""), and the implicit content (""You are sooo sweet"").",0
2080,"Inspired by this abusive language typology, we propose a new generic ALD framework, MACAS (Multi-Aspect Cross Attention Super Joint for ALD), using aspect models and a cross-attention aspect gate Dataset Source Size Composition Waseem (Waseem and Hovy, 2016) Twitter 16.2k Racism(11.97%), Sexism(19.43%), None(68.60%) HatEval (Basile et al., 2019) Twitter 13k Hateful(42.08%), Non-hateful(57.92%) OffEval (Zampieri et al., 2019) Twitter 13.2k Offensive(33.23%), Not-offensive(66.77%) Davids  Twitter 24.8k Hate(5.77%), Offensive(77.43%), Neither(16.80%) Founta (Djouvas et al., 2018) Twitter 99k Abusive(27.15%), Hateful(4.97%), Normal(53.85%), Spam(4.97%) FNUC (Gao and Huang, 2017) Fox News Discussion Threads 1.5k  flow.",1
2080,"First, we build four different types of abusive language aspect embeddings, including directed target, generalised target, explicit content, and implicit content.",1
2080,We also propose to use a heterogeneous graph to analyse the linguistic behaviour of each author and learn word and document embeddings with graph convolutional networks (GCNs).,1
2080,Not every online community (e.g.,0
2080,news forums) allows user-to-user relationship (e.g.,0
2080,"follower-following), so we avoid using user-community relationship information.",0
2080,"Then, we propose a cross-attention aspect gate flow to obtain the mutual enhancement between the two aspects.",1
2080,"The gate flow contains two gates, target gate and content gate, then fuses the outputs of those gates.",1
2080,"The target gate draws on the content probability distribution, utilising the semantic information of the whole input sequence along with the target source, while the content gate takes in the target aspect probability distribution as supplementary information for content-based prediction.",1
2080,"For evaluation, we test six stateof-the-art ALD models across seven datasets focused on different aspects and collected from different domains.",0
2080,Our proposed model rivals or exceeds those ALD methods on all of the evaluated datasets.,0
2080,"The contributions of the paper can be summarised as follows: 1) We perform a rigorous comparison of six state-of-the-art ALD models across seven ALD benchmark datasets, and find those models do not embrace different types of abusive language aspects in different online communities.",0
2080,"2) We propose a generic new ALD algorithm that enables explicit integration of multiple aspects of abusive language, and detection of generic abusive language behaviour in different domains.",1
2080,The proposed model rivals state-of-the-art algorithms on ALD benchmark datasets and performs best overall.,0
2080,2 Related Work,0
2135,Dependency parsing is a classical task in natural language processing.,0
2135,"The head-dependent relations produced by dependency parsing can provide an approximation to the semantic relationship between words, which is useful in many downstream NLP tasks such as machine translation, information extraction and question answering.",0
2135,"Nowadays, supervised dependency parsers can reach a very high accuracy (Dozat and Manning, 2017;Zhang et al., 2020).",0
2135,"Unfortunately, supervised parsing requires treebanks (annotated parse trees) for training, which are very expensive and time-consuming to build.",0
2135,"On the other hand, unsupervised dependency parsing requires only unannotated corpora for training, though the accuracy of unsupervised parsing still lags far behind that of supervised parsing.",0
2135,We focus on unsupervised dependency parsing in this paper.,0
2135,"Most methods in the literature of unsupervised dependency parsing are based on the Dependency Model with Valence (DMV) (Klein and Manning, 2004), which is a probabilistic generative model.",0
2135,A main disadvantage of DMV and many of its extensions is that they lack expressiveness.,0
2135,"The generation of a dependent token is only conditioned on its parent, the relative direction of the token to its parent, and whether its parent has already generated any child in this direction, hence ignoring other contextual information.",0
2135,"To improve model expressiveness, researchers often turn to disciminative methods, which can incorporate more contextual information into the scoring or prediction of dependency arcs.",0
2135,"For example, Grave and Elhadad (2015) uses the idea of disciminative clustering, Cai et al.",0
2135,"(2017) uses a discriminative parser in the CRF-autoencoder framework, and Li et al.",0
2135,(2018) uses an encoder-decoder framework that contains a discriminative transitioned-based parser.,0
2135,"For DMV, Han et al.",0
2135,(2019) proposes the discriminative neural DMV which uses a global sentence embedding to introduce contextual information into the calculation of grammar rule probabilities.,0
2135,"In the literature of supervised graph-based dependency parsing, however, there exists another technique for incorporating contextual information and increasing expressiveness, namely high-order parsing (Koo and Collins, 2010;Ma and Hai, 2012).",0
2135,"A first-order parser, such as the DMV, only considers local parent-children information.",0
2135,"In comparison, a high-order parser takes into account the interaction between multiple dependency arcs.",0
2135,"In this work, we propose the second-order neural DMV model, which incorporates second-order information (e.g., sibling or grandparent) into the original (neural) DMV model.",1
2135,"To achieve better learning accuracy, we design a new neural architecture for rule probability computation and promote direct marginal likelihood optimization (Salakhutdinov et al., 2003;Tran et al., 2016) over the widely used expectationmaximization algorithm for training.",1
2135,"One particular challenge faced by second-order neural DMVs is that the number of grammar rules grows cubically to the vocabulary size, making it difficult to store and train a lexicalized model containing thousands of words.",0
2135,"Therefore, instead of learning a second-order lexicalized model, we propose to jointly learn a second-order unlexicalized model (whose vocabulary consists of POS tags instead of words) and a first-order lexicalized model based on the agreement-based learning framework (Liang et al., 2007).",1
2135,The jointly learned models have a manageable number of grammar rules while still benefiting from both second-order parsing and lexicalization.,1
2135,We conduct experiments on the Wall Street Journal (WSJ) dataset and seven languages on the Universal Dependencies (UD) dataset.,0
2135,The experimental results demonstrate that our models achieve state-ofthe-art accuracies on unsupervised dependency parsing.,0
2143,"Nowadays construction and maintainance of lexical resources (ontologies, knowledge bases, thesauri) has become essential for the NLP community.",0
2143,"In particular, enriching the most acknowledged lexical databases like WordNet (Miller, 1992) and its implementations for almost 50 languages 1 or collaboratively created lexical resources such as Wiktionary is crucial.",0
2143,"Resources of this kind are widely used in multiple NLP tasks: Word Sense Disambiguation, Entity Linking (Moro and Navigli, 2015), Named Entity Recognition, Coreference Resolution (Ponzetto and Strube, 2006).",0
2143,"However, the manual annotation process is too costly: it is time-consuming and requires language or domain experts.",0
2143,"On the other hand, automatically created datasets and resources usually lag in quality compared to manually labelled ones.",0
2143,"Therefore, it would be beneficial to assist manual work by introducing automatic annotation systems to keep valuable lexical resources up-to-date.",0
2143,In this paper we analyse the approaches to automatic enrichment of wordnets.,1
2143,"Formally, the goal of the Taxonomy Enrichment task is as follows: given words that are not included in a taxonomy (further denoted as orphan words), we need to associate each word with the appropriate hypernyms from it.",0
2143,"For example, given an input word ""duck"" we need to provide a list of the most probable hypernyms the word could be attached to, e.g.",0
2143,"""waterfowl"", ""bird"".",0
2143,A word may have multiple hypernyms.,0
2143,"SemEval-2016 task 14 (Jurgens and Pilehvar, 2016) was the first effort to perform a controlled evaluation of taxonomy enrichment methods.",0
2143,"There, the participants were given definitions of the new words to insert to the taxonomy.",0
2143,"Consequently, the presented systems relied heavily on these definitions.",0
2143,"However, this information is often unavailable for new words, which makes the whole setting unrealistic.",0
2143,"In order to overcome this limitation, RUSSE-2020 shared task on taxonomy enrichment (Nikishina et al., 2020) introduced a different scenario: the new words did not have definitions, but were provided with contexts, i.e.",0
2143,a text corpus which contained these words.,0
2143,"The organisers of the shared task provided a baseline and a training and evaluation datasets based on RuWordNet (a Russian analogue of WordNet database) (Loukachevitch et al., 2016).",0
2143,"We extend the results of RUSSE-2020 by creating an English dataset for this task, suggesting new methods for taxonomy enrichment and performing their analysis.",1
2160,"Social media has become an important real-time information source, especially during emergencies, natural disasters and other hot events.",0
2160,"According to a new Pew Research Center survey, social media has surpassed traditional news platforms (such as TV and radio) as a news source for Americans: about twothirds of American adults (68%) get news via social media.",0
2160,"Among all major social media sites, Twitter is still the site Americans most commonly use for news, with 71% of Twitter's users get their news from Twitter.",0
2160,"However, it can often be daunting to catch up with the most recent contents due to high volume and velocity of tweets.",0
2160,"Hence, social summarization aiming to acquire the most representative and concise information from massive tweets when a hot event happens is particularly urgent.",0
2160,"In recent years, many large-scale summarization datasets have been proposed such as New York Times (Sandhaus, 2008), Gigaword (Napoles et al., 2012), NEWSROOM (Grusky et al., 2018) and CNN/DAILYMAIL (Nallapati et al., 2016).",0
2160,"However, most of these datasets focus on formal document summarization.",0
2160,"Actually, social media text has many different characteristics from formal documents: 1) Short.",0
2160,"The length of a tweet is limited to 280 words, which is much shorter than formal document.",0
2160,2) Informal.,0
2160,"Tweets usually contains informal expressions such as abbrivations, typos, special symbols and so on, which make tweets more difficult to deal with.",0
2160,3) Social signal.,0
2160,"There are different kinds of social signals on social media such as hashtags, urls and emojis.",0
2160,4) Potential relations.,0
2160,Tweets are generated by users and hence have potential connections through user relationship.,0
2160,"Because of these characteristics, traditional summarization methods often do not perform well on social media.",0
2160,"Though there exists some social media summarization datasets (Hu et al., 2015;Avinesh et al., 2018;Duan et al., 2012;Cao et al., 2016;Nguyen et al., 2018).",0
2160,"However, these datasets only consider the text on social media and ignore the potential social signals on social network.",0
2160,"In a social context, the interactions between friends are obviously different from that between strangers.",0
2160,This phenomenon demonstrates that social relationship can affect user behavior patterns and consequently affect the tweets content they post.,0
2160,This inspires us to consider integrating social relations relevant signals when analyzing social information.,0
2160,"In this paper, we construct an event-oriented large-scale dataset with user relations for social summarization called TWEETSUM.",1
2160,"It contains 12 real world hot events with a total of 44,034 tweets and 11,240 users.",1
2160,"In summary, this paper provides the following contributions: (1) Construct an event-oriented social media summarization dataset, TWEETSUM, which contains social signals.",1
2160,"To our knowledge, it is the first summarization dataset that contains user relationships relevant social signals, such as hashtags and user profiles and so on; (2) Create expert summaries for each socail event and verified the existence of sociological theory in real data, including social consistency and contagion; (3) Evaluate the performance of typical extractive summarization models on our TWEETSUM dataset to provide benchmarks and validate the effectiveness of the dataset.",1
2191,Text generation with deep learning models is data hungry.,0
2191,"For example, in Figure 1, to make a model learn how to verbalize knowledge graphs, researchers need to collect a large number of human-annotated text and graph pairs.",0
2191,"However, good annotation is both expensive and difficult to get -annotators need to have a thorough understanding of hundreds of edge types in the knowledge graphs, as well as proper verbalization of the text, so that the written text can conform to the distribution of the desired text style.",0
2191,"Moreover, for dataset curators, checking the quality of annotation is also non-trivial.",0
2191,"For example, the WebNLG dataset goes through five updates to fix errors in the annotation over the past 3 years.",0
2191,"1 These difficulties in dataset collection makes parallel data-to-text datasets small-sized, and even nonexistent for low-resource domains.",0
2191,"To make problems worse, data-to-text models are, in many cases, infeasible to transfer from one domain to another.",0
2191,"For example, a text generation model that can produce Wikipedia biography-like descriptions cannot be used to generate introductions of plants.",0
2191,This can happen even between domains with similar content but different text styles.,0
2191,"For example, given the knowledge graph triple "" (Obama, birthYear, 1961),"" one domain verbalizes it as ""Obama was born in 1961,"" whereas another domain prefers ""Obama (1961 -) ..."".",0
2191,A model trained in the first domain can only generate the entities correctly but fail on all other words in the second domain.,0
2191,"To overcome the lack of labelled data and difficulty in domain adaptation, unsupervised data-to-text generation has emerged as an active research field recently (Freitag and Roy, 2018;Schmitt and Schtze, 2019;Guo et al., 2020).",0
2191,"However, the progress of this line of research is slowed down due to the lack of large-scale unsupervised datasets.",0
2191,"Notably, the curation of graph-to-text unsupervised datasets are nontrivial, as it requires (1) same content distribution between graphs and text, (2) text with high-accuracy entity annotation, (3) a much larger scale than the supervised datasets, and (4) a human-annotated test set.",0
2191,"Unfortunately, lacking such an unsupervised dataset, most unsupervised works have to artificially remove the pairing information between text and structured data, to force parallel datasets to be nonparallel.",0
2191,"Obviously, splitting parallel datasets, such as the WebNLG dataset (13K), and E2E dataset (50K), into non-parallel ones remains the originally small data size.",0
2191,"Consequenty, the research on unsupervised models is limited, as these existing unsupervised models cannot even have a deep architecture.",0
2191,"In contrast, a relatively faster field, unsupervised machine translation, has dataset sizes on the order of billions, such as 1.6B German and 2.1B English text used in (Artetxe et al., 2019).",0
2191,"Therefore, we propose a large dataset, GenWiki, which contains 1.3 million non-parallel text and graphs with shared content, and meet all four requirements mentioned before.",1
2191,"To better facilitate research in unsupervised graph-to-text generation, we provide two versions of our dataset: the full dataset GenWiki FULL (1.3M), and a fine version, GenWiki FINE (750K), which adds constraints on the text and graphs to force them to contain highly overlapped entity sets.",1
2191,The overview of our two datasets are shown in Figure 1.,0
2191,"The GenWiki FULL on the bottom left contains graphs on the same topic (Dota 2) as the text, and GenWiki FINE imposes a stronger constraint that entities in the graph can largely overlap with entities in text.",0
2191,"Both datasets are collected in a scalable, automatic way.",1
2191,The comparison of our dataset and previous data-to-text datasets is illustrated in Table 2.,0
2191,"An additional contribution is our human-annotated test set with of 1,000 graph and text pairs.",0
2191,"Based on our large-scale training dataset and the human-annotated test set, we analyze the performance of several baselines and existing models.",0
2191,"ToTTO (Parikh et al., 2020) 100K Supervised Human Description of Wikipedia tables WikiBio (Lebret et al., 2016",0
285,"In the area of biomedical NLP, Named Entity Recognition (NER) is a widely discussed and studied topic.",0
285,"The aim of the task is to identify biomedical entities such as genes, proteins, cell types, and diseases in biomedical documents, to allow for knowledge discovery in this domain.",0
285,"Models for Biomedical NER (Bio-NER) offer the opportunity to mine information and knowledge and thereby foster biomedical and drug discovery research (Habibi et al., 2017).",0
285,Several shared tasks addressing Bio-NER have been organized.,0
285,"These attempts and tasks resulted in benchmark datasets for solving English Bio-NER tasks, e.g.",0
285,"the GENIA corpus (Kim et al., 2003), JNLPBA (Kim et al., 2004), and BC2GM (Smith et al., 2008).",0
285,"However, when turning our attention to Chinese bio-NER, only a few attempts have been made, and these attempts either had limitations in text resource types and amount (Gu et al., 2008) or did not predefine biomedical named entity categories .",0
285,"Another limitation is that these attempts mostly focus on clinical texts or biomedical scientific publications but not include other relevant text resources, in particular biomedical patents.",0
285,"Many biomedical discoveries are patented in China, not only because of the encouraging policy on patentability of genetic products, but also since the existence of the speedily progressed and cheaper gene sequencing services (Du, 2018).",0
285,"Patent texts are highly technical with long sentences (Verberne et al., 2010).",0
285,"Two additional challenges of Chinese biomedical patents that we encountered are OCR errors and the heavy usage of code-mixing expressions, mixing English and Chinese in one sentence.",0
285,"This is mainly because the protein and gene names are commonly written in English (or the English names are given after the Chinese ones), while the disease names and other contents are written in Chinese.",0
285,"For this reason, it is not possible to directly apply pre-trained NLP models to Chinese biomedical patents.",0
285,"Moreover, as mentioned before, because of the lack of related studies, we not only lack pre-trained models which were trained on Chinese biomedical text data, but also well-organized Chinese biomedical patents datasets.",0
285,The contributions of this paper are threefold.,0
285,"First, we release a hand-labeled dataset with 5,813 sentences and 2,267 unique named entities from 21 Chinese biomedical patents.",1
285,"Second, we obtain promising results for the extraction of genes, proteins, and diseases with BERT models using our labeled data in limited training time and with limited computing power.",1
285,"Third, we show that when we use our NER model to extract entities from a large patent collection, we can potentially identify novel gene-gene interactions.",1
285,We release our data and code for use by others.,0
285,"1 In the following parts of this paper, we discuss previous attempts to solve Chinese Bio-NER tasks and other related tasks in Section 2; our methods and implementation details are explained in Section 3; the results of all experiments, along with the post analysis results, are described and discussed in Section 4; in Section 5 we discuss challenges and limitations of our study, followed by conclusions in section 6.",0
339,"Twitter has shown to be an effective medium of pertinent messages for gaining situational awareness, urgent needs assessment of the affected population, and planning response (Vieweg, 2012;Hughes and Palen, 2009;Purohit et al., 2018).",0
339,"Furthermore, the platform often breaks events and thus considered a low-latency source for timely access to information when other traditional sources are not available.",0
339,"Despite these advantages, one major issue that hinders the usability of Twitter data is the lack of geolocation information.",0
339,"Only 1-3% of tweets has GPS-coordinates (Huang and Carley, 2019).",0
339,Response authorities and humanitarian organizations heavily rely on geolocation information for both situational awareness and response tasks.,0
339,"While extensive research has been conducted on processing tweets for humanitarian aid, limited focus has been given to infer and extract geolocation information from them.",0
339,"In this work, we focus on extracting toponyms, i.e., place or location names from tweets.",0
339,We refer to this as a Location Mention Recognition (LMR) task.,0
339,Two main factors that influence the robustness of a LMR system are: (i) the dataset used to train the classifier (ii) the learning algorithm/classifier.,0
339,"In this work, we explore how the choice of a training dataset influences the performance of a LMR system in the domain of humanitarian crises where the cost and time of acquiring training data should be minimized.",1
339,"For this purpose, one well-established approach is to use a standard Named Entity Recognition (NER) system trained on a general-purpose NER dataset such as CoNLL-2003 (Sang andDe Meulder, 2003).",0
339,"Standard NER datasets provide annotated entities such as location, organization, and person from news articles or other formal web documents.",0
339,"However, the general-purpose NER system may not effectively extract toponyms from Twitter messages due to the fact that tweets often contain informal language, misspellings, grammar mistakes, shortened words, and slangs (Han et al., 2013).",0
339,"Moreover, entities mentioned in tweets may have inconsistent capitalization, which is one of the main features standard NER systems rely on (Zheng et al., 2018).",0
339,An alternative choice is to train a system using Twitter-specific NER datasets.,0
339,"Moreover, since the focus of this work is to identify and extract toponyms from crisis-related tweets, one obvious choice is to train a system specifically on location entities and drop other entity types (i.e., ORG and PER).",0
339,"Furthermore, we seek to determine the performance difference between a system trained on a generalpurpose Twitter data versus a system trained on disaster-specific Twitter data.",1
339,We also examine the difference in effectiveness when using labeled data from past disasters compared with labeled data from the current (target) disaster.,1
339,"As different types of disasters such as floods and earthquakes occur in different parts of the world, we investigate whether combining labeled data from different types versus the same type help and whether having labeled data from events occurring in close-proximity versus far-away from the target event has any effect on the performance of a LMR system.",1
339,"Considering all these diverse settings, we formulate our research questions as follows:",0
339,"? RQ1: How effective is the LMR system when trained on the web-based general-purpose NER datasets with all types of entities (LOC, ORG, PER) versus Twitter general-purpose datasets?",0
339,"? RQ2: How effective are the web-based general-purpose datasets compared with Twitter generalpurpose datasets when using only location entities (i.e., without ORG and PER)?",0
339,? RQ3: Does training on crisis-related Twitter datasets improve the performance of the LMR system compared to the general-purpose Twitter datasets?,0
339,? RQ4: Does training on combined data from different types of crisis events yield better performance than training on data from the same type of events?,0
339,? RQ5: How does the geospatial proximity of source events to the target event affect the performance?,0
339,The research community focusing on the LMR task is currently lacking answers to all these questions.,0
339,"In this work, we perform extensive experiments in an effort to provide answers to these questions.",0
339,"We fix our learning model to a state of the art model (i.e., BERT-based) and use a variety of datasets, i.e., webbased general-purpose, Twitter general-purpose, and Twitter crisis-specific.",1
339,Our findings suggest that the general-purpose datasets are not suitable for LMR in crisis tweets.,1
339,"Moreover, the types of entities (e.g., PER, ORG, LOC) used to train a model make a difference.",0
339,"Specifically, training using only LOC entities gives better performance than using all entity types.",1
339,"Furthermore, while Twitter data are preferred over general-purpose data, we observe that Twitter crisis-related data help achieve better performance.",1
339,"While labeled data from the target event outperforms all our models, we remark that using labeled data from disasters happened in close proximity is helpful when the target labeled data is not available.",1
339,The rest of the paper is organized as follows.,0
339,We summarize related work in Section 2.,0
339,We present an overview of the LMR problem and define it formally in Section 3.,0
339,We discuss the experimental setup in Section 4.,0
339,"We thoroughly analyze the results, answer the research question, and discuss the lessons we learned in Section 5.",0
339,We finally conclude and list some future directions in Section 6.,0
341,"The availability of comparable treebanks -syntactically annotated corpora -for a growing number of typologically distinct languages (most prominently in the collaborative Universal Dependencies project (Nivre et al., 2016)) has led to a recent surge of interest in computational work aiming to detect systematic patterns in the grammatical systems of natural languages and/or to test hypotheses from theoretical work in language typology against empirical evidence.",0
341,"The treebank-based approach (Liu, 2010;Lochbihler, 2017;Gerdes et al., 2019;Bjerva et al., 2019c;Hahn et al., 2020) adds a more data-driven perspective to a strand of research in computational typology (Daum and Campbell, 2007;Malaviya et al., 2017;Oncevay et al., 2019;Bjerva et al., 2019a;Bjerva et al., 2019b) that is based on carefully curated typological databases such as WALS 1 (Dryer and Haspelmath, 2013) or URIEL 2 .",0
341,The established computational modeling approaches targeting systematic patterns in typological databases essentially rely on the language features that the long tradition of typological research has identified as most relevant for identifying language universals.,0
341,"Examples of such features are the relative order of verbs and their objects, and the order of nouns and their dependents such as adjectives, numerals and genitives.",0
341,"In the computational research relying on typological knowledge bases, the features are typically assumed to be Boolean and universals are formulated as propositional formulae.",0
341,"A major focus has been on (a) detecting universals that have the form of an implication between two typological variables, and (b) predicting the value of unknown features in typological databases based on systematic patterns in attested grammatical systems.",0
341,"Graphical models have been widely used to calculate the strength of an implication (Daum and Campbell, 2007;Lu, 2013;Bjerva et al., 2019b;Bjerva et al., 2019a).",0
341,"While this approach is suitable if one wants to marginalize out the influence of confounding variables, it also constraints the investigated universals to have the form of an implication consisting of one implicand and usually one (but possibly multiple) implicant(s).",0
341,"In principle, comparable treebanks can provide the basis for observing the empirical distribution of arbitrary grammatical patterns and thus explore a much larger space of potential candidates for universal typological properties -including combinations of more than two variables that cannot be reduced to logical implication.",0
341,"However, an integration of such an approach with linguistically grounded hypothesis checking has to address two related challenges: (i) a theoretically guided way of navigating the enormous space of candidate propositions has to be developed, and (ii) a perspicuous framework is required for expressing multi-variable propositions and for evaluating them empirically against a full collection of comparable treebanks -while doing justice to the possibility of language-internal variation and tentative preferences by modeling features as real-valued.",0
341,This paper proposes an expressive framework that addresses the latter challenge.,1
341,We specify a formalism and its semantics to evaluate typological formulae of arbitrary logical complexity.,1
341,The core elements of our framework are customizable which gives prospective users the freedom to use their own implementations.,1
341,"We also include a method for counteracting the bias in the sample of well-studied languages (which are much more likely to be included among the languages with a treebank), many of which are phylogenetically closely related, while other language families are only very sparsely represented.",1
341,"To demonstrate its usefulness for empirical hypothesis testing, we run a number of experiments: 1) We re-evaluate established universals to test whether the evidence for universals provided by the framework reflects the broad consent.",1
341,2) Universals are always evaluated on a subset of all natural languages.,1
341,We investigate how reliable it is to transfer the evaluation result obtained from a subset of languages or language families to unknown languages.,1
341,"Formulae can be constructed from the language-property matrix by logical connectives, such as the conjunction in (2).",1
341, := v SVO  v Prep.,0
345,"As a preliminary but critical processing step for Chinese language processing, word segmentation (WS) has been extensively studied for decades and made great progress (Zheng et al., 2013;Pei et al., 2014;Yang et al., 2019;He et al., 2020).",0
345,"However, most of previous works adopt the single-grained word segmentation (SWS) formulation, where a sentence corresponds to a single word sequence according to some pre-defined annotation guidelines.",0
345,"As shown in Figure 1 (left), the SWS annotations of the sentence are different according to the guidelines of Penn Chinese Treebank (CTB) (Xue et al., 2005), the People Daily Corpus of the Peking University (PPD) (Yu et al., 2003), and the Microsoft Research WS Corpus (MSR) (Huang et al., 2006).",0
345,"This is largely due to the fact that as a polysynthetic language, Chinese has plenty of compound words with morphemes (mostly characters as basic units).",0
345,"The boundary between compounds and morphemes is usually subtle and vague (Jernudd and Shapiro, 1989).",0
345,Sproat et al.,0
345,(1987) show that the consensus ratio over word boundaries is only 76% even among Chinese native speakers without training on any guideline.,0
345,"In order to guarantee the annotation consistency, people turn to detailed annotation guidelines according to specific tasks or applications.",0
345,"For example, CTB usually prefers fine-grained words over coarse-grained words to facilitate further annotation of syntax and semantics, while PPD accommodates more coarse-grained words with information extraction and retrieval tasks in mind.",0
345,This poses a strong challenge in Chinese WS since words of different granularities are necessary for a variety of tasks and applications at the same time.,0
345,Zhu and Li (2008) and Hou et al.,0
345,(2010) adopt heuristic rules based on lexicon dictionaries to accommodate the necessity.,0
345,Similar functions are provided by publicly available Chinese WS tools such as jieba and PullWord.,0
345,"However, the effectiveness of such tools are far below satisfactory without tackling segmentation ambiguity problem.",0
345,"It is worth emphasizing that even for the same task or application, MWS can be useful due to its potential complementarity: fine-grained words reduce data sparseness, whereas coarse-grained words reserve more semantics.",0
345,"This facilitates researchers to employ multiple SWS outputs at the same time in information retrieval (IR) (Liu et al., 2008) and machine translation (MT) (Su et al., 2017).",0
345,"Motivated by above perspectives, Gong et al.",0
345,(2017) propose an MWS formulation with a hierarchical tree structure to accommodate all words of different granularities.,0
345,"Figure 1 (right) presents an example, where ""W"" means the spanning characters compose a word.",0
345,"To solve the issue of lacking labeled MWS data, they construct a large-scale pseudo MWS data for model training and tuning.",0
345,They propose several MWS approaches and justify the superiority of treating MWS as constituent parsing.,0
345,"However, their approaches only learn from pseudo MWS data and do not fully exploit word boundary information from other available sources which are helpful and easy to obtain.",0
345,This paper advances the state-of-the-art MWS model with weakly labeled data.,1
345,"Particularly, we propose to accommodate two types of weakly labeled data, i.e., SWS and naturally annotated dictionary example (DictEx) sentences, as extra training data, by employing a simple but competitive graph-based parsing model with local span-wise loss.",1
345,"Besides, we develop a unified annotation guideline for MWS and manually annotate a large-scale high-quality MWS dataset containing over 9,000 sentences from both canonical newswire texts (NEWS) and non-canonical web texts (BAIKE) for better evaluation.",1
345,Detailed evaluation shows that our proposed model with weakly labeled data significantly improves the state-of-the-art MWS model by 1.12 on NEWS and by 5.97 on BAIKE in F1.,0
345,We will release all the newly annotated data and the codes at https://github.com with a demo at https://url.,0
354,"Sentiment analysis (Pang and Lee, 2007) has been studied extensively for many years.",0
354,"Multimodal sentiment analysis is a task of predicting the sentiment of a video, an image or a text based on multiple modal features.",0
354,"Based on the contributions to each other from different modalities, multimodal sentiment analysis has achieved a significant results and attracted the attentions of many researchers in recent years.",0
354,The main challenge of the multimodal sentiment analysis is to capture a better fusion of different modalities.,0
354,Previous studies have proposed different methods for the fusion in different point of views.,0
354,Some methods focus on the fusion of different modalities in the same stage(fuse on the same time step).,0
354,Zadeh et al.,0
354,(2018c) propose Tensor Fusion Network (TFN) to explicitly model the interactions through a 3-fold Cartesian product from modality embedding.,0
354,propose Low-rank Multimodal Fusion (LMF) network which first obtain the unimodal representation and perform low-rank multimodal fusion to improve efficiency.,0
354,The above method captures the interaction of multiple modals just at the same stage.,0
354,While other methods focus on the interaction fusion from multi-stage.,0
354,propose a Multi-attention Recurrent Network (MARN) to store view-specific dynamics of the assigned modality and cross-view dynamics related to the assigned modality to discover cross-view dynamics cross different modalities.,0
354,Zadeh et al.,0
354,(2018a) propose Memory Fusion Network to learn the viewspecific interactions and the cross-view interactions.,0
354,"Although the above methods achieved good results, there are still two points can be improved.",0
354,"(1) We find that the fusion of the above methods is from one direction, that is, when the two modals are fused, the representation of two modals are fused directly as a new representation, similar to the work of Zadeh et al.",0
354,(2018c) and .,0
354,This fusion strategy ignores the long range of context information of each modality.,0
354,"As shown in Figure 1, the fusion of language modality and vision modality, if we can capture the context information of each modality from bi-directions, we can get sufficient fusion information.",0
354,(2) Few of previous studies explicitly explored the knowledge contained in the language text which can be used to help the fusion of different modalities based on the rich information exists in the language.,0
354,"To this end, in this paper, we propose a Sentimental Words Aware Fusion Network(SWAFN) for multimodal sentiment analysis.",1
354,"More specifically, we first use LSTM to encode the original features of three modalities.",1
354,"Then coattention mechanism (Xiong et al., 2017) is used to learn the co-dependent representation between language and other modalities separately, through capturing the attention contexts of each modality.",1
354,We call it the shallow fusion part.,1
354,Figure 1 presents the illustration of crossmodal coattention for language and vision modalities.,0
354,"Then, we design a sentimental words prediction task as a multitask auxiliary work to guide the aggregation of the shallow fusion of multiple modals features and get the final joint fusion representation.",1
354,"Based on our model, the contributions of this work are as follows:",0
354,1) We propose to use crossmodal coattention to learn the long range of context information of each two modals to obtain sufficient fusion features for multiple modals.,1
354,We also design a sentimental words prediction multitask as an auxiliary task to guide the fusion of multimodal to learn sentimental words aware representation.,1
354,"To the best of our knowledge, this is the first time that multi-task learning has been applied in multimodal sentiment analysis.",1
354,"2) We conduct several experiments on different public datasets, and we will show that our model is effective for multimodal sentiment analysis.",0
354,"In addition, we carry out a series of experiments to investigate the modal contribution and the impact of proposed shallow fusion and the auxiliary task.",0
365,"The advancement of Neural Machine Translation (NMT) has brought great improvement in the translation quality when translating clean input such as text from the news domain (Sutskever et al., 2014;Luong et al., 2015), and it was recently claimed that NMT has even achieved human parity in certain language pairs (Hassan et al., 2018).",0
365,"Despite its remarkable advancements, applicability of NMT over User-Generated Contents (UGC) such as social media text, still remains limited (Michel and Neubig, 2018).",0
365,"Since UGC are prevailing in our real-life communication, it is undoubtedly one of the challenges we need to overcome to make MT systems invaluable for promoting cross-cultural communication.",0
365,"Recently, with the increasing interest in handling UGC, a shared task was organized to measure how well MT systems adapt to those text.",0
365,"However, the way in which they evaluate systems is just giving an overall score to a dataset, which is the same as traditional MT evaluation.",0
365,The overall score does not provide precise information for understanding what leads to the huge performance gap between the translation of clean input and translation of UGC.,0
365,"To find a clue for improving the performance of MT systems on UGC, we need a solid basis for more detailed error analysis.",0
365,"As a first step towards more refined evaluation of MT systems on UGC, we present a new dataset,",1
365,"PheMT: Phenomenon-wise Dataset for Machine Translation Robustness, designed for phenomenonwise evaluation in Japanese-English translation (Figure 1).",1
365,"More specifically, we create a set of datasets, each of which provides focused evaluation on one linguistic phenomenon in UGC, such as Proper Noun or Colloquial Expression.",1
365,"With our dataset, we reveal some of the phenomena are still not well handled even by overwhelming off-the-shelf systems.",1
365,We will make our dataset publicly available for further development in MT systems.,0
365,We hope our dataset will provide promising directions for future MT systems and move the community one step forward by providing an additional axis of evaluation.,0
365,The contributions of this paper are:,0
365,We proposed a novel dataset designed for phenomenon-wise evaluation in Japanese-English translation.,1
365,We revealed with our dataset that some of the phenomena greatly degrade current MT systems including overwhelming off-the-shelf systems.,1
378,"Coreference is one important topic in linguistics (Gordon and Hendrick, 1998;Pinillos, 2011), and coreference recognition has been extensively researched in the natural language processing (NLP) community (Ng and Cardie, 2002;Lee et al., 2017;Qi et al., 2012;Fei et al., 2019).",0
378,"There are different kinds of coreferences, such as pronoun anaphora and abbreviation (Mitkov, 1998;Ruslan Mitkov, 1999;Mu?oz and Montoyo, 2002;Choubey and Huang, 2018).",0
378,"Here we examine the phenomenon of Chinese lexical fusion, where two closely related words in a paragraph are united by a fusion form of the same meaning, and the fusion word can be seen as the coreference of the two separation words.",0
378,"Since the fusion words are always out-of-vocabulary (OOV) words in downstream paragraph-level tasks such as reading comprehension, summarization, machine translation and etc., which can hinder the overall understanding and lead to degraded performance, the new task can offer informative knowledge to these tasks.",0
378,"Cannavaro broke the convention of accepting an interview, did not accept an interview for public training.",0
378,"The first medical personnel returned to Hangzhou yesterday, the others are expected to return to Hangzhou within two weeks.",0
378,"The central bank announced to reduce the benchmark interest rate for RMB loans, which is the second time to reduce the interest rate recently.",0
378,Table 1: Examples of Chinese lexical fusion.,0
378,Table 1 shows three examples of Chinese lexical fusion.,0
378,"In the examples, """"(accept an interview), """"(returned to Hangzhou) and """"(reduce the interest rate) are the fusion words, and their coreferences are ""(accept)/(interview)"", ""(return)/(Hangzhou)"", and "" (reduce)/ (interest rate)"", respectively.",0
378,Each corresponding of the fusion word consists of two separation words.,0
378,"Besides, each fusion word character is semantically corresponding to one of the separation words, which can be regarded as fine-grained coreferences.",0
378,"As shown, we have six fine-grained coreferences by the three example fusion words: ""?(accept)"", ""? (interview)"", ""?(return)"", ""? (Hangzhou)"", ""? (reduce)"" and ""?(interest rate)"".",0
378,Lexical fusion is used frequently in the Chinese language.,0
378,"Moreover, the fusion words are usually rarer words compared with their separation words coreferred, which are more difficult to be handled by NLP models (Zhang and Yang, 2018;Gui et al., 2019).",0
378,"Luckily, the meaning of the fusion words can be derived from that of the separation words.",0
378,"Thus recognizing the lexical fusion would be beneficial for downstream paragraph (or document)-level NLP applications such as machine translation, information extraction, summarization, etc.",0
378,"(Li and Yarowsky, 2008;Ferreira et al., 2013;Kundu et al., 2018).",0
378,"For example, for deep semantic parsing or translation, the fusion words """"(UNK) can be substituted directly by the separation words """"(accept) and """"(interview), as the same fusion words are rarely occurred in other paragraphs.",0
378,The recognition of lexical fusion can be accomplished by two subtasks.,0
378,"Given one paragraph, the fusion words, as well as the separation words should be detected as the first step, which is referred to as mention detection.",0
378,"Second, coreference recognition is performed over the detected mentions, linking each character in the fusion words to their coreferences, respectively.",0
378,"By the second step, full lexical fusion coreferences are also recognized concurrently.",0
378,"The two steps can be conducted jointly in a single end-to-end model (Lee et al., 2017), which helps avoid the error propagation problem, and meanwhile, enable the two subtasks with full interaction.",0
378,"In this paper, we present a competitive end-to-end model for lexical fusion recognition.",1
378,"Contextual BERT representations (Devlin et al., 2019) are adopted as encoder inputs as they have achieved great success in a number of NLP tasks (Tian et al., 2019;Zhou et al., 2019;.",1
378,"For mention detection, a CRF decoder (Huang et al., 2015) is exploited to detect all mention words, including both the fusion and the separation words.",1
378,"Further, we use a BiAffine decoder for coreference recognition (Zhang et al., 2017;Bekoulis et al., 2018), determining a given mention pair either to be a coreference or not.",1
378,"Since our task is semantic oriented, we use the sememe knowledge provided in HowNet (Dong and Dong, 2003) to help capturing the semantic similarity between the characters and the separation words.",1
378,"HowNet has achieved success in many Chinese NLP tasks in recent years (Duan et al., 2007;Gu et al., 2018;.",0
378,"Both Chinese characters and words are defined by senses of sememe graphs in it, and we exploit graph attention networks (GAT) (Velickovic et al., 2018) to model the sememe graphs to enhance our encoder.",1
378,"Finally, we manually construct a high-quality dataset to evaluate our models.",1
378,"The dataset consists of 7,271 cases of the lexical fusion, which are all exploited as the test instances.",0
378,"To train our proposed models, we construct a pseudo dataset automatically from the web resource.",0
378,Experimental results show that the auto-constructed training dataset is highly effective for our task.,0
378,"The end-to-end models achieved better performance than the pipeline models, and meanwhile the sememe knowledge can also bring significant improvements for both the pipeline and end-to-end models.",0
378,Our final model can obtain an F-measure of 79.64% for lexical fusion recognition.,0
378,"Further, we conduct in-depth analysis work to understand the proposed task and our proposed models.",0
378,"In summary, we make the following contributions in this paper:",0
378,"(1) We introduce a new task of lexical fusion recognition, providing a gold-standard test dataset and an auto-constructed pseudo training dataset for the task, which can be used as a benchmark for future research.",1
378,"(2) We propose a competitive end-to-end model for lexical fusion recognition task, which helps to integrate the mention recognition with coreference identification based on BERT representations.",1
378,(3) We make use of the sememe knowledge from HowNet to help capturing the semantic relevance between the characters in the fusion form and the separation words.,1
378,All the codes and datasets will be open-sourced at https://github.com/xxx under Apache License 2.0.,0
453,Named Entity Recognition (NER) is part of the fundamental tasks in Natural Language Processing (NLP).,0
453,The main objective of NER is to detect and classify proper names (named entities) in a free text.,0
453,"Typically, named entities can be subdivided into four broad categories: persons, i.e., first and last names, locations such as countries or landscapes, organisations such as companies or political parties, and miscellaneous entities which serves as a catch-all category for other named entities such as brands, meals, or social events.",0
453,"NER is an active research field and state-of-the-art solutions such as spaCy 1 , flair (Akbik et al., 2018), and Primer 2 manage to achieve near-human performance.",0
453,"However, classical NER (which we refer to as coarse-grained NER in this paper) models typically distinguish between only a small number of entity types, usually fewer than a dozen distinct categories.",0
453,"While this kind of shallow classification is sufficient for many applications, there are industrial usecases in which more precise information is necessary such as financial documents processing in the banking and finance context.",0
453,"For instance, application forms for a business loan are usually supplied with several supporting textual documents.",0
453,"These can contain the names of different types of persons, such as the owner or the CEO of the applying company, the contact person(s) at the issuing bank, finance analysts, or lawyers.",0
453,"The same is true for organisation names such as the name of the issuing bank, a government agency, or the name of the applying company or third-party companies.",0
453,"It is necessary to not only detect entity names, but to also qualify and differentiate between various entity types.",0
453,"Indeed, in many contexts the actual name of an entity is important only if it can be associated to a role, or any other relevant quality.",0
453,"In the banking and finance world for example, the strict regulatory requirements cannot be satisfied with just a list of who is involved; knowing how entities are involved is a necessity.",0
453,The term "Fine-Grained Named Entity Recognition" (FG-NER) was first coined by Fleischman and Hovy (2002).,0
453,"It describes a subtask of NER, where the objective remains the same as standard NER, but where the number of entity types is considerably higher.",0
453,"In extreme cases, FG-NER models such as the ? RQ3: How does the choice of the domain influence the performance of the models?",0
453,We use the EWNERTC dataset published by Sahin et al.,0
453,"(2017a), containing roughly 7 million data samples in 49 different domains.",0
453,"To the best of our knowledge, our study is the first aiming to precisely evaluate the performance of these existing approaches on the FG-NER task.",1
455,"Several symbolic meaning representations (MRs) support human annotation of text with broad coverage Oepen et al., 2019).",0
455,"To date, it is still not completely clear what semantic content they all encode, and how it compares to the content represented by the others.",0
455,It therefore behooves us to develop a firm linguistic understanding of MRs.,0
455,"In particular: are they merely a coarsening and rearranging of syntactic information, such as encoded in Universal Dependencies (Nivre et al., 2016, UD)?",0
455,To what extent do they take lexical semantic properties into account?,0
455,And what does this suggest about the potential for exploiting simpler or better-resourced linguistic representations for improved MR parsing?,0
455,"Intuitively, we ask whether the following equation holds: sentence-level meaning representation ?",0
455,"= syntax + lexical semantics To address this question, we examine UCCA, a document-level MR often used for sentence-level semantics (see 2.1).",0
455,Hershcovich et al.,0
455,"(2019a) began to examine the relation of UCCA to syntax, contributing a corpus with gold standard UD and UCCA parses, heuristically aligning them, and quantifying the correlations between syntactic and semantic labels.",0
455,"Conversely, Hershcovich et al.",0
455,"(2018) provided some initial evidence that other MRs can be brought to bear on the UCCA parsing task via multitask learning, but left the details of the relationship between representations to latent (and opaque) parameters of neural models.",0
455,"In this paper, we aim to close the gap between the two previous investigations by (1) building an interpretable rule-based system to convert from shallower representations (syntax and lexical semantic units/tags) into UCCA, forcing us to be linguistically precise about what UCCA captures and how it ""decomposes""; and (2) training top-performing supervised parsers in a delexicalized setting with only syntactic and lexical semantics features, as a data-driven mapping corroborating the rule-based approach.",1
455,"We perform our analysis on the Reviews section of the English Web Treebank (Bies et al., 2012), which has been manually annotated with UD and UCCA; and STREUSLE for lexical semantics ( 2).",1
455,"Although at present we only have the necessary evaluation data for English, the linguistic representations we use have (like UCCA) been applied to multiple languages ( 2).",1
455,Our approach can thus be applied cross-linguistically with minimal adaptation.,1
455,Our specific contributions are:,0
455,? Developing rule-based and supervised UCCA parsers based only on syntax and lexical semantics.,1
455,? Finding similarities and differences between the frameworks by a linguistically motivated analysis.,1
455,"Our conversion, parsing and analysis code will be publicly available upon publication.",0
460,"Task-oriented dialog systems are designed to help users achieve specific goals with natural language, such as weather inquiry or restaurant reservation.",0
460,"Compared with traditional pipeline methods (Williams and Young, 2007;Young et al., 2013), end-to-end approaches recently have gained much attention (Zhao et al., 2017;Eric and Manning, 2017;, since they free the task-oriented dialog systems from the manually designed pipeline modules and can be automatically scaled up to new domains.",0
460,"Recently, sequence-to-sequence (Seq2Seq) models have dominated the study of end-to-end taskoriented dialog systems (Bordes et al., 2017).",0
460,"Different from typical Seq2Seq models for open-domain dialog systems, the successful conversations for task-oriented dialog systems heavily rely on both dialog history and domain-specific knowledge base (KB).",0
460,"To effectively incorporate KB information and perform knowledge-based reasoning, memory augmented models have been proposed Wu et al., 2019), which model the dialog history and the KB knowledge as a bag of words in a flat memory.",0
460,"Despite the remarkable progress of previous studies, current memory based models for multi-turn taskoriented dialog systems still suffer from the following limitations.",0
460,"First, existing methods concatenate dialog utterances of current turn and previous turns as a whole, which ignore previous reasoning process performed by the model and are incapable of dynamically tracking long-term dialog states.",0
460,"These methods introduce much noise since previous utterances as the context is lengthy and redundant (Zhang et al., 2018).",0
460,"Taking the dialog in Table 1 as an example, when answering the user question in 6-th turn, it is difficult for the model to infer that the name of the restaurant is ""cocum"" from a long concatenated dialog context.",0
460,"In addition, previous models struggle to work well in the situations that require many rounds of interactions to complete a specific task.",0
460,"Second, previous studies tend to confound dialog history with KB knowledge, and store them into a flat memory (Sukhbaatar et al., 2015;Eric and Manning, 2017;.",0
460,"The shared memory forces the model to encode the dialog context and KB information using a single strategy, making it hard to efficiently reason over the two different types of data, especially when the memory is large.",0
460,"Although some methods  have explored to separate memories for modeling dialog context and KB facts, they either focus on language generation or achieve progress towards the dialog task (i.e., KB modeling), but not both.",0
460,"To alleviate the aforementioned limitations, we propose a Dual Dynamic Memory Network (DDMN), which keeps track of the dialog history and KB knowledge with separate memories.",1
460,"Specifically, we leverage a dialog memory manager to effectively maintain history utterances with a dialog history memory and a dialog state memory.",1
460,"The dialog history memory keeps fixed to store the representation of dialog context throughout the whole conversation, and the dialog state memory keeps updated at each turn to track the flow of history information and capture proper information of current turn for generation.",1
460,We leverage a KB memory manager containing a KB memory and a KB memory pointer to effectively track KB knowledge.,1
460,"The KB memory stores the KB triples using an end-to-end memory network (Sukhbaatar et al., 2015) and is shared across the entire conversation.",1
460,"The KB memory pointer softly attends to the KB memory at each turn, and guides the model to select appropriate KB entries in decoding.",1
460,Our main contributions can be summarized as follows.,0
460,"(1) We propose a Dual Dynamic Memory Network (DDMN) for task-oriented dialog systems, which dynamically keeps track of long dialog context for multi-turn interactions and effectively incorporates KB knowledge into generation.",1
460,(2) We employ separate memories to model dialog context and KB triples.,1
460,The iterative interactions between the two kinds of memories make the decoder focus on relevant dialog context and KB facts for generating coherent and human-like dialogs.,1
460,(3) The experimental results on three public datasets show that DDMN achieves impressive results compared to the existing methods.,0
460,"More importantly, our model is able to maintain more sustained conversations than the compared methods with the increase of dialog turns.",1
510,The use of digital technologies is transforming the modern economy with significant implications for businesses.,0
510,"As a result, organizations are perceiving digital technologies to present both growth opportunities and existential threats (Sebastian et al., 2017).",0
510,"Despite efforts to adopt such technologies, research demonstrates that the success rate in improving business performance is very low due to the lack of a coherent digital strategy (World Economic Forum & Bain & Company, 2018).",0
510,A positive research step to address this challenge is enhancing the understanding of current approaches to digital strategy.,0
510,Corporate documents are increasingly being used to understand the performance of organizations for various purposes.,0
510,"Examples are, predicting expected returns from financial reports (Theil et al., 2019) and measuring compliance from sustainability reports (Smeuninx et al., 2020).",0
510,"In the present study, we use earnings calls transcripts, as they provide rich insights into companies' activities, specifically on their digital strategy.",1
510,"We quantify the approach taken by companies based on the progress made on the following three components related to the digital strategy (Vial, 2019):",1
510,"? Business value: the expressed value of using a specific digital solution (e.g., enhancing customer experience and increasing operational efficiency) ? Strategy management: the policies and practices in place to support the implementation of digital solutions (e.g., setup of innovation lab, acquisition of startups, and use of agile methods) ? Digital technology: the technology used as a part of the identified digital solution (e.g., artificial intelligence, Internet of things, and robotics) In this work, we offer two contributions.",1
510,"First, we set the baseline for using deep learning based NLP models to measure the digital strategy of companies from earnings calls transcripts.",1
510,"Second, we apply this framework to investigate the digital strategy of Fortune 500 companies.",1
52,Relation classification (RC) aims to identify the relation between two specified entities in a sentence.,0
52,"Previous supervised approaches on this task heavily depend on human-annotated data, which limit their performance on classifying the relations with insufficient instances.",0
52,"Therefore, making the RC models capable of identifying relations with few training instances becomes a crucial challenge.",0
52,"Inspired by the success of few-shot learning methods in the computer vision community (Vinyals et al., 2016;Sung et al., 2017;Santoro et al., 2016), Han et al.",0
52,(2018) first introduce the few-shot learning to RC task and propose the FewRel 1 dataset as the benchmark.,0
52,"Recently, many works focus on this task and achieve remarkable performance (Gao et al., 2019a;Snell et al., 2017;Ye and Ling, 2019).",0
52,Previous few-shot relation classifiers perform well on sentences with only one relation of a single entity pair.,0
52,"However, in real natural language, a sentence usually jointly describes multiple relations of different entity pairs.",0
52,"Due to the fact that these relations keep high co-occurrence in the same context, previous few-shot RC models struggle to distinguish them with few annotated instances.",0
52,"For example, Table 1 shows three instances from the FewRel dataset, where each sentence describes multiple relations with corresponding keyphrases highlighted (colored) as evidence.",0
52,"When specified two entities (bold black) in the sentence, there is a great opportunity for the instance to be incorrectly categorized as a confusing relation (red) instead of the true relation (blue).",0
52,"Specifically, the first instance should be categorized as the true relation 'parents-child' based on the given entity pair and natural language (NL) expression 'a daughter of '.",0
52,"However, since it also includes the NL expression 'his wife', it is probably misclassified into this confusing relation 'husband-wife'.",0
52,"In this paper, we name it as a relation confusion problem.",0
52,"To address the relation confusion problem, it is crucial for a model to be aware of which NL expressions cause confusion and learn to avoid mapping the instance into its easily-confused relation.",0
52,"True Relation Confusing Relation Instance parents-child husband-wife She was a daughter of prince Wilhelm of Baden and his wife princess Maria of Lichtenberg, as well as an elder sister of prince Maximilian.",0
52,"husband-wife uncle-nephew He was the youngest son of Prescott Sheldon Bush and his wife Dorothy Walker Bush, and the uncle of former president George W Bush.",0
52,"uncle-nephew parents-child Snowdon is the son of princess Margaret, countess of Snowdon, and the 1st earl of Snowdon, thus he is the nephew of queen Elizabeth ii.",0
52,Table 1: Example sentences containing confusing relations.,0
52,Their specified entities are marked as italics in bold.,0
52,The blue and red words respectively correspond to positive and confusing relations.,0
52,"these perspectives, we propose two assumptions.",0
52,"Firstly, in a sentence, words that keep high relevance to the given entities are more important in expressing the true relation.",0
52,"Intuitively, the specified entity information is crucial to identify the true relations.",0
52,"Secondly, explicitly learning of mapping an instance into its confusing relation with augmented data in turn boosts a few-shot RC model on identifying the true relation.",0
52,"Based on these assumptions, we propose two mechanisms in this paper: (1) An Entity-Guided Attention (EGA) encoder, which leverages the syntactic relations and relative positions between each word and the specified entity pair to softly select important information of words expressing the true relation and filter out the information causing confusion.",1
52,"(2) A Confusion-Aware Training (CAT) method, which explicitly learns to distinguish relations by playing a pushing-away game between classifying a sentence into a true relation and its confusing relation.",1
52,"In addition, inspired by the succeed of pre-trained language models, our approaches are based on the typical BERT (Devlin et al., 2018) to introduce language knowledge, which has been proved effective especially for few-shot learning tasks.",1
52,"Specifically, the backbone of the encoder of our model is a transformer equipped with the proposed EGA which guides the calculation of self-attention distributions by weighting the attention logits with entity-aware gates.",1
52,The gates are used to measure the relevance between each word and the given two entities.,1
52,Two types of information for each word are used to calculate its gate.,1
52,"One is the relative position (Zeng et al., 2015a) information, which is the relative distance between a word and an entity in the input sequence.",1
52,"The other is syntactic relation which is proposed in this paper, defined as the dependency relations between each word and the entities.",1
52,"Based on these information, the entity-aware gates in EGA are able to select those important words and control the contribution of each word in self-attention.",1
52,We also propose CAT to explicitly force the model to asynchronously learn the classification from an instance to its true relation and its confusing relation.,1
52,"After each training step, the CAT first selects those misclassified sentences, and regards the relations they are misclassified into as the confusing relations.",1
52,"After that, The CAT uses these misclassified instances and their confusing relations as augmented data to conduct an additional training process, which aims to learn the mapping between these instances into the confusing relations.",1
52,"Afterwards, the CAT adopts the KL divergence (Kullback and Leibler, 1951) to teach the model to distinguish the difference between the true and the confusing relations, which benefits the true relation classification from the confusing relation identification.",1
52,The contributions of this paper are summarized as follows:,0
52,"(1) We propose an Entity-Aware Attention encoder, which can select crucial words and filter out NL expressions causing confusion based on their relevance to the specified entities.",1
52,(2) We propose a Confusion-Aware Training process to enhance the ability of the model to distinguish the true and confusing relations.,1
52,"(3) We conduct extensive experiments on the widely used few-shot RC dataset FewRel, where the results show that our model achieves comparable and even much better results to strong baselines.",1
52,"Furthermore, ablation and case studies verify the effectiveness of our EGA and CAT, especially in addressing the relation confusion problem.",0
531,"Due to the explosive growth of the textual information, text summarization, which is an important task in Natural Language Processing (NLP), has been widely studied for several years.",0
531,It can be categorized into two types: extractive and abstractive.,0
531,"Extractive methods select sentences or phrases from the source text directly (Nallapati et al., 2017;Zhou et al., 2018;Zhang et al., 2018a;, while abstractive methods, which is more similar to how humans summarize texts, attempt to understand the semantic information of source text and generate new expressions as the summary.",0
531,"Recently, neural network methods have led to encouraging results in the abstractive summarization of single-speaker documents like news, scientific publications, etc (Rush et al., 2015;Gehrmann et al., 2018).",0
531,These approaches employ a sequence-to-sequence general framework where the documents are fed into an encoder network and another decoder network learns to decode the summary.,0
531,"With the popularity of phone calls, e-mails, and social network applications, people share information in more different ways, which are often in the form of dialogues.",0
531,"Different from news texts, dialogue is a dynamic information exchange flow, which is often informal, verbose and repetitive, sprinkled with false-starts, backchanneling, reconfirmations, hesitations, and speaker interruptions (Sacks et al., 1974).",0
531,"Besides, utterances are often turned from different interlocutors, which leads to the topic drifts, and lower information density.",0
531,These problems need to be solved using natural language generation techniques with a high level of semantic understanding.,0
531,"Some early works benchmarked the abstractive dialogue summarization task using the AMI meeting corpus, which contains a wide range of annotations, including dialogue acts, topic descriptions, etc.",0
531,"(Carletta et al., 2005;Mehdad et al., 2014;Banerjee et al., 2015).",0
531,Goo and Chen (2018) proposed to use the high-level topic descriptions (e.g.,0
531,costing evaluation of project process) as the gold references and leveraged dialogue act signals in a neural summarization model.,0
531,They assumed that dialogue acts indicated interactive signals and used these information for a better performance.,0
531,"Because this meeting dataset has a low number of summaries and is different from real dialogues, this network can not reflect its effectiveness on dialogue summarization.",0
531,"Customer service interaction is also a common form of dialogue, which contains questions of the user and solutions of the agent.",0
531,Liu et al.,0
531,(2019a) collected a dialogue-summary dataset from the logs in the DiDi customer service center.,0
531,"They proposed a novel Leader-Writer network, which relies on auxiliary key point sequences to ensure the logic and integrity of dialogue summaries, and designs a hierarchical decoder.",0
531,"The rules of labeling the key point sequences are given by domain experts, which needs to consume a lot of human efforts.",0
531,"Considering the lack of highquality datasets, Gliwa et al.",0
531,(2019) created the SAMSum Corpus and further investigated the problems of dialogue summary generation.,0
531,They only proposed the dataset and experimented with general networks of text summarization.,0
531,"Although some progress has been made in abstractive dialogue summarization task, previous methods do not develop specially designed solutions for dialogues, and are all dependent on sequence-to-sequence models, which can not handle the sentence-level long-distance dependency and capture the cross-sentence relations.",0
531,"To mitigate these issues, an intuitive way is to model the relations of sentences using the graph structures, which can break the sequential positions of dialogues and directly connect the related long-distance utterances.",0
531,"In this paper, we propose a Topic-word Guided Dialogue Graph Attention (TGDGA) network that discovers the intra-sentence and inter-sentence relations by graph neural networks, and generates summaries relied on the graph-to-sequence framework and topic words.",1
531,"Nodes of different granularity levels represent topic word features and utterance sequence features, respectively.",1
531,The edges in the graph are initialized by the linguistic information relationships between the nodes.,1
531,The masking mechanism operated in the graph self-attention layer only leverages related utterances and filters out redundant utterances.,1
531,The dialogue graph aggregates the useful conversation history and captures cross-sentence relations effectively.,1
531,"Besides, we encode the topic words to the topic information representation and integrate it into the decoder, to guide the process of generation.",1
531,The key contributions of this work include:,0
531,"? To the best of our knowledge, we are the first to construct the whole dialogue as a graph for abstractive dialogue summarization.",1
531,The proper graph structure permits easier analysis of various key information in the dialogue and separates available utterances.,1
531,"Graph neural networks avoid the problem of long-distance dependency and the cross-sentence relations can be extracted, which makes the information flow of the dialogue more clearer.",1
531,? We devise a topic-word guided graph-to-sequence network that generates dialogue summaries in an end-to-end way.,1
531,"The topic word information is leveraged through graph attention mechanism, coverage mechanism, and pointer mechanism, which makes the summary more centralized with key elements.",1
531,Experiments show that our model outperforms all baselines on two benchmark datasets without the pre-trained language models.,0
531,2 Related Work,0
583,"Standard test, such as TOFEL and SAT, is an efficient and essential tool to assess knowledge proficiency of a learner (Ch and Saha, 2018).",0
583,"According to testing results, teachers or ITS (Intelligent Tutoring System) services can develop personalized study plans for different students.",0
583,"When organizing a standard test, a vital issue is to select a suitable question form.",0
583,"Among various question forms, multiple choice question (MCQ) is widely adopted in many notable tests, such as GRE, TOFEL and SAT.",0
583,"MCQs have many advantages including less testing time, more objective and easy on the grader (Ch and Saha, 2018).",0
583,"A typical MCQ consists of a stem and several candidate answers, among which one is correct, the rest are distractors.",0
583,"As shown in Figure 1, in addition to a stem, some tests also include a long reading passage to provide the context of this MCQ.",0
583,The quality of an MCQ depends heavily on the quality of the distractors.,0
583,"If the distractors can not confuse students, the correct answer could be concluded easily.",0
583,"As a result, the discrimination of the question will degrade, and the test will also lose the ability of the assessment.",0
583,"However, it is a challenging job to design useful and qualified distractors.",0
583,"Rather than being a trivial wrong answer, the distractor should have the plausibility which confuses learners who did not master the knowledge (Liang et al., 2018).",0
583,"A good distractor should be grammatically correct given the question and semantically consistent with the passage context of the question (Gao et al., 2018).",0
583,"Meanwhile, the question composers need to enhance the plausibility of the distractor without hurting its inherent incorrectness.",0
583,"Otherwise, the distractor easily becomes a definitely wrong answer, further making the question to be sloppy.",0
583,"Hence, the manual preparation of distractors is time-consuming and costly (Ha and Yaneva, 2018).",0
583,"It is an urgent issue to automatically generate useful distractors, which can help to alleviate question composers' workload and relax the restrictions on experience.",0
583,It could also be helpful to prepare a large train set to boost the machine reading comprehension (MRC) systems .,0
583,"In this paper, we focus on automatically generating semantic-rich distractors for MCQs in realworld standard tests, such as RACE (Lai et al., 2017) which is collected from the English exams for Chinese students from grades 7 to 12. zp10211059 Passage: ... Held on a farm, the Glastonbury Festival is the most well-known and popular festival in the UK.",0
583,It began in 1970 and the rst festival was attended by one thousand ve hundred people each paying an admission price of PS1 ...,0
583,"Since then the Glastonbury Festival has gone from strength to strength -in 2004 one hundred and fty thousand fans attended, paying PS112 each for a ticket to the three-day event.",0
583,Tickets for the event sold out within three hours.,0
583,... Glastonbury is not unique in using live music to raise money to ght global poverty.,0
583,"Existing works conduct some attempts on generating short distractors (Stasaski and Hearst, 2017;Guo et al., 2016).",0
583,These approaches formulate distractor generation as a similar word selection task.,0
583,They leverage the pre-defined ontology or word embeddings to find similar words/entities of the correct answer as the generated distractors.,0
583,These word-level or entity-level methods can only generate short distractors and do not apply to semantic-rich and long distractors for RACE-like MCQs.,0
583,"Recently, generating longer distractors has been explored in a few studies (Zhou et al., 2019;Gao et al., 2018).",0
583,"For example, Gao et al.",0
583,"(2018) proposes a sequence-to-sequence based model, which leverages the attention mechanism to automatically generate distractors from the reading passages.",0
583,"However, these methods mainly focus on the relation between the distractor and the passage and fail to comprehensively model the interactions among the passage, question and correct answer which helps to ensure the incorrectness of the generated distractors.",0
583,"To better generate useful distractors, we propose a novel quEstion and answer guided Distractor GEneration (EDGE) framework.",1
583,"More specifically, given the passage, the question and the correct answer, we first leverage a contextual encoder to generate the semantic representations for all text materials.",1
583,Then we use the attention mechanism to enrich the context of the question and the correct answer.,1
583,"Next, we break down the distractor's usefulness into two aspects: the incorrectness and plausibility.",1
583,"Incorrectness is the inherent attribute of the distractor, while plausibility refers to the ability to confuse the students.",1
583,We introduce two modules by leveraging the gate layer to guarantee the incorrectness: Reforming Question Module and Reforming Passage Module.,1
583,"We further leverage an attention-based distractor generator, plus the previous two reforming modules, to guarantee the plausibility.",1
583,"Finally, in the generation stage, we use the beam search to generate several diverse distractors by controlling their distances.",1
583,We conduct experiments on a large-scale public distractor generation dataset prepared from RACE.,0
583,The experimental results demonstrate the effectiveness of our proposed framework.,0
583,"Moreover, our method achieves a new state-of-the-art result in the distractor generation task.",0
595,"Relation Classification (RC) is an important task in information extraction, aiming to extract the relation between two given entities based on their related context.",0
595,"RC has attracted increasing attention due to its broad applications in many downstream tasks, such as knowledge base construction (Luan et al., 2018) and question answering (Yu et al., 2017).",0
595,Conventional supervised RC approaches can not satisfy the practical needs of the relation classification.,0
595,"In the real world, there exist massive amounts of fine-grained relations.",0
595,"And, the labeled relation types are limited, and each type usually has a certain number of labeled samples.",0
595,"Naturally, it is prohibitive to generalize to new (unseen) relations (i.e., the model will fail when predicting a type with no training examples).",0
595,"For example, in Figure 1, basin country is an unseen relation type with no labeled sentence in the training stage.",0
595,"To this end, it is urgent for models to be able to extract relations in a zero-shot scenario.",0
595,"Previous zero-shot relation classification (ZSRC) approaches leverage transfer learning procedures by reading comprehension (Levy et al., 2017), textual entailment (Obamuyide and Vlachos, 2018), and so on.",0
595,"However, those methods have to rely on artificial descriptive information to improve the understandability of relation types.",0
595,"Inspired by the zero-shot learning in computer vision (Palatucci et al., 2009), it is natural to learn a mapping from the feature space of input samples to the semantic space such as class labels through a projection function.",0
595,The hypothesis is to build the semantic connections between seen and unseen relations.,0
595,Conventional approaches usually leverage word embeddings  of labels as a common semantic space.,0
595,"We argue that for relation classification, rich semantic knowledge is neglected in the relation labels space:",0
595,Implicit Semantic Connection with Knowledge Graph Embedding.,0
595,"Previous studies (Yang et al., 2014) have shown that the Knowledge Graph Embeddings (KGEs) of semantically similar relations are located near each other in the latent space.",0
595,"For instance, the relation place lived and nationality are more relevant, whereas the relation profession has less correlation with the former two relations.",0
595,"Thus, it is natural to leverage this knowledge from KGs to build connections between seen and unseen relations.",0
595,Explicit Semantic Connection with Rule Learning.,0
595,We human can easily recognize unseen relations via symbolic reasoning.,0
595,"As the example shown in Figure 1, with the rule that basin country of(y,z) can be deduced if located in country(x,y) and next to body of water(x,z), we can recognize the unseen relation basin country of based on seen relations located in country and next to body of water.",0
595,"To this end, it is intuitive to infuse rule knowledge to bridge the connections between training and zero-shot relations.",0
595,"Motivated by this, we take the first step to propose a novel approach, namely, Logic-guided Semantic Representation Learning (LSRL) for zero-shot relation classification.",1
595,"To begin with, we propose to utilize pre-trained knowledge graph embedding such as TransE (Bordes et al., 2013) to build the implicit semantic connection.",1
595,KGE embeds entities and relations into a continuous semantic vector space and can capture semantic connections between relations in semantic space.,1
595,"Further, we leverage logic rules mined via AMIE (Galrraga et al., 2013) from the knowledge graph and introduce rule-guided representation learning to obtain explicit semantic connection.",1
595,"It should be noted that our approach is model-agnostic, and therefore orthogonal to existing approaches.",1
595,"We integrate our approach with two well-known zero-shot methods, namely DeViSE  and ConSE (Norouzi et al., 2013).",1
595,Extensive experimental results demonstrate the efficacy of our approach.,0
595,The main contributions of this work are as follows:,0
595,? We introduce implicit semantic connection with knowledge graph embedding and explicit semantic connection with rule learning for zero-shot relation classification.,1
595,? We propose a novel rule-guided semantic representation learning to build connections between the seen and unseen relations.,1
595,Our work is model-agnostic and can be plugged into different kinds of zero-shot learning approaches.,1
595,? Extensive experimental results show the efficacy of our approach and also reveals the usefulness of knowledge graph embedding and rule learning.,0
628,"Semantic role labeling (SRL) is a fundamental task in natural language processing (NLP), which aims to find the predicate argument structures (Who did what to whom, when and where, etc.)",0
628,in a sentence (see Figure 1 as an example).,0
628,"Recent SRL works can mostly be divided into two categories, i.e., syntax-aware (Roth and Lapata, 2016;He et al., 2018b;Strubell et al., 2018) and syntax-agnostic (He et al., 2017;He et al., 2018a) approaches according to whether incorporating syntactic knowledge or not.",0
628,"Most syntax-agnostic works employ deep BiLSTM or self-attention encoder to encode the contextual information of natural sentences, with various kinds of scorers to predict the probabilities of BIO-based semantic roles (He et al., 2017;Tan et al., 2018) or predicate-argument-role tuples (He et al., 2018a;.",0
628,"Motivated by the strong relevance of syntax and semantics, researchers explore various approaches to integrate syntactic knowledge into syntax-agnostic models.",0
628,Roth and Lapata (2016) propose to use dependency-based embeddings in a neural SRL model for dependency-based SRL.,0
628,(2018b) introduce k-order pruning algorithm to prune arguments according to dependency trees.,0
628,"However, previous syntax-aware works mainly employ singleton/homogeneous automatic dependency trees, which are generated by a syntactic parser trained on a specific syntactic treebank, like Penn Treebank (PTB) (Marcus et al., 1994).",0
628,Our work follows the syntax-aware approach and enhances SRL with heterogeneous syntactic knowledge.,0
628,"All is well known, there exist many published dependency treebanks that follow different anno- tation guidelines, i.e., English PTB (Marcus et al., 1994), Universal Dependencies (UD) (Silveira et al., 2014), Penn Chinese Treebank (PCTB) (Xue et al., 2005), Chinese Dependency Treebank (CDT) (Che et al., 2012) and so on.",0
628,These dependency treebanks contain high-quality dependency trees and provide rich syntactic knowledge.,0
628,We believe that these heterogeneous syntactic treebanks 1 provide more valid information than each homogeneous treebank.,0
628,"In this work, we propose two kinds of methods from the perspective of explicit and implicit to take advantages of heterogeneous syntactic knowledge, which we believe are highly complementary.",1
628,Our baseline model follows the architecture of He et al.,0
628,"Afterwards, we inject the heterogeneous syntactic knowledge into the base model using two proposed methods.",1
628,"For the explicit method, we explore to encode the heterogeneous automatic dependency trees with the recent popular graph convolutional networks (GCN) (Kipf and Welling, 2016).",1
628,"For the implicit method, which is inspired by the powerful representations from pre-trained language models, like ELMo (Peters et al., 2018) and BERT (Devlin et al., 2019), we introduce a method to extract implicit syntactic representations from the dependency parser trained with heterogeneous syntactic treebanks.",1
628,It is well known that the main reason for the success of pre-trained language model representations is the use of large amounts of natural text.,0
628,"However, it is difficult to obtain and costly to annotate large amounts of syntactic data.",0
628,"Therefore, making full use of existing heterogeneous data is the most feasible and natural idea.",0
628,"To verify the effectiveness of injecting heterogeneous syntactic knowledge, we conduct experiments on the widely-used Chinese and English SRL benchmarks, i.e., Chinese Proposition Bank 1.0 and English CoNLL-2005.",0
628,Our contributions are listed as follows:,0
628,"? To our best knowledge, we are the first to utilize heterogeneous syntactic knowledge to help neural semantic role labeling.",1
628,"? We introduce two kinds of methods that effectively encode the heterogeneous syntactic knowledge for SRL, and achieve significant improvements over the strong baselines.",1
628,? Detailed analyses clearly show that integrating heterogeneous syntactic knowledge outperforms homogeneous syntactic knowledge and also demonstrate the effectiveness of our methods for encoding heterogeneous syntactic knowledge.,0
632,"""The biggest deficit we have in our society, and in the world right now, is an empathy deficit.",0
632,We are in great need of people being able to stand in somebody else's shoes and see the world through their eyes.",0
632,"Barack Obama in 2009 when talking to students in Istanbul As Barack Obama, former president of the United States, stated, empathy is not only an elementary skill for our society and daily interaction but also for professional communication as well as successful teamwork and thus elementary for educational curricula (i.e., Learning Framework 2030 (OECD, 2018)).",0
632,It is the "ability to simply understand the other person's perspective [.,0
632,"and to react to the observed experiences of another"" (Davis (1983), p.1), which is defined as empathy.",0
632,"1 Empathy skills not only pave the foundation for successful interaction in digital companies, e.g., in agile work environments (Luca and Tarricone, 2001), but they are also one of the key abilities in the future that might distinguish human work force and artificial intelligence agents (Poser and Bittner, 2020).",0
632,"However, besides the growing importance of empathy, research has shown that empathy skills of US college students have decreased from 1979 to 2009 by more than thirty percent and even more rapidly from 2000 to 2009 (Konrath et al., 2011).",0
632,"On these grounds, the Organization for Economic Cooperation and Development (OECD) claims that training empathy skills should receive a more prominent role in today's higher education (OECD, 2018).",0
632,"To train empathy to students, educational institutions traditionally rely on experiential learning scenarios, such as shadowing, communication skills training or role playing (Lok and Foster, 2019;van Berkhout and Malouff, 2016).",0
632,"Individual empathy training is therefore only available for a limited number of students, since individual tutoring through a student's learning journey is often hindered due to traditional large-scale lectures or the growing field of distance learning scenarios such as Massive Open Online Classes (MOOCs) (Seaman et al., 2018).",0
632,"However, to develop skills such as empathy, it is of great importance for the individual student to receive continuous feedback throughout their learning journey (Hattie and Timperley, 2007;Vygotsky, 1980).",0
632,"In fact, educational institutions are limited in providing these individual learning conditions especially for empathy skill training.",0
632,"One possible path for providing individual learning conditions is to leverage recent developments in Computational Linguistics in the form of computer-assisted writing, which enables the development of writing support systems that provide tailored feedback and recommendations (Santos et al., 2018) (e.g., already used for argumentation skill learning (Wambsganss et al., 2020)).",0
632,"However, even if the detection of empathy in texts has been gaining attention recently, annotated corpora based on rigorous annotation schemes and annotation studies to train predictive models are rather rare in current literature .",0
632,"Nevertheless, for the detection of emotions, NLP has made major progress, with sentiment analysis being one of the most prominent areas in recent years (Liu, 2015).",0
632,"Recently, studies have also started investigating more elaborated models of human emotions (e.g., Wang et al.",0
632,"(2016), Abdul-Mageed andUngar (2017), Mohammad andBravo-Marquez (2017) or Buechel and Hahn (2018)), but available corpora for empathy detection are rather rare.",0
632,"As  and Strapparava and Mihalcea (2007) argue, one reason for that might be that the construction of corpora for empathy and emotion detection and modelling in general -usually based on psychological theory -are more challenging compared to usual sentiment analysis.",0
632,"Most existing works for empathy detection focus on spoken dialogue, addressing conversational agents, psychological interventions or call center applications (e.g., McQuiggan and Lester (2007), Prez-Rosas et al.",0
632,(2017) or Alam et al.,0
632,"Only a few studies address the detection and prediction of empathy in natural texts (Khanpour et al., 2017;Xiao et al., 2012), and to the best of our knowledge only one corpus is publicly available for empathy modelling based on news story reactions .",0
632,"Past literature in Computational Linguistics therefore lacks 1) publicly available empathy annotated data sets, 2) empathy annotation models based on rigorous annotation guidelines combined with annotation studies to assess the quality of the data and 3) the alignment of empathy in literature on psychological constructs and theories.",0
632,"In order to overcome these limitations, we introduce an empathy annotation scheme for student-written peer reviews.",1
632,"Moreover, we present a corpus of 500 student-written peer reviews that are annotated for the three types of review components strengths, weaknesses and suggestions for improvements and their emotional and cognitive empathy level embedded and based on psychological theory (Davis, 1983;Spreng et al., 2009).",1
632,"Our contribution is threefold: 1) we derive a novel annotation scheme for a new data domain, empathy modeling based on psychological theory and previous work on annotation schemes for empathy modeling ; 2) we present an annotation study based on 92 student peer reviews and three annotators to show that the annotation of empathy in student peer reviews is reliably possible; 3) to the best of our knowledge we present the second freely available corpus for empathy detection in general and the first corpus for empathy detection in the educational domain based on 500 student peer reviews collected in our lecture about business innovation in German.",1
632,We therefore hope to encourage future research on student-generated empathetic texts and on writing support systems to train empathy skills of students based on Natural Language Processing (NLP).,0
632,2 Conceptual Background and Related Work,0
660,"Characters are some of the most central elements of narratives, and the concept of character plays an important role in most definitions of narrative.",0
660,"As an example, Monika Fludernik defines a narrative as ""a representation of a possible world .",0
660,at whose centre there are one or several protagonists of an anthropomorphic nature .,0
660,who (mostly) perform goal-directed actions .,0
660,"(Fludernik, 2009, p.6;emphasis ours).",0
660,This definition clearly states that characters are central to stories per se.,0
660,"Therefore, it is natural to assume that character identification is an important step in automatic approaches to story understanding.",0
660,A number of approaches have been proposed for automatically identifying characters.,0
660,"Some approaches, for example, have sought to solve the character identification task using domain-specific ontologies (Declerck et al., 2012) or reasoning by reference to an existing case base (Valls-Vargas et al., 2014).",0
660,"Others have taken supervised machine learning approaches (Calix et al., 2013;Barros et al., 2019), where a classifier is trained over data annotated by people.",0
660,"Some approaches, e.g., examining characters' social networks (Sack, 2013), take character identification for granted, implementing heuristic-driven approaches over named entities or coreference chains that are not examined for their efficacy.",0
660,"Regardless of approach, all prior work of which we are aware has, unfortunately, had a relatively impoverished concept of character, at least from a narratological point of view.",0
660,"In particular, a key aspect of any character is that it contributes to the plot-characters are not just any animate entity in the narrative-and all prior work essentially ignores this point.",0
660,"Here we build on a prior proposal of ours (Anonymized, 2019) to incorporate this narratologically grounded definition of character into automatic character identification.",0
660,"We first define and operationalize our concept of character, and use that concept to generate annotated data (170 narrative texts drawn from 3 different corpora) with high inter-annotator agreement.",1
660,Then we demonstrate a straightforward supervised machine learning model using seven features that performs quite well on these data.,1
660,"Our error analysis reveals several choke points in the performance of the system, most importantly the quality of the co-reference chains.",1
660,The paper proceeds as follows.,0
660,"First, we discuss the definition of the character as presented by narratologists, contrasting this concept with those used in prior computational work, and describe an operationalized concept of character that can annotated with high inter-annotator agreement ( 2).",0
660,"We next describe the data to which we applied this concept ( 3), following which we discuss the experimental setup, including the features and classification model ( 4).",0
660,"We present the results ( 5) and analyze the error patterns of the system, discussing various aspects, which leads us to a discussion of future work ( 6).",0
660,"Although we have discussed prior work briefly in the introduction, we summarize work related to this study ( 7) before we conclude by enumerating our contributions ( 8).",0
660,2 An Operationalized Concept of Character,0
666,"In autoregressive Neural Machine Translation (NMT), a decoder generates one token at a time, and each output token depends on the output tokens generated so far.",0
666,The decoder's prediction of the end of the sentence determines the length of the output sentence.,0
666,This prediction is sometimes made too earlybefore all of the input information is translated-causing a so-called under-translation.,0
666,"Transformer has sinusoidal positional encoding to incorporate the token position information in the sequence into its encoder and decoder (Vaswani et al., 2017).",0
666,There are some previous studies for controlling an output length in Transformer.,0
666,Takase and Okazaki (2019) proposed two variants of length-aware positional encodings called length-ratio positional encoding (LRPE) and length-difference positional encoding (LDPE) to control the output length based on the given length constraints in automatic summarization.,0
666,Lakew et al.,0
666,(2019) applied LDPE and LRPE to NMT.,0
666,"They trained an NMT model using output length constraints based on LDPE and LRPE along with special tokens representing length ratio classes between input and output sentences, while they used the input sentence length at the inference time.",0
666,"However, the input sentence length is not a good estimator of the output length.",0
666,Using length constraints in the decoder is a promising approach to the under-translation problem.,0
666,We propose an NMT method based on LRPE and LDPE with a BERT-based output length prediction.,1
666,The proposed method adds noise to the output length constraints in training to improve its robustness against the possible length variances in the translation.,1
666,"In our experiments with an English-to-Japanese dataset, the BERT-based output length prediction outperformed the use of the input length, and the proposed method, including noise injection into the training-time length constraints, improved the translation performance in BLEU for short sentences.",0
673,Graph-based representations of sentence meaning offer an expressive and flexible means of modeling natural language semantics.,0
673,"In recent years, a number of different graphbanks have annotated large corpora with graph-based semantic representations of various types (Oepen and L?nning, 2006;Ivanova et al., 2012;Banarescu et al., 2013;Abend and Rappoport, 2013).",0
673,"Because of differences in graphbank design principles, individual graphs can differ greatly and often in fundamental strategies (Oepen et al., 2019).",0
673,"1 illustrates distinct graphs from the three graphbanks of the SemEval 2015 Shared Task on Semantic Dependency Parsing, which we take as our focus in this paper.",0
673,"The graphs visibly differ with respect to edge labels, edge directions, the treatment of a periphrastic verb construction, and coordination.",0
673,"In this paper, we develop a methodology to (i) understand the nature of the differences across graphbanks and (ii) normalize annotations from different graphbanks at the level of their compositional structures.",1
673,This approach allows us to identify which design differences between graphbanks are linguistically meaningful and important for the design of future corpora; as well as to identify more unified approaches to certain structures to potentially facilitate multi-task learning (MTL).,1
673,"In Section 2 we build upon , who used AM dependency trees to represent the compositional structure of graph-based meaning representations (MRs) based on the AM algebra (Groschwitz et al., 2018).",0
673,"We detail a new methodology for identifying and quantifying mismatches between the compositional structures assigned to the same sentence by different graphbanks; we then present our extended AM+ algebra, with which we can systematically reshape these compositional structures to align them across graphbanks (Section 3).",1
673,"Finally, we provide key examples of how our methodology normalizes specific linguistic phenomena that pose challenges to MR design and differ in representation across graphbanks (Section 4).",0
673,"Using our methods, we increase the match between the AM dependency trees (compositional structure) of the different graphbanks to 76.3 (DM-PAS), 78.8 (DM-PSD), and 82.0 (PAS-PSD) directed unlabeled F-score, compared to 63.5, 55.7, and 57.0 for Lindemann et al.",0
673,'s AM dependency trees (Section 5).,0
673,"This is a drastic improvement over the unlabeled F-scores of 64.2, 26.1, and 29.6 for the original graphs (Oepen et al., 2015).",0
673,"We additionally demonstrate that when training a graph parser on a tiny graphbank combined with larger graphbanks using multi-task learning (MTL), our methods improve parsing accuracy by up to 2.6 points F-score over MTL without normalized AM dependency trees.",1
673,Our work serves as a proof of concept that linguistically-informed methods to normalize compositional structure across graphbanks is successful and can be applied at broad-scale and potentially to more complex graphbanks (Section 6).,1
673,2 Background,0
707,Data-to-text Natural Language Generation (NLG) is the computational process of generating natural language in the form of text or voice from non-linguistic data.,0
707,"A traditional micro-planning task within the pipeline data-to-text architecture is referring expression generation (REG) (Krahmer and van Deemter, 2019), which aims to automatically generate appropriate noun phrases (e.g., The mathematician Ada Lovelace) to refer to entities (e.g., Ada Lovelace) mentioned as discourse unfolds (e.g., """,0
707,was the first to recognise that the machine had applications beyond pure calculation.,0
707,"Traditionally, REG systems produce references to discourse entities in two explicit steps.",0
707,"First, they decide on the referential form, i.e., choosing whether a referring expression should be a pronoun (She), a proper name (Ada Lovelace), a description (The mathematician), etc.",0
707,"Once the choice is made, such systems textually realize the referring expression based on the chosen referential form and discourse context.",0
707,"If the first step selects a proper name as the form to refer to Ada Lovelace for instance, the ensuing step is responsible for deciding, among Ada, Ada Lovelace, or another text realization, i.e.",0
707,the one that is the most appropriate referring expression to that entity in a given discourse context.,0
707,"With the advent of large amounts of data, REG systems have undergone a significant change in their architecture.",0
707,"From being rule-based modular, they have become data-driven end-to-end systems that aim to perform the choice of referential form and surface realization jointly.",0
707,"An example of these more integrated approaches is NeuralREG (Castro Ferreira et al., 2018a), an end-to-end neural REG model that produces referring expressions deciding on form and content jointly based on representations of the referent and its surrounding context.",0
707,"Although NeuralREG is able to generate adequate referring expressions to discourse entities seen during the training phase, the model does not generalize to unseen ones, i.e.",0
707,it can not generate referring expressions to entities which were not seen during its training.,0
707,This study aims to fill this gap by proposing two extensions to the model's original architecture.,1
707,"The first is a copy mechanism, which may decide at each decoding timestep whether the next token of the referring expression should be generated from the output vocabulary or copied from the input representation of the target entity.",1
707,We thereby hypothesize that the model will be able to generate a token from the vocabulary for seen entities and to copy tokens from the input representation in the case of unseen ones.,1
707,The second extension consists of representing the gender and type of the entity as input to the model.,1
707,"Such information can be easily extracted from the Semantic Web and may help the model to generate pronominal (e.g., She) and descriptive (e.g., The country) referring expressions to unseen entities.",1
707,"To evaluate our approach, we conducted experiments relying on a delexicalized version (Castro Ferreira et al., 2018b) of the WebNLG corpus (Gardent et al., 2017b).",0
707,"We first compare our proposal with the original NeuralREG and other related approaches as ProfileREG (Cao and Cheung, 2019).",0
707,"Second, to assess the quality of the texts generated by our model, we conducted a supplementary evaluation with human judges.",0
707,"Next, we follow the rationale of ablation studies to analyze the importance of each feature in our model within the process of referring expression generation.",0
707,"Finally, we discuss some advantages of the introduced features and how they interact to improve accuracy, variety, and generalization.",0
779,"Dependency parsing aims to capture syntax with a dependency tree and is proven to be helpful for various natural language processing (NLP) tasks, such as semantic role labeling (Xia et al., 2019), natural language generation (Park and Kang, 2019), and machine translation (Hadiwinoto and Ng, 2017).",0
779,Given an input sentence s = w 1 w 2 .,0
779,"w n , a dependency tree, as depicted in Figure 1, is defined as d = {(h, m, l), 0  h  n, 1  m  n, l  L}, where (h, m, l) is a dependency from the head word w h to the child word w m with the relation label l  L, and w 0 is pseudo word that points to the root word of the sentence.",0
779,"In recent years, neural network based approaches have achieved remarkable improvement and outperformed the traditional discrete-feature based approaches by a large margin in dependency parsing (Chen $       $ very cool , praise praise praise  and Manning, 2014;Kiperwasser and Goldberg, 2016;Andor et al., 2016;Dozat and Manning, 2017).",0
779,"Most remarkably, Dozat and Manning (2017) propose a simple yet effective deep BiAffine parser and achieve state-of-the-art accuracy on a variety of datasets and languages.",0
779,"However, the domain adaptation problem, i.e., how to improve parsing performance on texts that are very different from the training data, remains a key challenge for the parsing community, especially when trying to apply the parsing technique to real-life web data.",0
779,"Taking the examples in Figure 1, we can see that as user-generated texts, the left sentence from the product comment (PC) domain is quite non-canonical and contains a lot of ellipsis phenomena.",0
779,"In contrast, the right one from the balanced corpus (BC) domain is a typical sentence from newswire texts and is much more formal.",0
779,"Hence, domain differences can be represented with both sentence and parse tree distribution changes due to new words and phrases, new expression structures, etc.",0
779,The key for domain adaptation is how to model differences and commonalities between different domains.,0
779,"Most previous works focus on unsupervised cross-domain parsing, assuming there is no target-domain labeled data.",0
779,"Typical methods include self-training (McClosky and Charniak, 2008;Yu et al., 2015) and co-training (Sarkar, 2001).",0
779,"However, due to the intrinsic difficulty of domain adaptation, progress in this direction is very slow.",0
779,"In the past few years, semi-supervised cross-domain parsing attracts more attention due to the emerge of more labeled data.",0
779,"Particularly, Li et al.",0
779,"(2019b) release a large-scale labeled and unlabeled data, and they find that their proposed domain embedding (DE) approach is more effective than the direct concatenation (CON) method.",0
779,"The feature argumentation (FA) method is another typical technique for semi-supervised domain adaptation, which is proposed by kim et al.",0
779,( 2016) for sequence labeling tasks.,0
779,"To learn the domain-invariant and domain-specific features, the DE method uses explicit domain indicators as extra inputs, whereas the FA method employs one shared and multiple domainspecific BiLSTM encoders.",0
779,This work proposes to model the distinguish and commonality representations between domains by adversarial learning and fine-tuning BERT.,1
779,"To alleviate the domain-invariant representations from being contaminated by domain-specific ones, we apply adversarial learning to enhance three typical semisupervised approaches, i.e., CON, FA, and DE with two useful strategies, i.e., fused target-domain word representations and orthogonality constraints.",1
779,"At the same time, we utilize a large-scale target-domain unlabeled data to fine-tune BERT and obtain more reliable contextualized word representations, leading to a large improvement over using off-the-shelf BERT.",1
779,Our final single model achieves nearly the same state-of-the-art performance as the ensemble models with BERT of Li et al.,0
779,"(2019c), which won the first place in the cross-domain parsing shared task recently organized at the international conference on natural language processing and Chinese computing (NLPCC-2019).",0
779,"Although we focus on semisupervised domain adaptation for dependency parsing, the techniques and findings may be applicable to domain adaptation for other NLP tasks.",0
779,We will release our codes at http://url.,0
784,"Neural Machine Translation (NMT) adopts the encoder-decoder paradigm to model the entire translation process (Bahdanau et al., 2015).",0
784,"Specifically, the encoder finds a multi-layer representation of the source sentence, and the decoder queries the topmost encoding representation to produce the target sentence through a cross-attention mechanism (Wu et al., 2016;Vaswani et al., 2017).",0
784,"However, such overreliance on the topmost encoding layer is problematic in two aspects: (1) Prone to over-fitting, especially when the encoder is under-trained, such as in low-resource tasks ; (2) It cannot make full use of representations extracted from lower encoder layers, which are syntactically and semantically complementary to higher layers (Peters et al., 2018;Raganato and Tiedemann, 2018).",0
784,"To mitigate this issue, researchers have proposed many methods to make the model aware of various encoder layers besides the topmost.",0
784,"Almost all of them resort to the adjustment of network structure, which can be further divided into two categories.",0
784,"The first is to merge the feature representations extracted by distinct encoder layers before being fed to the decoder Dou et al., 2018;Wang et al., 2019b).",0
784,"The differences between them lie in the design of the merge function: through self-attention , recurrent neural network (Wang et al., 2019b), or tree-like hierarchical merge (Dou et al., 2018).",0
784,"Moreover, the second makes each decoder layer explicitly align to a parallel encoder layer (He et al., 2018) or all encoder layers (Bapna et al., 2018).",0
784,"However, the above methods either complicate the original model Dou et al., 2018;Wang et al., 2019b;Bapna et al., 2018) or limit the flexibility of the model, such as requiring the number of the encoder layers to be the same as the decoder layers (He et al., 2018).",0
784,"Instead, in this work, we propose layer-wise multi-view learning to address this problem from the perspective of model training, without changing the model structure.",1
784,"The highlight of our method is that only the training process is concerned, while the inference speed is guaranteed to be exactly the same as that of the standard model.",1
784,The core idea is that we regard the off-the-shelf output of each encoding layer as a view for the input sentence.,1
784,"Therefore, it is straightforward and cheap to construct multiple views during a standard layer-by-layer encoding process.",1
784,"Further, in addition to the output of the topmost encoder layer used in standard models (refer to the primary view), we also incorporate an intermediate encoder layer as the auxiliary view.",1
784,We feed the two views to a partially shared decoder for independent predictions.,1
784,An additional regularization loss based on prediction consistency between views is used to encourage the auxiliary view to mimic the primary view.,1
784,"Thanks to the co-training on the two views, the gradients during back-propagation can flow into the two views simultaneously, which implicitly realizes the knowledge transfer between the views.",1
784,"Extensive experimental results on five translation tasks (KoEn, IWSLT'14 DeEn, WMT'17 TrEn,WMT'16 RoEn,and WMT'16 EnDe) show that our method can stably outperform multiple baseline models (Vaswani et al., 2017;Dou et al., 2018;Bapna et al., 2018).",0
784,"In particular, we have achieved new state-of-the-art results of 10.8 BLEU on KoEn and 36.23 BLEU on IWSLT'14 DeEn.",0
784,"Further analysis shows that our method's success lies in the robustness to encoding representations and dark knowledge (Hinton et al., 2015) provided by consistency regularization.",0
802,It is expected that a document consists of text units that are logically connected within the context through discourse relations.,0
802,"Text-level discoursing parsing plays a significant role in many NLP downstream task, such as question-answering (Jansen et al., 2014), sentiment analysis (Mukherjee and Bhattacharyya, 2012) and abstractive summarization (Koto et al., 2019).",0
802,"The Rhetorical Structure Theory (RST) (Mann and Thompson, 1988) is one of the widely explored text-level discourse parsing theory, which parses a document into a hierarchical discourse tree.",0
802,"The discourse tree is built over its smallest parsing unit, namely, elementary discourse unit (EDU), placed in the leaf nodes.",0
802,"The tree's non-terminal nodes bear the information of span, nuclearity and relation.",0
802,"The span represents the cover range of a sequence of EDUs; while the nuclearity status is the semantic role in a relation (i.e., nucleus or satellite).",0
802,A text tagged by nucleus is more essential than the satellite.,0
802,"Conventionally, there are three nuclearity types including Nucleus-Satellite (NS), Satellite-Nucleus (SN) and Nucleus-Nucleus (NN).",0
802,"As for relation, there are mainly two types of relations: the mono-nuclear relation, such as ""Attribution"" or ""Summary"", with the the nuclearity type being either ""NS"" or ""SN""; and the multi-nuclear relation, such as ""Contrast"" or ""Same-Unit"", with the nuclearity types of ""NN"".",0
802,"Among all the parsing systems dedicated to RST parsing, the transition-based systems obtain comparative performance with the CYK-like system.",0
802,"The transition-based algorithms are first explored in (Marcu, 1999), which adopt a sequence of shift-reduce action to construct rhetorical structures of texts.",0
802,The study of Wang et al.,0
802,"(2017) achieves the best performance among all the transition-based systems, by parsing the discourse tree in two stages, namely, parsing a naked discourse tree (identifying span and nuclearity) in the first stage and assigning relation in the second stage.",0
802,"The researchers use only one classifier for the joint action of shift-reduce and nuclear types (e.g., Reduce-NS) to parse the tree.",0
802,"In this study, we explore whether two separate classifiers for shift-reduce action and nuclear type respectively (i.e., identifying span first, and then nuclearity comes later) is more effective in the RST parsing task.",1
802,Previous transition systems do not differentiate the nuclear type between multi-nuclear relations and binary-nuclear relations.,1
802,"In addition, they represent RST by the right-heavy binary trees, which inevitably create the redundant nonexistent node in the original RST tree.",1
802,"To solve these problems, we come up with a new nuclear type "" N"" for the multi-nuclear relation, and a new action "" R"" to construct a multi-branch tree.",1
802,Experimental results tell us that the newly designed nuclear type and action labels are capable of capturing multi-nuclear relations.,0
802,"In sum, our paper mainly make the following contributions:",0
802,? We propose a new nuclear type ( N) to distinguish the multi-nuclear types from the binary-nuclear types.,1
802,? We come up with a new action: the Flat Reduce ( R) to construct a multi-branch tree.,1
802,? We examine that the joint action of shift-reduce and nuclear type is more suitable in the transitionbased system comparing to the separate one.,1
814,"The availability of large monolingual and labeled corpora, paired with innovations in neural text processing have led to a rapid improvement of the quality of text classification over the last years.",0
814,"1 However, the effectiveness of deep-learning-based text classification models depends on the amount of monolingual and labeled data.",0
814,"Low-resource languages are traditionally left behind because of the few available prepared resources for these languages to extract the data from (Joshi et al., 2020).",0
814,"However, nowadays, the increase in internet use in many African developing countries has made access to information more easily.",0
814,This in turn has strengthened the news agencies of those countries to cover many stories in their native languages.,0
814,"For example, BBC News now provides online news in Arabic,Hausa,Kiswahili,Somali,Amharic,Oromo,Igbo,Pidgin,Tigrinya,Yoruba,Kinyarwanda and Kirundi.",0
814,2 This development makes news the most reliable source of data for low-resource languages.,0
814,"We explore this opportunity for the example of Kinyarwanda and Kirundi, two African low-resource Bantu languages, and build news classification benchmarks from online news articles.",0
814,"This has the goal to enable NLP researchers to include Kinyarwanda and Kirundi in the evaluation of novel text classification approaches, and diversify the current NLP landscape.",0
814,Kinyarwanda is one of the official languages of Rwanda 3 and belongs to the Niger-Congo language family.,0
814,"According to The New Times, 4 it is spoken by approximately 30 million people from four different African countries: Rwanda, Uganda, DR Congo, and Tanzania.",0
814,"Kirundi is an official language of Burundi, a neighboring country of Rwanda, and it is spoken by at least 9 million people.",0
814,5 Kinyarwanda and Kirundi are mutually intelligible.,0
814,Table 1 shows two example sentences to illustrate the similarity of the languages.,0
814,The two languages are part of the wider dialect continuum known as Rwanda-Rundi.,0
814,"6 In this family, they are other four indigenous low-resource languages: Shubi, 7 Hangaza, 8 Ha, 9 and Vinza.",0
814,"10 Among these four languages, Ha is also mutually intelligible for Kirundi and Kinyarwanda speakers, while other three are partially mutually intelligible.",0
814,"Developing NLP models for these languages is the goal for future work, because we could not retrieve any written news data for them.",0
814,Turafise ubwoba ko inyuma y'ikiza ca coronavirus bazohava bakena cane kuruta mbere.,0
814,Kinyarwanda Dufite ubwoba ko inyuma y'ikiza cya coronavirus bashobora gukena cyane kuruta mbere.,0
814,"We fear that after the coronavirus epidemic, they may become poor than before.",0
814,Kinyarwanda Ashushanya inyama n'ibintu bikozwe mu masashe n'impapuro.,0
814,Kirundi Ashushanya inyama n'ivyamwa bikozwe mu masashe n'impapuro.,0
814,He draws meat and things made of sacks and paper.,0
814,Table 1: Two examples of news titles from our datasets that show the similarity level between Kinyarwanda and Kirundi.,0
814,Same words in both languages for each sentence are shown in bold.,0
814,Joshi et al.,0
814,"(2020) classify the state of NLP for both Kinyarwanda and Kirundi as ""Scraping-By"", which means that they have been mostly excluded from previous NLP research, and require the creation of dedicated resources for future inclusion in NLP research.",0
814,"To this aim, we introduce two datasets KIN-NEWS and KIRNEWS for multi-class text classification in this paper.",1
814,They consist of the news articles written in Kinyarwanda and Kirundi collected from local news websites and newspapers.,1
814,KINNEWS samples are annotated using fourteen classes while that of KIRNEWS are annotated using twelve classes based on the agreement of the two annotators for each dataset.,1
814,"We describe a data cleaning pipeline, and we introduce the first ever stopword list for each language for preprocessing purposes.",1
814,"We present word embedding techniques for these two low-resource languages, and evaluate various classic and neural machine learning models.",1
814,"Together with the data, these baselines and preprocessing tools are made publicly available as benchmarks for future studies.",0
814,"In addition, pre-trained embeddings are published to facilitate studies for other NLP tasks on Kinyarwanda and Kirundi.",0
814,"11 In the following, we will first discuss previous work on Kinyarwanda and Kirundi and low-resource African languages in general in Section 2, and then describe the dataset creation in Section 3.",0
814,"We then present a range of experiments for text classification on the collected data in Section 4, concluding with an outlook to future work in Section 5.",0
87,Text mining is receiving continued attention from both researchers and practitioners.,0
87,"In the last years, several application areas have become popular, such as spam detection (Wood, 2016), sport performance prediction (Gruettner et al., 2020), web mining to predict pandemic outbreaks (Jahanbin and Rahmanian, 2020), predictive policing (Meijer and Wessels, 2019), or procedural knowledge extraction (Wambsganss and Fromm, 2019).",0
87,"Fueled with big data, artifical neural networks (ANN) and transfer learning algorithms such as BERT (Devlin et al., 2018) have been very successful in processing large amounts of data and predicting outcomes precisely (Alom et al., 2019).",0
87,"For certain use cases, however, traditional machine learning models are often the more promising modelling approach, since only little labelled data is available, and transparency as well as explainability play a major role for the practical success of the application of those algorithms.",0
87,"Exemplary use cases can be found in human-centered decision support systems (Stoica et al., 2017;Alom et al., 2019), e.g., writing support systems for educational feedback (Wambsganss et al., 2020b) or auditing for predictive policing (Scanlan, 2019).",0
87,"The main challenge of text mining is to preprocess written text and extract valuable features to enable information extraction (IE), data analytics (DA), or machine learning (ML) algorithms to reach their maximum performance (Rajman and Vesely, 2004;Johnson et al., 2015;Allahyari et al., 2017).",0
87,"However, a large amount of text mining applications simply rely on basic feature generation techniques such as bags of words (BOW) (Khadjeh Nassirtoussi et al., 2014) in which information such as the order and co-occurrence of words is not taken into account.",0
87,"This may lead to an overall underperformance, since copious training data is often not available (Pustejovsky and Stubbs, 2013).",0
87,"As a result, different teams of authors have indicated that enhancing text mining algorithms with the appropriate design, implementation, and evaluation of text features -a process commonly referred to as feature engineering -bears high chances of improving text mining outcomes significantly (Bird et al., 2009;Khadjeh Nassirtoussi et al., 2014;Johnson et al., 2015).",0
87,"However, feature engineering comes with a number of challenges.",0
87,"First and foremost, feature engineering still depends largely on human intuition and experience, since it requires deep domain knowledge to identify and operationalize relevant features.",0
87,"Second, as Talib et al.",0
87,"(2016) point out, text mining has been influenced by different disciplines like computer science, statistics, computational linguistics, and library and information sciences.",0
87,"Accordingly, text mining features that evolved in one particular discipline are often unknown or rarely used in other disciplines.",0
87,"Third, systematic feature frameworks that might help researchers and practitioners to select from, compare, and evaluate new or existing features across different disciplines or areas of application are rather rare.",0
87,"In this regard, existing literature focuses more on the comparison of particular domain-specific features and their learning algorithms but lacks a holistic text mining feature classification framework.",0
87,"Hence, our goal is to develop a comprehensive multidisciplinary taxonomy of text features to provide guidance by revealing the complex relationships of text features and by illustrating the application of certain feature groups based on a similarity analysis.",1
87,"To achieve our goal, we 1) illustrate the diversity of text feature engineering through a systematic literature review in which we found 133 distinct text mining features in different applications domains, 2) we outline a classification framework through a taxonomy with five dimensions and multiple characteristics, 3) we identify patterns and ""white spots"" in the application of text feature groups to provide further assistance for a) the development and design of new text mining solutions and b) the assessment and comparison of existing applications.",1
87,"For practical guidance, we therefore present our taxonomy as a text mining feature canvas, in which a single text feature can be categorized and compared along five dimensions.",1
87,"Moreover, we illustrate all 133 identified text features on a web platform and encourage researchers to add missing features accordingly.",1
87,Our ultimate goal is to present an eclectic collection of text features and a corresponding classification framework towards a shared understanding across application domains.,1
87,"Therefore, we contribute to both scientific literature by empirically analyzing the manifold use of features in text mining and to practice by proposing a menu of features and categories that might be of value in text mining projects.",1
87,"The resulting taxonomy should simplify the comparability of text features between different studies, domains, or applications and reduce the costly effort of feature engineering for practitioners and scientists when traditional machine learning methods are applied.",1
87,We present five dimensions of features diverse enough to demonstrate their commonalities and differences and comprehensive enough to encompass every feature we found in our systematic literature review.,1
87,We believe that such a menu of features will stimulate researchers and practitioners to enhance their knowledge about feature generation and to use the most appropriate features for their research.,0
889,The quasi-standard method in machine learning to determine the performance of a newly proposed method is to evaluate it on benchmark data sets.,0
889,The same applies for the evaluation of pre-trained language models frequently utilized for transfer learning in Natural Language Processing (NLP).,0
889,"Collections of benchmark data sets for different natural language understanding (NLU) tasks (Rajpurkar et al., 2016;Lai et al., 2017;Wang et al., 2018;Wang et al., 2019) have gained massive popularity among researchers in this field.",0
889,These benchmark collections stand out mainly due to two aspects: They are extremely well documented with respect to their creation and they are fixed with respect to the train-test split and the applied evaluation metrics.,0
889,"Furthermore they provide public leaderboards 1234 , where the results of submitted models are displayed in a unified fashion.",0
889,"For the majority of the proposed benchmark data sets the task is either a binary (Dolan and Brockett, 2005;Socher et al., 2013;Shankar et al., 2017;Warstadt et al., 2018;Wang et al., 2018) or a multi-class (Cer et al., 2017;Williams et al., 2017) classification task.",0
889,"To our knowledge, however, no existing (extreme) multi-label data sets (Lewis et al., 2004;Mencia and Frnkranz, 2008) have been used for performance evaluation by any of the current state-of-the-art transfer learning models in the context of social sience survey research.",0
889,"These, and other (tabular) multi-label data sets can e.g.",0
889,be found in repositories like MULAN 5 .,0
889,"In the social sciences, especially in survey research, definitive standards for raw data formatting of open-ended survey questions have not yet been established to our knowledge.",0
889,"This is not to say that there exist no current standards for handling and organizing survey research data in general (Inter-University Consortium For Political And Social Research (ICSPR), 2012; Lck and Landrock, 2019;CESSDA Training Team, 2020) or the metadata describing the primary data (Vardigan et al., 2008;Zenk-M?ltgen, 2012;Hoyle et al., 2016).",0
889,"Yet, for open-ended survey questions and their coding 6 , these standards have not been well established, apart from descriptions of best practices by some authors (Zll and Mohler, 2001;Zll, 2016;Zll and Menold, 2019;Lupia, 2018b;Lupia, 2018a).",0
889,"Because of the resulting structure (a text + multiple labels), this type of data presents an interesting type of task (multi-label classification) for NLP models, which is not yet included in the benchmark collections mentioned above 7 .",0
889,"Thus, in the spirit of the growing overall need for standardized data sets and for reproducibility, we propose a transparently described pre-processing of the ANES 2008 data set which enables its usage for benchmark comparisons for multi-label classification.",1
889,We provide baseline performance values for a simple machine learning model as well as for more recently proposed transfer learning architectures.,1
939,"Hypernymy, or the IS-A relation, is one of the most important lexical relations.",0
939,"It is used to create taxonomies of terms and it is the main organizational criterion of nouns and verbs in WordNet (Fellbaum, 1998).",0
939,"Learning hypernymy is also important in practice, as knowing a word's hypernyms gives an approximation of its meaning, and enables inferences in downstream tasks such as question answering and reading comprehension.",0
939,"Predicting hypernymy is still a challenging task for word embeddings (Pinter and Eisenstein, 2018;Bernier-Colborne and Barriere, 2018; and it is more difficult to predict hypernymy than other lexical relations (Bala?evi? et al., 2019).",0
939,"Hypernymy prediction is often evaluated against a given taxonomy, typically WordNet (Fellbaum, 1998).",0
939,"The main hypothesis that we pursue in this paper is that knowledge of this taxonomy, in particular of taxonomy paths, will be helpful for hypernymy prediction.",0
939,So we introduce two simple encoderdecoder based models for hypernym prediction that make use of information in the full taxonomy paths.,1
939,There has been much recent work on modeling lexical relations based on distributed representations.,0
939,"However, the task formulations and evaluation datasets have differed widely, making it hard to compare different approaches.",0
939,"We focus on evaluating on hypernymy, rather than jointly on many relations, which can mask strong performance differences across relations, and we focus on the prediction of hypernyms, as knowledge of hypernyms is important for inferences.",1
939,We evaluate our encoder-decoder models against several previous models that have not been evaluated in the same setting before.,1
939,"Like many other approaches, we use WordNet as the basis for our experiments.",0
939,We formulate the task as the task of finding the correct point to attach a new node (synset) to the WordNet taxonomy.,1
939,"We build on the existing WN18RR dataset (Dettmers et al., 2018), but filter its hypernymy pairs to produce WN18RR-hp, a subset that is leak-free with respect to approaches that use taxonomy paths during training, as we do.",0
939,"We find that one of our new models, hyper2path, achieves state-of-the-art performance on hypernym prediction on WN18RR-hp, with improvements over benchmark models of up to 5.17 points in accuracy of the highest-ranked prediction (hit-at-one, H@1).",0
955,Word embeddings have become ubiquitous in natural language processing applications.,0
955,"Usually, embeddings are trained from a large corpus of news or web data that contains writing from many sources and authors (Mikolov et al., 2013;Pennington et al., 2014).",0
955,These embeddings capture syntactic and semantic properties of the language of all authors who contributed to this corpus.,0
955,"Multi-source corpora provide large volumes of data, but they may not lead to the ideal representations for individuals.",0
955,"For instance, the word ""hometown"" may have a different representation for different individuals.",0
955,"For some, it may relate to words such as ""hills,"" ""trees,"" and ""family,"" whereas for others may be more strongly connected to ""ocean,"" ""beach,"" and ""friends.""",0
955,"These personalized representations differ among individuals, and also differ from a more generic representation that often tends to capture words that are semantically related at concept level, such as ""city,"" ""town,"" or ""place.""",0
955,"In this paper, we present the idea of personalized word embeddings.",0
955,We explore differences in personalized word representations using a corpus of English Reddit posts that contains a large number of posts per author (to be released).,1
955,We use the embeddings to initialize a language model and show that personalization leads to better results than generic embeddings.,1
955,One potential application of this work is personalized text generation for auto-completion to speed up text entry.,0
955,"Another application is dialog systems that follow the speaking style of certain professionals (e.g., counselors, advisors).",0
955,"Finally, personalized word representations could particularly help users with atypical writing styles that are not currently well served by models trained to suit the majority.",0
996,Mining translation equivalents to automate the process of generating and extending dictionaries from bilingual corpora is a well known Natural Language Processing application.,0
996,"Initially conducted on parallel data, this modus operandi has rapidly moved to using comparable corpora, mainly due to their availability and the fact that they are easier to acquire (Sharoff et al., 2013).",0
996,"Comparable corpora gather texts of the same domain, often over the same period without being in a translation relation.",0
996,"Journalistic or encyclopedic websites as well as web crawl data are usually popular for this purpose (Rapp, 1999;Li and Gaussier, 2010;Jakubina and Langlais, 2016;Rapp et al., 2020).",0
996,"In technical and scientific domains, comparable corpora suffer from their modest and limited size compared to general language corpora.",0
996,"This phenomenon is amplified by the difficulty to obtain many specialized documents in a language other than English (Morin and Hazem, 2014).",0
996,"This aspect, which is gradually changing with the deployment of open access, is nevertheless a major difficulty.",0
996,"One way to overcome the limited size of specialized (in-domain) comparable corpora for bilingual lexicon induction (BLI) task is to associate them with out-of-domain resources as lexical databases (Bouamor et al., 2013) or large general domain corpora (Hazem and Morin, 2018).",0
996,"For instance, by combining a specialized comparable corpus with a general domain corpus, methods based on distributional analysis are boosted (Hazem and Morin, 2018).",0
996,General domain corpora greatly enhance the context of specialized vocabulary by adding new contexts.,0
996,The main drawbacks of this data augmentation approach is the introduction of polysemous information as well as the tremendous increase of computation time.,0
996,"Consider for instance, a French/English comparable corpus in the medical domain and the French terms os (bone) and sein (breast).",0
996,"When looking for additional contexts in general corpora, os is very likely to recover contexts dedicated to an operating system since OS is the abbreviation used in French.",0
996,"Similarly, when studying the French term sein, many contexts related to the French preposition au sein de (among) will be added.",0
996,"In the same way, the English medical term breast can be associated with food just as in chicken breast to mention only the most obvious ones.",0
996,Associating out-of-domain data with a specialized corpus is not the mainstream research direction in bilingual lexicon extraction from specialized comparable corpora.,0
996,In computational terminology it is generally accepted that the specialized corpus conveys knowledge of the domain.,0
996,"In the same way, polysemy is often considered as a marginal phenomenon in technical and scientific domains.",0
996,"In this sense, associating out-of-domain data with a specialized comparable corpus can be seen as a ""heresy"" or, as in the previous examples, an unfortunate way to introduce polysemy.",0
996,"In the context of Statistical Machine Translation (SMT), Wang et al.",0
996,(2014) demonstrated that adding out-of-domain data to the training material of a system was detrimental when translating scientific and technical domain.,0
996,"Instead of using a data augmentation strategy, data selection is proposed to improve the quality of SMT systems (Moore and Lewis, 2010;Axelrod et al., 2011;Wang et al., 2014, among others).",0
996,The basic idea is that in-domain training data can be enriched with suitable sub-parts of out-of-domain data.,0
996,"Within this context, we show that a subtle selection of the contexts of out-of-domain data allows to improve the representation of the specialized domain, while preserving us of polysemy induced by data augmentation.",1
996,"Therefore, this strategy improves the quality of bilingual lexicons extracted from specialized comparable corpora while being more efficient from a computation time point of view.",1
