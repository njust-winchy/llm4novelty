# NovBench: Evaluating Large Language Models on Academic Paper Novelty Assessment

## Overview

**Dataset and source code for paper "NovBench: Evaluating Large Language Models on Academic Paper Novelty Assessment".**
The pipeline for constructing NovBench.<br>
[Figure1.pdf](https://github.com/user-attachments/files/24422549/Figure1.pdf)


## Dataset
The raw data and calculated data can be obtained from here (We will provide it after the peer review is completed).<br>
## Directory structure

<pre>
NovBench                                    Root directory
├── .............                           Code for ..
│
└── README.md
</pre>


## Dependency packages
System environment is set up according to the following configuration:
- transformers==4.16.2
- nltk==3.6.7
- matplotlib==3.5.1
- scikit-learn==1.1.3
- pytorch==2.0.1
- tqdm==4.65.0
- numpy==1.24.1
- pandas==2.2.3
- openai==1.53.0
- sentence-transformers==3.4.1
- ai_researcher
## Acknowledgement

The datasets we use come from Dycke et al.（2023）(https://github.com/UKPLab/nlpeer)

>Nils Dycke, Ilia Kuznetsov, and Iryna Gurevych. 2023. NLPeer: A Unified Resource for the Computational Study of Peer Review. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 5049–5073, Toronto, Canada. Association for Computational Linguistics.<br>

## Citation
Please cite the following paper if you use this code and dataset in your work.




